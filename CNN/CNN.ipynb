{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"a8nd8nWJO1u1"},"source":["# Edge Detection\n","\n","This is an algorithm for classifying images. Edge detection on an image is performed in both vertically and horizontally.\n"," \n","Now suppose we have a grayscale picture. Different edge detection operations of that grayscale image are shown below -\n","\n","#### Example - 1:\n","![alt text](http://what-when-how.com/wp-content/uploads/2011/09/tmp17F46_thumb.jpg)\n","\n","* **(a)** -> The original **Grayscale Image**.\n","* **(b)** -> Output after applying **Vertical Edge Detection Filter**.\n","* **(c)** -> Output after applying **Horizontal Edge Detection Filter**.\n","* **(d)** -> Output after applying **Both Edge Detection Filter**.\n","\n","Let's see another example -\n","#### Example - 2:\n","![alt text](http://media5.datahacker.rs/2018/10/edges.png)\n","\n","### How does it work and what is Filter ?\n","\n","By looking at the image below, we can say-\n","\n","#### Example - 1:\n","![alt text](https://miro.medium.com/max/761/1*32zCSTBi3giSApz1oQV-zA.gif)\n","\n","* We have an input image of $6*6*1$ sized.\n","* We're applying a $3*3$ filter which will perform vertical edge detection.\n","* Then we're getting an output image of $4*4*1$ sized.\n","* The $3*3$ filter needs to be multiplied with the entire input image. We know a $6*6$ matrix cannot get multiplied with a $3*3$ matrix. \n","\n"," So the multiplication process is as follows-\n","  * The Red squared shape box which is moving around the input image is showing which $3*3$ matrix of the $6*6$ sized input image should get multiplied with the $3*3$ filter.\n","  * At the beginning, the red squared box is covering a $3*3$ matrix on the top left corner of the input i.e. \n","$\\begin{bmatrix}\n","  10 & 10 & 10 \\\\\n","  10 & 10 & 10 \\\\\n","  10 & 10 & 10\n"," \\end{bmatrix}$\n"," * Then the filter gets multiplied with the above matrix, i.e.-\n"," $$\\begin{bmatrix}\n","  10 & 10 & 10 \\\\\n","  10 & 10 & 10 \\\\\n","  10 & 10 & 10\n"," \\end{bmatrix} * \\begin{bmatrix}\n","  1 & 2 & 1 \\\\\n","  0 & 0 & 0 \\\\\n","  -1 & -2 & -1\n"," \\end{bmatrix}$$\n"," * So we should get a 3*3 matrix after applying the above multiplication, but that is not the case with Convolution. The output which will get gets flattened out, i.e. $$[10*1+10*0+10*(-1)]+[10*2+10*0+10*(-2)]+[10*1+10*0+10*(-1)] = 0$$\n"," * So the **Convolution Operation** looks like this - \n","  $$\\begin{bmatrix}\n","  10 & 10 & 10 \\\\\n","  10 & 10 & 10 \\\\\n","  10 & 10 & 10\n"," \\end{bmatrix} * \\begin{bmatrix}\n","  1 & 2 & 1 \\\\\n","  0 & 0 & 0 \\\\\n","  -1 & -2 & -1\n"," \\end{bmatrix} = 0$$\n"," As the Red square moves right, the next matrix gets multiplied with the filter and returns an output of $-20$. Like this the entire convolution operation gets executed.\n"," * Now the input is of $6*6$ sized but the output is of $4*4$. That is because we are converting a $3*3$ matrix to a single number. These single numbers then form the output matrix. \n"," \n","    **If the input is of $n*n$ sized and the filter is of $f*f$ sized, then the output will be of $(n-f+1)*(n-f+1)$ sized**.\n","\n","#### Example - 2:\n","![alt text](http://media5.datahacker.rs/2018/10/multiplication_slicice.png)\n","\n","In this example, we can see-\n","* The first 3 coloumns on the left side of the input images are $>0$ so they are bright & and the rest are $=0$ so they are dark.\n","* Now after applying vertical filter, the output which we get shows us the extreme left and extreme right portions are dark and the middle portion is bright. \n","\n","  That bright section is nothing but the transition point of input image that creates an edge between the bright and the dark portion.\n"]},{"cell_type":"markdown","metadata":{"id":"HTOPo2fzh4vc"},"source":["# Padding\n","\n","As we've seen earlier, after applying a $3*3$ filter to an image of $6*6$, the output we got was of $4*4$.\n","\n","The details from edges of the input images were lost in this process. In a big scale convolution process this may cause a huge data loss. So to overcome this problem, we use **Padding**.\n","\n","![alt text](https://cdn-media-1.freecodecamp.org/images/d0ufdQE7LHA43cdSrVefw2I9DFceYMixqoZJ)\n","\n","Padding adds an additional layer with the input image.\n","\n","In the above example, we can see-\n","* The original input image which is defined by solid blue colour is of $5*5$ sized. \n","* An additional layer i.e. Padding has been added with that input image which is defined by dotted lines. Each square of that layer contains $0$.\n","* By using this additional layer, now the size of the input image is $8*8$ and after applying a $3*3$ filter the size of the output becomes $6*6$.\n","\n","  So, initially the size of the input was $6*6$ and after applying filter the size still remains the same. \n","\n","  Thus we get all the details in the output that the input image was featuring.\n","* Padding is denoted by $p$. In this case we've added only 1 additional layer, so $p=1$. \n","\n","  In some cases $p>1$, so- \n","\n","  **If the input is of $n*n$ sized and the filter is of $f*f$ sized & padding is $p$ then the output will be of $(n+2p-f+1)*(n+2p-f+1)$ sized**.\n","\n","### The main benefits of padding are the following:\n","\n","1. It allows us to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as we go to deeper layers. An important special case is the \"same\" convolution, in which the height/width is exactly preserved after one layer.\n","\n","2. It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.\n","\n","### How to choose how much padding we need?\n","Suppose, $n+2p-f+1 = n$\n","\n","Then, $p = \\frac{f-1}{2}$\n","\n","* If our filter is of $3*3$ shaped, then $f=3$ & $p = \\frac{3-1}{2}=1$\n","* If our filter is of $5*5$ shaped, then $f=5$ & $p = \\frac{5-1}{2}=2$\n","\n","In convolution, $f$ is always an odd number.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4TRx7N51jE08"},"source":["# Stride Convolution:\n","\n","![alt text](https://miro.medium.com/max/2130/1*itcofCIVsGe7rBmciJcmVw.gif)\n","\n","Previously while using convolution operation, the square was shifting to the 2nd coloumn after executing convolution with the matrix which is considered from the first coloumn. \n","\n","After using Stride($s$), it'll be shifted to 3rd coloumn directly after executing convolution with the matrix which is considered from the first coloumn. In this case $s=2$.\n","\n","In this case($s=2$) the convolution process starts from $\\begin{bmatrix}\n","  1 & 9 & 8 \\\\\n","  4 & 8 & 6 \\\\\n","  4 & 0 & 5\n"," \\end{bmatrix}$ and then it jumps directly to $\\begin{bmatrix}\n","  8 & 4 & 4 \\\\\n","  6 & 7 & 9 \\\\\n","  5 & 9 & 3\n"," \\end{bmatrix}$ i.e. the 3rd coloumn.\n","\n","**If we apply convolution on an image of $n*n$ with the filter of $f*f$, padding $p$ & stride $s$, we'll get the o/p of $\\frac{n+2p-f}s +1* \\frac{n+2p-f}s +1$ matrix**."]},{"cell_type":"markdown","metadata":{"id":"Lj7OvPGz5p8B"},"source":["# Convolutions on RGB image\n","\n","![alt text](https://i2.wp.com/ramok.tech/wp-content/uploads/2018/09/RGBConvolve.png?resize=840%2C324)\n","\n","In the above example we can see that there is an RGB image of $6*6*3$ matrix, which has 3 layers of colours - Red, Green, Blue. \n","* The input matrix at the top representing Red layer.\n","* The input matrix at the middle representing Green layer.\n","* The input matrix at the bottom representing Blue layer.\n","* Horizontal edge detection filter is going to be applied in these layers.\n","* **If the input image is RGB then the filter should also have 3 layers**\n","* Those 3 layers of filter will be seperately applied in each of the layers of input image.\n","* After applying the horizontal edge detection filter on the input layers, we'll have outputof 3 matrix each for red, green & blue layers.\n","* After applying Matrix Addition between these 3 matrix, we'll be having the output RGB image.\n","* **In the input image the top 3 rows are bright and rest are dark for each RGB layer**.\n","* **In the output image only middle 2 rows are bright and rest are dark**. Those bright area in the middle of the output shows nothing but the transition between dark and bright side of the input image.\n","* **If we use $n*n*n_c$ sized input image and perform convolution with a filter of $f*f*n_c$ sized, where $n_c$ = number of channel(RGB), then we'll get output of $(n-f+1)*(n-f+1)*n_c^`$, where $n_c^`$ = number of channel in output i.e. the no. of filter applied**.\n","\n","  **$n_c$ of input must be equals to $n_c$ of output**. \n"]},{"cell_type":"markdown","metadata":{"id":"0C4rG1nOr6p0"},"source":["# One Layer of CNN\n","\n","![alt text](https://i2.wp.com/ramok.tech/wp-content/uploads/2018/09/RGBConvolve.png?resize=840%2C324)\n","\n","In this pic, as we've seen earlier, we're getting 3 layers of o/p for RGB layers, then we're stacking them (i.e. converting 3D matrix to 1D matrix) up into a single matrix.\n","\n","An entire layer of CNN doesn't  stop there. The 3 layers of o/p which we got, bias gets added with each 3 of them. Those matrix works as $w$ and $b$ gets added with them and forms $z$.\n","\n","We know, $a^{[1]} = g(z^{[1]})$, in this case, $g$ is ReLU function.\n","\n","* $$\\begin{bmatrix}\n","  0 & 0 & 0 & 0 \\\\\n","  735 & 735 & 735 & 735 \\\\\n","  735 & 735 & 735 & 735 \\\\\n","  0 & 0 & 0 & 0\n"," \\end{bmatrix} + [b_1] = [z_1]$$\n","  \n","  $$a^{[1]}=g(z^{[1]})$$\n"," \n","* $$\\begin{bmatrix}\n","  0 & 0 & 0 & 0 \\\\\n","  735 & 735 & 735 & 735 \\\\\n","  735 & 735 & 735 & 735 \\\\\n","  0 & 0 & 0 & 0\n"," \\end{bmatrix} + [b_2] =[z_2]$$ \n"," \n"," $$a^{[1]}=g(z^{[1]})$$\n"," \n","* $$\\begin{bmatrix}\n","  0 & 0 & 0 & 0 \\\\\n","  735 & 735 & 735 & 735 \\\\\n","  735 & 735 & 735 & 735 \\\\\n","  0 & 0 & 0 & 0\n"," \\end{bmatrix} + [b_3] = [z_3]$$\n"," \n"," $$a^{[1]}=g(z^{[1]})$$\n"," \n","where '$g$' is a **ReLU** Function & $b_1, z_1$ both are $4*4$ matrix.\n","\n","### Notations:\n","If layer $l$ is a convolution layer,\n","* $f^{[l]}$ = Filter size\n","* $p^{[l]}$ = Padding\n","* $s^{[l]}$ = Stride\n","* $n_H^{[l-1]}*n_W^{[l-1]}*n_C^{[l-1]}$ = Dimension of input image\n","* $f_H^{[l-1]}*f_W^{[l-1]}*f_C^{[l-1]}$ = Dimension of filter\n","* $n_H^{[l]}*n_W^{[l]}*n_C^{[l]}$ = Dimension of output image, where- \n","  * $n_H^{[l]}=\\frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1$\n"," * $n_W^{[l]}=\\frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1$\n"," * $n_c^{[l]}=$ Number of filters\n","* $n_H^{[l]}*n_W^{[l]}*n_C^{[l]}$ = Dimension of Activation\n","* $f_H^{[l]}*f_W^{[l]}*n_c^{[l-1]}*n_c^{[l]}$ = Dimension of Weight, where $n_c^{[l]}$ is the number of filters\n","* $n_c^{[l]}$ is the dimension of Bias.\n","\n","#### Hyperparameters in CNN:\n","Dimensions of Filter is known as **Hyperparameter**.\n","\n","####There are 3 types of layers in a convolution network-\n","1. Convolution or Conv\n","2. Pooling or Pool\n","3. Fully Connected or FC"]},{"cell_type":"markdown","metadata":{"id":"t0xQckEY2QKj"},"source":["# Simple CNN - ConvNet\n","* Suppose we have an input image of $39*39*3$ sized. So\n","  * $n_H^{[0]}=n_W^{[0]}=39$\n","  * $n_c^{[0]}=3$\n","* Now applying 10 filters, where-\n","  * $f^{[1]}=3$\n","  * $s^{[1]}=1$\n","  * $p^{[1]}=0$\n","* The first output we get after applying those filters is of $37*37*10$ sized. Where-\n","  * $n_H^{[1]}= \\frac{n_H^{[0]}+2p^{[1]}-f^{[1]}}{s^{[1]}}+1=\\frac{39+2*0-3}{1}+1=37$\n","  * $n_H^{[0]}=n_W^{[0]}$ & $n_H^{[1]}=n_W^{[1]}$\n","  * $n_c^{[1]}=10$ as we've applied 10 filters.\n","* Again applying 20 filters on the first output, where-\n","  * $f^{[2]}=5$\n","  * $s^{[2]}=2$\n","  * $p^{[2]}=0$\n","* The second output we get after applying those filters is of $17*17*20$ sized. Where-\n","  * $n_H^{[2]}= \\frac{n_H^{[1]}+2p^{[2]}-f^{[2]}}{s^{[2]}}+1=\\frac{37+2*0-5}{2}+1=17$\n","  * $n_H^{[1]}=n_W^{[1]}$ & $n_H^{[2]}=n_W^{[2]}$\n","  * $n_c^{[2]}=20$ as we've applied 20 filters.\n","* Again applying 40 filters on the second output, where-\n","  * $f^{[3]}=5$\n","  * $s^{[3]}=2$\n","  * $p^{[3]}=0$\n","* The third output we get after applying those filters is of $7*7*40$ sized. Where-\n","  * $n_H^{[3]}= \\frac{n_H^{[2]}+2p^{[3]}-f^{[3]}}{s^{[3]}}+1=\\frac{17+2*0-5}{2}+1=7$\n","  * $n_H^{[2]}=n_W^{[2]}$ & $n_H^{[3]}=n_W^{[3]}$\n","  * $n_c^{[3]}=40$ as we've applied 40 filters.\n","* Then we flatten out this 3D matrix to 1D matrix, i.e. $7*7*40=1960$ features.\n","* Then we use Softmax or Logistic Regression to classify objects.\n"]},{"cell_type":"markdown","metadata":{"id":"nKA9JSOW_oW1"},"source":["# Pooling\n","## Maxpooling:\n","\n","The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n","\n","![alt text](https://developers.google.com/machine-learning/practica/image-classification/images/maxpool_animation.gif)\n","\n","In the above example-\n","* Hyperparameters $f=2$ & $s=2$. In this case, Hyperparameters are fixed and doesn't change while performing Gradient Descent.\n","* The maximum weighted number is selected from the submatrix of input image where the filter is applied.\n","  Like in this case, \n","  * $8$ is selected from $\\begin{bmatrix}\n","  7 & 3 \\\\\n","  8 & 7 \\\\\n"," \\end{bmatrix}$\n","  * $6$ is selected from $\\begin{bmatrix}\n","  5 & 2 \\\\\n","  1 & 6 \\\\\n"," \\end{bmatrix}$\n","  * $9$ is selected from $\\begin{bmatrix}\n","  4 & 9 \\\\\n","  0 & 8 \\\\\n"," \\end{bmatrix}$\n","  * $9$ is selected from $\\begin{bmatrix}\n","  3 & 9 \\\\\n","  4 & 5 \\\\\n"," \\end{bmatrix}$\n","* These selected numbers form the output matrix, i.e.$\\begin{bmatrix}\n","  8 & 6 \\\\\n","  9 & 9 \\\\\n"," \\end{bmatrix}$\n"," \n","## Average pooling:\n","\n","The average weighted number is selected from the submatrix of input image where the filter is applied.\n","  Like in this case, \n","  * $6.25$ will be selected from $\\begin{bmatrix}\n","  7 & 3 \\\\\n","  8 & 7 \\\\\n"," \\end{bmatrix}$\n","  * $3.5$ is selected from $\\begin{bmatrix}\n","  5 & 2 \\\\\n","  1 & 6 \\\\\n"," \\end{bmatrix}$\n","  * $5.25$ is selected from $\\begin{bmatrix}\n","  4 & 9 \\\\\n","  0 & 8 \\\\\n"," \\end{bmatrix}$\n","  * $5.25$ is selected from $\\begin{bmatrix}\n","  3 & 9 \\\\\n","  4 & 5 \\\\\n"," \\end{bmatrix}$\n","* These selected numbers form the output matrix, i.e.$\\begin{bmatrix}\n","  6.25 & 3.5 \\\\\n","  5.25 & 5.25 \\\\\n"," \\end{bmatrix}$\n"," \n","#### Another example of Maxpooling and Average Pooling:\n","\n","![alt text](https://qph.fs.quoracdn.net/main-qimg-cf2833a40f946faf04163bc28517959c)"]},{"cell_type":"markdown","metadata":{"id":"wiehqZcwDXdr"},"source":["# Convolution with Pooling\n","![alt text](https://qph.fs.quoracdn.net/main-qimg-07e35ae21eb9cda29813a0683545eb25)\n","\n","* Suppose we have an input image of $28*28$ pixels.\n","* We're applying our first convolution layer of 10 filters(Kernel = Filter) $5*5$ pixels and the output is $24*24$\n","* Then we're applying our first pooling layer of 10 filters of $2*2$ pixels and the output is $12*12$.\n","* Then we're applying our second convolution layer of 20 filters of $5*5$ pixels and the output becomes $8*8$.\n","* Again we're applying our second pooling layer of 20 filters of $2*2$ pixels and the output becomes $4*4$.\n","* Then we are applying Fully Connected Dropout, where the 'KeepProb' = $0.25$ to get rid of those redundant data that the final output have.\n","* Then we're applying Softmax classification to classify our targets.\n"]},{"cell_type":"markdown","metadata":{"id":"h3xoJmOBCAhd"},"source":["# CNN - Application\n","\n","This SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.\n","\n","![alt text](https://raw.githubusercontent.com/tejaslodaya/tensorflow-signs-nn/master/signs_dataset.png)\n","\n","## Creating a model that can tell us which sign is showing what number:"]},{"cell_type":"markdown","metadata":{"id":"IGG4XSm1CJnx"},"source":["### Importing Libraries:"]},{"cell_type":"code","metadata":{"id":"pyair9wDeRsM"},"source":["import math\n","import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.python.framework import ops"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUz6k6fqCNhN"},"source":["### Loading the dataset:"]},{"cell_type":"code","metadata":{"id":"EuVaFnBleU17"},"source":["def load_dataset():\n","    train_dataset = h5py.File('train_signs.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) \n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n","\n","    test_dataset = h5py.File('test_signs.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n","    classes = np.array(test_dataset[\"list_classes\"][:])\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vycucwweYm4"},"source":["X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zqDENofRCXR1"},"source":["### Number of training and testing images:"]},{"cell_type":"code","metadata":{"id":"gaGuITX0gYzR","executionInfo":{"status":"ok","timestamp":1567228304139,"user_tz":-330,"elapsed":3484,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"2691776e-167e-42d8-be45-7a0892185ab5","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print (\"Number of training examples = \" + str(len(X_train_orig)))\n","print (\"Number of test examples = \" + str(len(X_test_orig)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples = 1080\n","Number of test examples = 120\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TTdgLMY7LW4u"},"source":["### Visualizing the raw pixel values of the 1st example of training dataset:"]},{"cell_type":"code","metadata":{"id":"UVW9lXxULM_J","executionInfo":{"status":"ok","timestamp":1567228304140,"user_tz":-330,"elapsed":3471,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"409fff0b-788c-47eb-886b-8fe59c47782c","colab":{"base_uri":"https://localhost:8080/","height":850}},"source":["X_train_orig[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[227, 220, 214],\n","        [227, 221, 215],\n","        [227, 222, 215],\n","        ...,\n","        [232, 230, 224],\n","        [231, 229, 222],\n","        [230, 229, 221]],\n","\n","       [[227, 221, 214],\n","        [227, 221, 215],\n","        [228, 221, 215],\n","        ...,\n","        [232, 230, 224],\n","        [231, 229, 222],\n","        [231, 229, 221]],\n","\n","       [[227, 221, 214],\n","        [227, 221, 214],\n","        [227, 221, 215],\n","        ...,\n","        [232, 230, 224],\n","        [231, 229, 223],\n","        [230, 229, 221]],\n","\n","       ...,\n","\n","       [[119,  81,  51],\n","        [124,  85,  55],\n","        [127,  87,  58],\n","        ...,\n","        [210, 211, 211],\n","        [211, 212, 210],\n","        [210, 211, 210]],\n","\n","       [[119,  79,  51],\n","        [124,  84,  55],\n","        [126,  85,  56],\n","        ...,\n","        [210, 211, 210],\n","        [210, 211, 210],\n","        [209, 210, 209]],\n","\n","       [[119,  81,  51],\n","        [123,  83,  55],\n","        [122,  82,  54],\n","        ...,\n","        [209, 210, 210],\n","        [209, 210, 209],\n","        [208, 209, 209]]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"tyidXuozCgaY"},"source":["### Showing the data samples and their corresponding labels(`y`)"]},{"cell_type":"code","metadata":{"id":"kjvlXqkEee3M"},"source":["def show_data(index):\n","  plt.imshow(X_train_orig[index])\n","  return print (\"Label of the image(y) = \" + str(np.squeeze(Y_train_orig[:, index])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QaMKgo93D1mx","executionInfo":{"status":"ok","timestamp":1567228304142,"user_tz":-330,"elapsed":3448,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"74428a0c-417b-481b-d179-a7225ff654e9","colab":{"base_uri":"https://localhost:8080/","height":286}},"source":["show_data(100)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Label of the image(y) = 3\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"4viUeOayLrzs"},"source":["### Data pre-processing:\n","Scaling the data to keep all the pixel values between 0 and 1."]},{"cell_type":"code","metadata":{"id":"Na6kFfcrLpei"},"source":["X_train = X_train_orig/255.\n","X_test = X_test_orig/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GF0RJP0PME0z"},"source":["### Visualizing scalled pixel data from the 1st example of training dataset:"]},{"cell_type":"code","metadata":{"id":"rAv98HCSL9O4","executionInfo":{"status":"ok","timestamp":1567228304143,"user_tz":-330,"elapsed":3412,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"e31ba44a-412a-4004-96eb-16763b8a225e","colab":{"base_uri":"https://localhost:8080/","height":850}},"source":["X_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.89019608, 0.8627451 , 0.83921569],\n","        [0.89019608, 0.86666667, 0.84313725],\n","        [0.89019608, 0.87058824, 0.84313725],\n","        ...,\n","        [0.90980392, 0.90196078, 0.87843137],\n","        [0.90588235, 0.89803922, 0.87058824],\n","        [0.90196078, 0.89803922, 0.86666667]],\n","\n","       [[0.89019608, 0.86666667, 0.83921569],\n","        [0.89019608, 0.86666667, 0.84313725],\n","        [0.89411765, 0.86666667, 0.84313725],\n","        ...,\n","        [0.90980392, 0.90196078, 0.87843137],\n","        [0.90588235, 0.89803922, 0.87058824],\n","        [0.90588235, 0.89803922, 0.86666667]],\n","\n","       [[0.89019608, 0.86666667, 0.83921569],\n","        [0.89019608, 0.86666667, 0.83921569],\n","        [0.89019608, 0.86666667, 0.84313725],\n","        ...,\n","        [0.90980392, 0.90196078, 0.87843137],\n","        [0.90588235, 0.89803922, 0.8745098 ],\n","        [0.90196078, 0.89803922, 0.86666667]],\n","\n","       ...,\n","\n","       [[0.46666667, 0.31764706, 0.2       ],\n","        [0.48627451, 0.33333333, 0.21568627],\n","        [0.49803922, 0.34117647, 0.22745098],\n","        ...,\n","        [0.82352941, 0.82745098, 0.82745098],\n","        [0.82745098, 0.83137255, 0.82352941],\n","        [0.82352941, 0.82745098, 0.82352941]],\n","\n","       [[0.46666667, 0.30980392, 0.2       ],\n","        [0.48627451, 0.32941176, 0.21568627],\n","        [0.49411765, 0.33333333, 0.21960784],\n","        ...,\n","        [0.82352941, 0.82745098, 0.82352941],\n","        [0.82352941, 0.82745098, 0.82352941],\n","        [0.81960784, 0.82352941, 0.81960784]],\n","\n","       [[0.46666667, 0.31764706, 0.2       ],\n","        [0.48235294, 0.3254902 , 0.21568627],\n","        [0.47843137, 0.32156863, 0.21176471],\n","        ...,\n","        [0.81960784, 0.82352941, 0.82352941],\n","        [0.81960784, 0.82352941, 0.81960784],\n","        [0.81568627, 0.81960784, 0.81960784]]])"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"va1SBvXjlV1C"},"source":["### Performing One hot encoding:\n","#### One hot encoding:\n","A one hot encoding is a representation of categorical variables as binary vectors.\n","\n","This first requires that the categorical values be mapped to integer values.\n","\n","Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n","\n","If we have 3 classes(`'red', 'red', 'green'`) in any problem, then we can represent them as `[0,0,1]`.\n","\n","But in One-hot encoding, this is represented as,\n","\n","`[1,0,0]` for `red`\n","\n","`[1,0,0]` for `red`\n","\n","`[0,0,1]` for `green`\n","\n","#### Why Use a One Hot Encoding?\n","\n","Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical.\n","\n","We could use an integer encoding directly, rescaled where needed. This may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.\n","\n","There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels ‘dog’ and ‘cat’\n","\n","In these cases, we would like to give the network more expressive power to learn a probability-like number for each possible label value. This can help in both making the problem easier for the network to model. When a one hot encoding is used for the output variable, it may offer a more nuanced set of predictions than a single label.\n","\n"]},{"cell_type":"code","metadata":{"id":"btcf8UG1hcJ-"},"source":["def convert_to_one_hot(Y, C):\n","    Y = np.eye(C)[Y.reshape(-1)].T\n","    return Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OikJ5RfMWXt"},"source":["Y_train = convert_to_one_hot(Y_train_orig, 6).T\n","Y_test = convert_to_one_hot(Y_test_orig, 6).T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izjLzlX6Mcb7","executionInfo":{"status":"ok","timestamp":1567228304146,"user_tz":-330,"elapsed":3380,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"dfc6b7bf-2c35-4f7a-b533-02e684ba0ed0","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print(\"Result of One-hot encoding of 1st training examples \" + str(Y_train[0]))\n","print(\"Result of One-hot encoding of 1st test examples \" + str(Y_test[0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Result of One-hot encoding of 1st training examples [0. 0. 0. 0. 0. 1.]\n","Result of One-hot encoding of 1st test examples [1. 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UapalZJDlbUt"},"source":["### Data Preprocessing:\n","Scaling the Train and Test data between 0 to 1, so dividing these data with 255."]},{"cell_type":"code","metadata":{"id":"EP6NoULhgMS8","executionInfo":{"status":"ok","timestamp":1567228304146,"user_tz":-330,"elapsed":3369,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"60e951c3-73a1-4aaf-918e-accd5cb2ff1d","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["print (\"number of training examples = \" + str(X_train.shape[0]))\n","print (\"number of test examples = \" + str(X_test.shape[0]))\n","print (\"X_train shape: \" + str(X_train.shape))\n","print (\"Y_train shape: \" + str(Y_train.shape))\n","print (\"X_test shape: \" + str(X_test.shape))\n","print (\"Y_test shape: \" + str(Y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["number of training examples = 1080\n","number of test examples = 120\n","X_train shape: (1080, 64, 64, 3)\n","Y_train shape: (1080, 6)\n","X_test shape: (120, 64, 64, 3)\n","Y_test shape: (120, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"koH67SfmZ8dG"},"source":["### Creating Tensorflow placeholders:\n","\n","    Arguments:\n","    n_H0 -- scalar, height of an input image\n","    n_W0 -- scalar, width of an input image\n","    n_C0 -- scalar, number of channels of the input\n","    n_y -- scalar, number of classes\n","        \n","    Returns:\n","    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype \"float\"\n","    Y -- placeholder for the input labels, of shape [None, n_y] and dtype \"float"]},{"cell_type":"code","metadata":{"id":"kTlB3gr9hVAe"},"source":["def create_placeholders(n_H0, n_W0, n_C0, n_y):\n","\n","    X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0])\n","    Y = tf.placeholder(tf.float32, [None, n_y])\n","    \n","    return X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpLyVhgsh6Wh","executionInfo":{"status":"ok","timestamp":1567228304148,"user_tz":-330,"elapsed":3348,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"c5ee270e-4c64-4dd9-c4dc-d6ecfe37847f","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["X, Y = create_placeholders(64, 64, 3, 6)\n","print (\"X = \" + str(X))\n","print (\"Y = \" + str(Y))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X = Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n","Y = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GP9CPOKRa3r7"},"source":["### Initializing Parameters:\n","    \n","    \n","    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n","                        W1 : [4, 4, 3, 8]\n","                        W2 : [2, 2, 8, 16]\n","    Returns:\n","    parameters -- a dictionary of tensors containing W1, W2\n","    "]},{"cell_type":"code","metadata":{"id":"I1FjXugch8oN"},"source":["def initialize_parameters():\n","    \n","    tf.set_random_seed(1)\n","\n","    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n","    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n","\n","    parameters = {\"W1\": W1,\n","                  \"W2\": W2}\n","    \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xbncQNwiBq2","executionInfo":{"status":"ok","timestamp":1567228306842,"user_tz":-330,"elapsed":6020,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"614859f2-6d7f-4511-8bb7-cf2d802bfbf8","colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["tf.reset_default_graph()\n","with tf.Session() as sess_test:\n","    parameters = initialize_parameters()\n","    init = tf.global_variables_initializer()\n","    sess_test.run(init)\n","    print(\"W1 = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n","    print(\"W2 = \" + str(parameters[\"W2\"].eval()[1,1,1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W0831 05:11:44.404834 139727774410624 lazy_loader.py:50] \n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["W1 = [ 0.00131723  0.1417614  -0.04434952  0.09197326  0.14984085 -0.03514394\n"," -0.06847463  0.05245192]\n","W2 = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n"," -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n"," -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GDpCQj0XbTV5"},"source":["### Calculating Forward Propagation:\n","    \n","**Implementing the forward propagation for the model**:\n","\n","**CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED**\n","    \n","    Arguments:\n","    X -- input dataset placeholder, of shape (input size, number of examples)\n","    parameters -- python dictionary containing our parameters \"W1\", \"W2\"\n","                  the shapes are given in initialize_parameters\n","\n","    Returns:\n","    Z3 -- the output of the last LINEAR unit\n","    "]},{"cell_type":"code","metadata":{"id":"HS4Uf8JJiO4e"},"source":["def forward_propagation(X, parameters):\n","    \n","    # Retrieve the parameters from the dictionary \"parameters\" \n","    W1 = parameters['W1']\n","    W2 = parameters['W2']\n","    \n","    # CONV2D: stride of 1, padding 'SAME'\n","    Z1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n","    # RELU\n","    A1 = tf.nn.relu(Z1)\n","    # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n","    P1 = tf.nn.max_pool(A1, ksize = [1, 8, 8, 1], strides = [1, 8, 8, 1], padding='SAME')\n","    # CONV2D: filters W2, stride 1, padding 'SAME'\n","    Z2 = tf.nn.conv2d(P1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","    # RELU\n","    A2 = tf.nn.relu(Z2)\n","    # MAXPOOL: window 4x4, stride 4, padding 'SAME'\n","    P2 = tf.nn.max_pool(A2, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding='SAME')\n","    # FLATTEN\n","    P2 = tf.contrib.layers.flatten(P2)\n","    # FULLY-CONNECTED without non-linear activation function\n","    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \n","    Z3 = tf.contrib.layers.fully_connected(P2, 6, activation_fn=None)\n","\n","    return Z3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5D03ss_qiqyB","executionInfo":{"status":"ok","timestamp":1567228306845,"user_tz":-330,"elapsed":6001,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"66846bda-1e8f-4d3f-ce11-c80ba9ee6cca","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["tf.reset_default_graph()\n","\n","with tf.Session() as sess:\n","    np.random.seed(1)\n","    X, Y = create_placeholders(64, 64, 3, 6)\n","    parameters = initialize_parameters()\n","    Z3 = forward_propagation(X, parameters)\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    a = sess.run(Z3, {X: np.random.randn(2,64,64,3), Y: np.random.randn(2,6)})\n","    print(\"Z3 = \" + str(a))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W0831 05:11:44.497904 139727774410624 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n"],"name":"stderr"},{"output_type":"stream","text":["Z3 = [[ 1.4416982  -0.24909675  5.4504995  -0.26189643 -0.2066989   1.3654672 ]\n"," [ 1.4070848  -0.02573231  5.0892797  -0.48669893 -0.40940714  1.2624854 ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UQABvhFvbwBG"},"source":["### Calculating Cost:\n","   \n","    Arguments:\n","    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (number of examples, 6)\n","    Y -- \"true\" labels vector placeholder, same shape as Z3\n","    \n","    Returns:\n","    cost - Tensor of the cost function"]},{"cell_type":"code","metadata":{"id":"t3t74Ty2itev"},"source":["def compute_cost(Z3, Y):\n","    \n","    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMa7U3Tmi6eQ","executionInfo":{"status":"ok","timestamp":1567228306846,"user_tz":-330,"elapsed":5980,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"c15ff644-f50b-4ed6-9cd9-d40fef5ae394","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["tf.reset_default_graph()\n","\n","with tf.Session() as sess:\n","    np.random.seed(1)\n","    X, Y = create_placeholders(64, 64, 3, 6)\n","    parameters = initialize_parameters()\n","    Z3 = forward_propagation(X, parameters)\n","    cost = compute_cost(Z3, Y)\n","    init = tf.global_variables_initializer()\n","    sess.run(init)\n","    a = sess.run(cost, {X: np.random.randn(4,64,64,3), Y: np.random.randn(4,6)})\n","    print(\"cost = \" + str(a))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W0831 05:11:44.969911 139727774410624 deprecation.py:323] From <ipython-input-72-23835f61c9cf>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["cost = 4.6648703\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"df7arWoicBQD"},"source":["### Creating Mini-Batches:\n","Creates a list of random minibatches from (X, Y)\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n","    mini_batch_size - size of the mini-batches, integer\n","    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n","    \n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    "]},{"cell_type":"code","metadata":{"id":"XJsoNyX_jU3a"},"source":["def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n","\n","    \n","    m = X.shape[0]  # number of training examples\n","    mini_batches = []\n","    np.random.seed(seed)\n","    \n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[permutation,:,:,:]\n","    shuffled_Y = Y[permutation,:]\n","\n","    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n","    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n","        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    # Handling the end case (last mini-batch < mini_batch_size)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n","        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    return mini_batches"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DvehkYocUQl"},"source":["### Creating the model:\n","\n","**Implementing a three-layer ConvNet in Tensorflow**:\n","\n","**CONV2D -> RELU -> MAXPOOL -> CONV2D -> RELU -> MAXPOOL -> FLATTEN -> FULLYCONNECTED**\n","    \n","    Arguments:\n","    X_train -- training set, of shape (None, 64, 64, 3)\n","    Y_train -- test set, of shape (None, n_y = 6)\n","    X_test -- training set, of shape (None, 64, 64, 3)\n","    Y_test -- test set, of shape (None, n_y = 6)\n","    learning_rate -- learning rate of the optimization\n","    num_epochs -- number of epochs of the optimization loop\n","    minibatch_size -- size of a minibatch\n","    print_cost -- True to print the cost every 100 epochs\n","    \n","    Returns:\n","    train_accuracy -- real number, accuracy on the train set (X_train)\n","    test_accuracy -- real number, testing accuracy on the test set (X_test)\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\""]},{"cell_type":"code","metadata":{"id":"hWUWarD2i9Mx"},"source":["def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\n","          num_epochs = 1000, minibatch_size = 64, print_cost = True):\n","    \n","    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n","    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n","    seed = 3                                          # to keep results consistent (numpy seed)\n","    (m, n_H0, n_W0, n_C0) = X_train.shape             \n","    n_y = Y_train.shape[1]                            \n","    costs = []                                        # To keep track of the cost\n","    \n","    # Create Placeholders of the correct shape\n","    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n","\n","    # Initialize parameters\n","    parameters = initialize_parameters()\n","    \n","    # Forward propagation: Build the forward propagation in the tensorflow graph\n","    Z3 = forward_propagation(X, parameters)\n","    \n","    # Cost function: Add cost function to tensorflow graph\n","    cost = compute_cost(Z3, Y)\n","    \n","    # Backpropagation: Define the tensorflow optimizer. Using an AdamOptimizer that minimizes the cost.\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n","    \n","    # Initialize all the variables globally\n","    init = tf.global_variables_initializer()\n","     \n","    # Start the session to compute the tensorflow graph\n","    with tf.Session() as sess:\n","        \n","        # Run the initialization\n","        sess.run(init)\n","        \n","        for epoch in range(num_epochs):\n","\n","            minibatch_cost = 0.\n","            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n","            seed = seed + 1\n","            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n","\n","            for minibatch in minibatches:\n","\n","                # Selecting a minibatch\n","                (minibatch_X, minibatch_Y) = minibatch\n","                # Running the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n","                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n","                \n","                minibatch_cost += temp_cost / num_minibatches\n","                \n","\n","            # Print the cost every epoch\n","            if print_cost == True and epoch % 100 == 0:\n","                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n","            if print_cost == True and epoch % 1 == 0:\n","                costs.append(minibatch_cost)\n","        \n","        \n","        # plot the cost\n","        plt.plot(np.squeeze(costs))\n","        plt.ylabel('cost')\n","        plt.xlabel('iterations (per hundreds)')\n","        plt.title(\"Learning rate =\" + str(learning_rate))\n","        plt.show()\n","\n","        # Calculate the correct predictions\n","        predict_op = tf.argmax(Z3, 1)\n","        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n","        \n","        # Calculate accuracy\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n","        print(accuracy)\n","        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n","        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n","        print(\"Train Accuracy:\", train_accuracy)\n","        print(\"Test Accuracy:\", test_accuracy)\n","                \n","        return train_accuracy, test_accuracy, parameters"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kX0QDx3DcsT-"},"source":["### Training the Model:"]},{"cell_type":"code","metadata":{"id":"jqpqr75xjV_k","executionInfo":{"status":"ok","timestamp":1567228453895,"user_tz":-330,"elapsed":152992,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"df9c6fb6-edc5-4982-9838-f016995a7b28","colab":{"base_uri":"https://localhost:8080/","height":516}},"source":["_, _, parameters = model(X_train, Y_train, X_test, Y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cost after epoch 0: 1.921332\n","Cost after epoch 100: 0.997713\n","Cost after epoch 200: 0.767211\n","Cost after epoch 300: 0.661776\n","Cost after epoch 400: 0.599643\n","Cost after epoch 500: 0.573793\n","Cost after epoch 600: 0.582103\n","Cost after epoch 700: 0.567776\n","Cost after epoch 800: 0.582168\n","Cost after epoch 900: 0.534871\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd8FHX6wPHPkx5ISCiRLlVARAFF\nQbFgx3YqPz317I3zLHennh6eZy/nnXp69oqcnmJvFCuCKCASOoQWemgJLb1t8vz+mNllk2waZLPJ\n5nm/Xnm5M/Pd2Wd2cZ75lvmOqCrGGGMMQESoAzDGGNN0WFIwxhjjY0nBGGOMjyUFY4wxPpYUjDHG\n+FhSMMYY42NJwYQlEflKRK4OdRzGNDeWFEyDEpENInJaqONQ1bNU9b+hjgNARGaIyA2N8DmxIjJe\nRHJEZLuI3FFL+dvdcjnu+2L9tvUUkekiUiAiK/1/U/dznhGRrSKyR0ReEpHoYB6baTyWFEyzIyJR\noY7BqynFAjwIHAL0AE4G7haR0YEKisiZwDjgVLd8b+AhvyITgYVAe+Be4GMRSXG3jQOGAYOAfsCR\nwN8b+FhMqKiq/dlfg/0BG4DTqtl2LrAI2AvMBo7w2zYOWAvkAmnAhX7brgFmAc8Au4BH3XU/A08B\ne4D1wFl+75kB3OD3/prK9gJmup/9PfAi8L9qjmEUkAH8FdgOvAO0BSYDWe7+JwPd3PKPAWVAEZAH\nvOCuHwB8B+wGVgG/bYDvfitwht/yI8D71ZR9D3jcb/lUYLv7uh9QDCT6bf8JuMl9nQpc7Lftd8Dm\nUP/bs7+G+bOagmkUIjIUGA/8Hufq81XgS78mi7XACUASzhXr/0Sks98uhgPrgI44J1rvulVAB+Bf\nwJsiItWEUFPZ94Bf3bgeBK6s5XA6Ae1wrrDH4tS433KXDwYKgRcAVPVenBPqraqaoKq3ikhrnITw\nHnAQcCnwkogMDPRhbvPM3mr+lrhl2gKdgcV+b10MHFbNMRwWoGxHEWnvblunqrk17Esqve4mIknV\nfJZpRiwpmMYyFnhVVeeqapk67f3FwAgAVf1IVbeqarmqfgCsAY7xe/9WVX1eVT2qWuiu26iqr6tq\nGfBfnJNix2o+P2BZETkYOBq4X1VLVPVn4MtajqUceEBVi1W1UFV3qeonqlrgnkgfA06q4f3nAhtU\n9S33eBYCnwAXByqsqjeranI1f0e4xRLc/2b7vTUbSKwmhoQAZXHLV95WeV9fA38SkRQR6QT80V3f\nqtojNs1GU2oPNeGtB3C1iNzmty4G6AIgIlcBdwA93W0JOFf1XpsD7HO794WqFrgX/gkBytVUtgOw\nW1ULKn1W9xqOJUtVi7wLItIKp2lrNE5TEkCiiES6SaiyHsBwEdnrty4Kpylqf+W5/22D01TlfZ0b\nuDh57nb8yuKWr7yt8r4eA5JxmgKLgdeBocCO/YzdNCFWUzCNZTPwWKWr3FaqOlFEeuCcWG4F2qtq\nMrCMik0UwZrOdxvQzj2xe9WUEALFcifQHxiuqm2AE931Uk35zcCPlb6LBFX9Q6APE5FXRCSvmr/l\nAKq6xz2WwX5vHQwsr+YYlgcou0NVd7nbeotIYqXt3s8qVNVbVbWrqvbG6eeZr6rl1XyWaUYsKZhg\niBaROL+/KJyT/k0iMlwcrUXkHPfE0xrnxJkFICLX4oxsCTpV3YjTcfqgiMSIyLHAefXcTSJOP8Je\nEWkHPFBp+w6c0T1ek4F+InKliES7f0eLyKHVxHiTmzQC/fm3878N/F1E2orIAOBGYEI1Mb8NXC8i\nA0UkGWf00AT381bj1AIecH+/C4EjcJq4EJGuItLF/R1HAPcFOGbTTFlSMMEwFeck6f17UFVTcU5S\nL+CM0EnHGRWEqqYBTwNzcE6gh+OMNmoslwPHsm9k0wc4zSJ19SwQD+wEfsFpc/f3H+Aid0z/c26/\nwxk4HcxbcZq2/gnEcmAewOmw3wj8CDypql8DiMjBbs3iYAB3/b+A6cAm9z3+J/ZLcYad7gGeAC5S\n1Sx3Wx+c0WP5OP0z41T12wOM3TQRomoP2THGn4h8AKxUVbv6NS2O1RRMi+c23fQRkQj3Zq/zgc9D\nHZcxoWCjj4xx7jv4FOc+hQzgD+4wUWNaHGs+MsYY42PNR8YYY3yaXfNRhw4dtGfPnqEOwxhjmpX5\n8+fvVNWU2so1u6TQs2dPUlNTQx2GMcY0KyKysS7lrPnIGGOMjyUFY4wxPpYUjDHG+FhSMMYY42NJ\nwRhjjI8lBWOMMT6WFIwxxvi0mKSwcnsO//x6JdmFpaEOxRhjmqwWkxQ27Srg5RlrWb8zP9ShGGNM\nk9Xs7mjeXz3atwbgp9VZdE2OR1E8ZUp0ZARtW0UTFdli8qMxxlSrxSSFg9u1IiYygqe/W83T362u\nsu39sSPokhwfouiMMaZpaDFJIT4mkil/PJ5Fm/dSVFqGiBAVIWTsKeTVmWu5/4tlvHH10aEO0xhj\nQqrFJAWAQzomckjHxCrrt2YXMmNVVoB3GGNMy2IN6UDX5Hj2FpRQXm4PHDLGtGyWFIDkVjGUK+QU\n2XBVY0zLZkkBaNsqGoDd+SUhjsQYY0LLkgLQOtbpWikoKQtxJMYYE1qWFIAY9x6F0rLyEEdijDGh\nZUkBiIlyvoYSjyUFY0zLZkkBiPbVFGz0kTGmZbOkwL6agjUfGWNaOksKQHSkAFBszUfGmBYuaElB\nRMaLSKaILKtme5KITBKRxSKyXESuDVYstYm1moIxxgDBrSlMAEbXsP0WIE1VBwOjgKdFJCaI8VTL\n26dgHc3GmJYuaElBVWcCu2sqAiSKiAAJbllPsOKpSbQNSTXGGCC0fQovAIcCW4GlwJ9UNeBZWUTG\nikiqiKRmZTX8xHW+IamWFIwxLVwok8KZwCKgCzAEeEFE2gQqqKqvqeowVR2WkpLS4IF4awrFpZYU\njDEtWyiTwrXAp+pIB9YDA0IRiLej+bGpK0Lx8cYY02SEMilsAk4FEJGOQH9gXSgC8SYFAFW7gc0Y\n03IFc0jqRGAO0F9EMkTkehG5SURucos8AhwnIkuBacBfVXVnsOKpJVYeOG8gALtsplRjTAsWtCev\nqepltWzfCpwRrM+vr9YxzldRVGozpRpjWi67o9kVG+12Ntu9CsaYFsySgivGbmAzxhhLCl5WUzDG\nGEsKPjGRkQAUW5+CMaYFs6Tg8tYU7K5mY0xLZknB5b1Xwe5qNsa0ZJYUXDb/kTHGWFLwiY1y+xQ8\n1qdgjGm5LCm4vM1HhSVWUzDGtFyWFFztE2IQgR05RaEOxRhjQsaSgis2KpKOiXG8OnOt3cBmjGmx\nLCn42Z5TRFFpOW/8HJLJWo0xJuQsKQSQUxiSp4IaY0zIWVLwc3zfDgAkxgVt8lhjjGnSLCn4ee2q\nowCb6sIY03JZUvDTKiaKxLgovk3bQVZucajDMcaYRmdJoZIOCbGs3J7L3z5bGupQjDGm0VlSqOSZ\nS4YAUFZuz2o2xrQ8lhQqGdI9mYGd2yChDsQYY0LAkkIA7VrHsCu/JNRhGGNMowtaUhCR8SKSKSLL\naigzSkQWichyEfkxWLHUV5+U1qRtzWHNjtxQh2KMMY0qmDWFCcDo6jaKSDLwEvAbVT0MuDiIsdTL\nTaP64Ckv563ZG0IdijHGNKqgJQVVnQnsrqHI74BPVXWTWz4zWLHUV+ekeAZ3T+a9uZsY9uh3FNl9\nC8aYFiKUfQr9gLYiMkNE5ovIVdUVFJGxIpIqIqlZWVmNEtwj5w8CYGdeCf+1GoMxpoUIZVKIAo4C\nzgHOBO4TkX6BCqrqa6o6TFWHpaSkNEpwg7om+V6X2tPYjDEtRCiTQgbwjarmq+pOYCYwOITxVHH3\n6P4AeOyeBWNMCxHKpPAFcLyIRIlIK2A4sCKE8VRx86i+dGoTx9qs/FCHYowxjSJo04GKyERgFNBB\nRDKAB4BoAFV9RVVXiMjXwBKgHHhDVasdvhoqJ/brwKTF21i1PZf+nRJDHY4xxgSVqDavppFhw4Zp\nampqo33etuxCRj05g2JPOcseOpOEWJtW2xjT/IjIfFUdVls5u6O5Fp2T4hk9qBMAT369MsTRGGNM\ncFlSqIM7TncGRa3YZnc4G2PCmyWFOujRvjXXjezFooy9diObMSasWVKoo5F921PiKWfBxj2hDsUY\nY4LGkkIdHdOrHQA3vJ2Kx25mM8aEKUsKdZQYFw1AQUkZnyzICHE0xhgTHJYU9kNRqdUUjDHhyZLC\nfti4qyDUIRhjTFBYUqiHV644CoDxs9aHOBJjjAkOSwr14L2JzRhjwpUlhXq6/vhetI6JDHUYxhgT\nFJYU6ql1bBT5JWWU23TaxpgwZEmhnhLdCfHySzwhjsQYYxqeJYV6au0mhbxiSwrGmPBjSaGe2ifE\nALAjpzjEkRhjTMOzpFBP/Ts6D9pZvd1mTDXGhB9LCvXUKSkOgKw8qykYY8KPJYV6io2KQASbQtsY\nE5YsKdSTiBAfHUlhiSUFY0z4CVpSEJHxIpIpIstqKXe0iHhE5KJgxdLQ4qMjKbSagjEmDAWzpjAB\nGF1TARGJBP4JfBvEOBpcnCUFY0yYClpSUNWZwO5ait0GfAJkBiuOYIiPibQ+BWNMWApZn4KIdAUu\nBF6uQ9mxIpIqIqlZWVnBD64WrWKsT8EYE55C2dH8LPBXVa31iTWq+pqqDlPVYSkpKY0QWs3ioiMp\nsKRgjAlDUSH87GHA+yIC0AE4W0Q8qvp5CGOqk6T4aDL2FIY6DGOMaXAhSwqq2sv7WkQmAJObQ0IA\nSI6PZvmW7FCHYYwxDS5oSUFEJgKjgA4ikgE8AEQDqOorwfrcxpDcKpq9haWhDsMYYxpc0JKCql5W\nj7LXBCuOYEiKj6agpIxiTxmxUfbAHWNM+LA7mvdDSmIsAFm5Nv+RMSa8WFLYD52T4gHYurcoxJEY\nY0zDsqSwH7q1dZLC+p15IY7EGGMaliWF/dCzfWs6JMQyd11tN2wbY0zzYklhP0RECB3bxJJtI5CM\nMWHGksJ+sknxjDHhyJLCfoqPtknxjDHhx5LCfoqLjqCotNZpm4wxplmxpLCfYqMjKfJYTcEYE14s\nKeyn+OhIiq2mYIwJM5YU9lNcdIR1NBtjwk6dkoKIXFyXdS1JXJR1NBtjwk9dawr31HFdi9G2dQwF\nJWX2BDZjTFipcZZUETkLOBvoKiLP+W1qA3iCGVhT1yU5DoCt2YX0SUkIcTTGGNMwaqspbAVSgSJg\nvt/fl8CZwQ2taevimxTPnsBmjAkfNdYUVHUxsFhE3lPVUgARaQt0V9U9jRFgU9Ul2ZKCMSb81LVP\n4TsRaSMi7YAFwOsi8kwQ42ryOiXFIQJbbPpsY0wYqWtSSFLVHGAM8LaqDgdODV5YTV90ZATtW8eS\nlWtJwRgTPuqaFKJEpDPwW2ByEONpVpJbRdtMqcaYsFLXpPAw8A2wVlXniUhvYE1NbxCR8SKSKSLL\nqtl+uYgsEZGlIjJbRAbXL/TQS4qPZm+BJQVjTPioU1JQ1Y9U9QhV/YO7vE5V/6+Wt00ARtewfT1w\nkqoeDjwCvFaXWJqS5HirKRhjwktd72juJiKfuVf+mSLyiYh0q+k9qjoTqPbRZKo6228E0y9Ajftr\nipJaRbMnvyTUYRhjTIOpa/PRWzj3JnRx/ya56xrK9cBX1W0UkbEikioiqVlZWQ34sQemT0oCW7OL\nrLZgjAkbdU0KKar6lqp63L8JQEpDBCAiJ+Mkhb9WV0ZVX1PVYao6LCWlQT62QQzqmgRA2tacEEdi\njDENo65JYZeIXCEike7fFcCuA/1wETkCeAM4X1UPeH+N7bAubQB4bebaEEdijDENo65J4Tqc4ajb\ngW3ARcA1B/LBInIw8ClwpaquPpB9hUqHhFj6HpTAmsy8UIdijDENoj5DUq9W1RRVPQgnSTxU0xtE\nZCIwB+gvIhkicr2I3CQiN7lF7gfaAy+JyCIRSd3PYwipMwZ2ZHt2EWXlGupQjDHmgNU495GfI/zn\nOlLV3SIytKY3qOpltWy/Abihjp/fZPXs0BpPufLj6kxOGdAx1OEYY8wBqWtNIcKdCA8Adw6kuiaU\nsHbO4Z0BWJKRHeJIjDHmwNX1xP40MEdEPnKXLwYeC05IzUvr2Cg6tollyx6bLdUY0/zVKSmo6ttu\nm/8p7qoxqpoWvLCalz4pCcxdvxtVRURCHY4xxuy3ujYfoappqvqC+2cJwc9Zh3dm0+4CPlmwJdSh\nGGPMAalzUjDVG+Ter/CXjxajaqOQjDHNlyWFBjCgUxvf6/kbW/QD6YwxzZwlhQYQHxPpe33RK3MA\nUFXOe/5nJi/ZGqqwjDGm3iwpNJDJtx1fYbnYU87SLdn8ceLCEEVkjDH1Z0mhgXRrG+97nZ6ZR2FJ\nGQBRkfYVG2OaDztjNZCk+Gjf6z9OXEh+iQeAGEsKxphmxM5YDUREmP6XUYDTx+B9TGd0pN23YIxp\nPiwpNKBeHVozZmhX5m/cw7nP/wxY85ExpnmxM1YDG9GnfYVlaz4yxjQndsZqYO1bx1RYtuYjY0xz\nYkmhgR3Vo22F5VYxNpmsMab5sKTQwJJbxXDzqD6+5YRYSwrGmObDkkIQ/N9R3Xyvo6Os+cgY03xY\nUgiCPikJvtelZTZBnjGm+bCkECQL7zudLklxlHjK2bq3kNnpO0MdkjHG1MqSQpC0bR3DYV2TKCwp\n46z//MTv3pgb6pCMMaZWQUsKIjJeRDJFZFk120VEnhORdBFZIiJHBiuWUEmIjWLVjlyyC527m0vL\nykMckTHG1CyYNYUJwOgatp8FHOL+jQVeDmIsIdH3oIQKy97kYIwxTVXQkoKqzgR211DkfOBtdfwC\nJItI52DFEwpjT+xdYdmSgjGmqQtln0JXYLPfcoa7rgoRGSsiqSKSmpWV1SjBNYToyAim3XmSb9mS\ngjGmqWsWHc2q+pqqDlPVYSkpKaEOp176pCT4EsPazLwQR2OMMTULZVLYAnT3W+7mrgs7vdq3pnVM\nJHd9vMSGphpjmrRQJoUvgavcUUgjgGxV3RbCeIImIkLokuw8me0P7y4IcTTGGFO9oE3MIyITgVFA\nBxHJAB4AogFU9RVgKnA2kA4UANcGK5amoJ07e2p2YSkLNu1haPdkRGwKDGNM0xK0pKCql9WyXYFb\ngvX5TU1M1L5K2ZiXZvPIBYO4ckSPEEZkjDFVNYuO5nDQrtJzFhZt2huiSIwxpnqWFBrJg+cdVmG5\nyFNWYXn0szMZ8fi0xgzJGGOqsKTQSNq2jmH1o2f5lotLnaTwwg9rGP7496zcnsv2nKJQhWeMMYAl\nhUYVExXB93ecCMDm3YWc8vQMnvp2NTtyikMcmTHGOCwpNLK+ByVy5YgerNqRy7qs/FCHY4wxFVhS\nCIHTBnYMdQjGGBOQJYUQGNmnfahDMMaYgCwphEBUZASrHh3NkO7JVbbZMxeMMaFkSSFEYqMiOWtQ\npyrrC0rKApQ2xpjGYUkhhCrf0AawK89GIhljQseSQgide0QXLhjSpcK6rXudexXKy9UShDGm0VlS\nCKH4mEievXQoqx4dzStXOI+oXrBpDwD3fLqUox79nmKPNScZYxqPJYUmIDYqktGDOnNMz3ZMXerM\nHv5BqvNQupxCTyhDM8a0MJYUmpCTBxzEyu25/PaVOb51BSWWFIwxjceSQhNy8bBuAPy6YbdvXV6x\nJQVjTOOxpNCEdEiIrTIiKb84cJ9CemYuPcdNYfWO3MYIzRjTQlhSaGKm3XESCbH7nn20YlsO/529\ngZyi0grlPp7vPM76ro8W4zyvyBhjDpwlhSambesY5txzCq9ccRQAD3y5nAe+XM7rM9f5ynjKyhn/\n83oAFmdk88PKzJDEaowJP5YUmqDEuGhG9q04P1JO4b6awsOT0yjxmw5j9Y68RovNGBPegpoURGS0\niKwSkXQRGRdg+8EiMl1EForIEhE5O5jxNCeJcdEVlr9ZvgOAn9Zk8facjRW2VW5aMsaY/RW0pCAi\nkcCLwFnAQOAyERlYqdjfgQ9VdShwKfBSsOJpjv7928HcdkpfALbnFLEjp4gr3/y1Srl8G6FkjGkg\nwawpHAOkq+o6VS0B3gfOr1RGgTbu6yRgaxDjaXbGHNmNO8/oz6kDDgJg/c7AD+VJz8xj0uLqv7qC\nEg877FGfxpg6CGZS6Aps9lvOcNf5exC4QkQygKnAbYF2JCJjRSRVRFKzsrKCEWuT9uBvDgPg/i+W\nBdw+e+0ubpu4kKLSwMNXb3w7leGPT7NRSsaYWoW6o/kyYIKqdgPOBt4RkSoxqeprqjpMVYelpKQ0\nepCh1q1tPCP7tq+1Qzm7MHDfwqz0XQA8Ny29wWMzxoSXYCaFLUB3v+Vu7jp/1wMfAqjqHCAO6BDE\nmJolEeHpi4fQJ6U1px3qNCVFRwprHjurQrkxL81m2ZbsKu/v1jYegA/mbQp+sMaYZi2YSWEecIiI\n9BKRGJyO5C8rldkEnAogIofiJIWW1z5UB52S4ph25yhevuIo/u/Ibnx280iiIyP46KZjfWW27C3k\n3Od/Zm1WxRpF/46JABzWNalRYzbGND9RtRfZP6rqEZFbgW+ASGC8qi4XkYeBVFX9ErgTeF1Ebsfp\ndL5GreG7RtGRETz928G+5aN7tqtSJmNPIYlxUfyybjd/+3QpfVJaA/YAH2NM7YKWFABUdSpOB7L/\nuvv9XqcBI4MZQ0v0yOQ00jP31Ra8fRG78ktCFZIxppkIalIwjeOLW0YSHxPJd2k7ePKbVRUSAkCh\nOyppV54lBWNMzSwphIHB3ZMB6NcxkU8WZLAuK/D9DHnFHopKy4iLjmzM8IwxzUioh6SaBta9bauA\n671TctelCWlHTpE9BtSYFsqSQph5fMzhPHhe5dlE4Ihuzsij3XkllJcrZeWB+/PLypXhj0/jLx8t\nCWqcxpimyZJCmOmaHM81I3vxyhVHVphpdcyRzlPdduYXc8t7C+j/969YsyOXF6enV7jT2TtCyfus\naGNMy2J9CmFq9KDOjB7UmZ7jpgAwpJvT73DtW/N8ZS56ZQ7ZhaWMObIrZeVKaZnyn+9XAxAXZdcL\nxrRElhTC3Oe3jCQ2KoKUxNgq27zTYqzYlsN1E1IrbMsvKeOdXzZy+TEH4ylXYuqYJIo9ZYz7ZCl3\nntGPbtX0bwTy4JfLnf+68zwZY0LDLgfD3JDuyRzauQ3xMZH8dPfJActUTghe932+jN5/m8olr82p\nsF5V+TB1M4UlTmd0YUkZd320mJ15xfy0eiefLdzCA18sr1ecE2ZvYMLsDQAs2ryX2ek76/V+Y0zD\nsKTQgnRv14plD53JzLsCJ4fqLNy0l4cnpfFf96Q9Z+0u7v54CY9PXQHAlKXb+Gh+Bv/6emWDxHnB\ni7P43RtzG2Rfxpj6seajFiYhNoqE2Cgm3jiCvGIPN74duJZQ2fhZzjOhfzusu+9Jbxt2OfdDREcK\nAAUl+4axeruuN+zMJyYqgi7J8dXuu7yakVDGmMZnNYUW6tg+7Tl9YEdO6le/qcgPvf9rsnKdEUo/\nrdnJjpwioiOdf0Y784rZXVDxPohRT83guCd+qHGfeSVN98lxkxZv5aUZNuW4aTksKbRw/73uGN66\n5mjuOrM/ANcf34v5fz+txvfc59dfcPsHi3x9C7+s283dHzv3N2QXlvKsO5KpNtU9HKgpuG3iQv71\n9apQh2FMo7HmI8PJAw7ipH4pXHp0d9onOKOUlj10Jn+auJBpKzNrfO/stbt8zUn+5m/cw/yNe3zL\nqkq5QmSE+Nbtzi9h+dZserZv7VtX4ik/0MMxYezTBRmUeMq59JiDQx1K2LKaggEgIkJ8CQGcvodn\nLh3CYxcO8j0j2p//ENVlW3Jq3f/tHyxiyEPfArB6Ry4zVmVyzVu/cuWbv1aYwO+VH9ceyGE0uu3Z\nRbw9Z0Oow2gx7vhwMeM+XRrqMMKa1RRMtdrERXP58B5cPrwH5eVKSVk5K7blcOFLsznn8M58trDy\ng/Sq9/mirQCkZ+ZyxjMzK2y7dsK+G+r+/V3NTU678orZmVdCXrGHo3q0rbFsUWkZadtyOPLgmsvV\nhaoiIlXW3/D2PJZtyeGMgZ3olBR3wJ8Tjjbuyqdt6xjaxEUf0H5smHLjsKRg6iQiQoiLiGTowW35\n6KZjGdQlicO6tOGlGWtp2yqa9gmx/Lp+d637Oe3fM2st41VWrmQXlpJf7KF7u1aUlytHPfq9b/uK\nh0cTHxN4xldVZcB9XwPwyz2nHvAJu9hTHnB22e3ZTqd7eRN7NtSuvGLGz1rPHaf3r9Bk57W3oARV\naOtOlBhMJz05g94prfnhzlEHtJ9JS/ZNvVLsKSM2ymb7DQZLCqbevE97u+GE3txwQm/AuSrfW1DK\nH99f6EsOk249nvNe+Hm/P2fjrnzGvDybvQWlXHbMwXRNrnhi355TRK8OTn/E4s176dgmznfyzyna\nN6Ipt6j0gJNCfrEnYFIocWeTbWp9Ifd+toyvl29neK/2nBhghNnIJ34gv6SMDU+c0yjxVDede320\nbbWvppFb5CE2wZJCMFifgmkQcdGRdEqK48PfH8uyh87khztP4vBuSax8ZDT/uXRIhavVxy4cVKd9\nXjthHnsLnE7sib9u4qlvKzYtrd6RS2mZczI+/8VZnPmsUwsp8ZTz3LQ1vnLeBLE2K4/1O/fv5OR/\nD4a/0jKnhlBcS1KYtmIHPcdNYU8DPf1u9tqdFYbKzt+4mwmz1vsmN8wtrtr57y/fPR5PWdNKZjXZ\n6fc42dJmFHdzY0nBNLiE2Ch6pyQATrI4f0hXfv3bqb7t5w/pWqf9bNxVUOP2378zn0Pu/co36V92\nYSnbs4v419crefPn9b5yOYWl7Mwr5tSnf+Tkp2YAsC27sMoMsW/8tM43O+yH8zYzafFW37bCSsNm\np6/M5OFJaZS4J6fanj/hjSdtW+2d8nXxu9fnVhgq+/jUlTw4KY0lGdnAvmTln4xVlSlLtlUYAryn\noObkcaAa8sbEXL/aX6mn9v2+M2cDO3KK6v05hSVlXPraHFZtz6217PKt2WH37JGgJgURGS0iq0Qk\nXUTGVVPmtyKSJiLLReS9YMZjQqd9QiyvXHEUi+4/nYTYKJY9dCZ3j+7PeYO7cNWxPRrsc0b8Yxpv\n+CUEcJLF2+4UHeCcqP70/iIkh0eWAAAX1klEQVSe/GYVj01ZwVvu3dqPTlnBze8uYHt2EXd/soTb\nJi70vaegpIylGdm+RHHthHmMn7Xe91yKotKqV64bd+X7ThjepqfCamoc+8ub1Lwnf2/TnTeuwpIy\nnv1+NQUlHlZuz+WW9xZw63sLfO+v7R6R9Mw8Fm/eW2OZWek7q20+Ky2v/xW9qjJh1nqyKyUs/1hL\naqkpbN5dwH1fLOeWdxfUWC6QXzfs5pd1u3l0Slqtn3HOcz/zj6kNM71LUxG0pCAikcCLwFnAQOAy\nERlYqcwhwD3ASFU9DPhzsOIxoTd6UCeSWzkdmwmxUdw8qi/PXzaUh88fxLrHz+a8wV247JjuPHLB\nIF654ijf+z4YO6LCfmaNO6Ven/vI5DTf6CeAwQ996zt5vvHzeh6alEaB313VI/4xrco+Cko8nPfC\nz9w2cSErt1e92q98tZhdUMpJT87goUnOiSXemxQCnIRVlbdmrWd7dsWr2rVZeQx79Hu27i2ssN7/\nAUnek2NSvNPenrqxYlL439yNPPv9Gl74Id3XBPb9in33ngSKx99p//6R81+c5Vv+bGEGny3M8C3P\n27Cby9+Yy/M/rAn0dl+NpSalZeUs25LtW164eS8PTkrjb59VHHrqn3hr68PxHuvewoavCf24Oos7\nPlzEpt1OTTZta8PU/pqKYNYUjgHSVXWdqpYA7wPnVypzI/Ciqu4BUNWa75QyYSsiQnj+sqH8Y8wR\nXDmiB138OpWP6dWOu0c7d1zfe/ahdE2O54IhXRjRu12N+/z9iU4n+K78Et//wAC5xVWn1fC/0S6Q\nFdv2NSWMfXt+le3FlWoKb7q1j/fmbgL21RR2+/UplHjKycwt4tSnf+ShSWncNrHiVe3EuZvYmVfM\npMVbK7T9+ycJb83D26exYadznB73Ct07PXpOUWnAWkp1fSWBlHjKuf2Dxdz+wWLfOm8fzZZKicv/\nPTW55d0FHHLvV5z7/M++fZS676nc9FPkl3hr61PwJnnvFCz1obWMJLt6/K98umCLL4knt6rbUNuM\nPQXNYp6vYCaFrsBmv+UMd52/fkA/EZklIr+IyOhAOxKRsSKSKiKpWVlZQQrXNCVHdEvmo5uOZfpf\nRiEi/P7EPjx24SCuOs5panr20qG8P/ZYX/n4SiODvr/jRO45+9AK6047tOpNeF5XvvlrjfE8Mnlf\nU0Kg4afFnnKKPWXM2+Bcqf+4at/1TWZOEW3inYF+M/zW//WTJRzz2DTWuSfWeRv2sMivqSbKPaF9\nMG8zAx/4hvTMPKavyiQzd1+Ha26Rh39MXUHGHicZ5LkJz+NeoS/c5OyvrFwD1goWbao5GXr1umcK\nP6zc4VveW1DC2qw8ctykk7Y1h/d/3VTlff4n7wWb9rB8674aQUGJhyl+T/grcGMvcONc4ld7ACcB\nxkVHVNlvIN7BBd7JGoPBm8QqP2tkzY7cKoklPTOP4/85ndd/Whe0eBpKqDuao4BDgFHAZcDrIpJc\nuZCqvqaqw1R1WEpK/SZwM83X0T3b+YacRkYIlw/vUWVs+tgTe3PqgINY8uAZvnXf/PlE+h6UCMC7\nNwz3rY+MEE7u7/z7efzCw0mMi6rzw4P8ZeypelWcW1TKY1NWcPErcxj15HQWZ2T7EtXkJdt8naSz\n1+4iM7fIXb+1yn4ueHEWPcdNoee4KeS7J8l1O/OdqR1em8O1b83j62X7TqQfzNvMqzPX+YZ8eqcc\n8VS6Iq0uKTw4KY38Yg+XvDqH1A3V32eiCnd9vO+53ac/M5NTn/7Rd/JduT2XcZ8uZc2OXF/tBCrW\nFMa8NJtznts3RPk6v5sWYd+IKG9fQomnvMKVdbGn3HcD3MRfNweshbw4PZ135270JatANYXycmX1\njuo7kb0PfKqNtybjf/6flb6T05+ZyUepGRXKLslwkrM3STdlwUwKW4Dufsvd3HX+MoAvVbVUVdcD\nq3GShDF18rezD+XNa44mOjKCT28+jgnXHk3/Tom+7SP7duCrP50AQM/2rbnquJ4AHNalDal/P43l\nD53Jg+ft6+ry3rR89uGdavzcfh0TKiyP+3Qpb8/ZCMAGd9TUfy4dQsc2sTw8OY2P5zsniWJPOcc8\nNo0PUzfX2t7+zi8bKyzvzCupsH+AF6ZXnME1r9hDebkSWenua0+ZUljNbLQvTk9n7vrdXPPWPOas\n3QUEbkLxH/3jnSm38hDb05+ZyZVvzuWUp2bw39kbGPtO1aa2N9yr5V/WVUxC3iTon1T2uLPuFpR4\nWL8z39eh/smCDAbe/3WVfT/5zSru/WyZLzlGRwq5RaW8NCPd18/y1LerOOOZmVWGJy/fms3arLwK\n36+/hyelMSt9J94BXTtynO/Afziyd8qWJVsqnvy9Zb1NTcWeMjZU+nxVrdC8GCrBTArzgENEpJeI\nxACXAl9WKvM5Ti0BEemA05zU9OtXpkk68uC2jOpftYno0M5t+Onuk/nzaf04uf9BrHxkNIO7JxMb\nFUl0ZASnH+YkgMTYKP577TEAHNu7fY2ftbeglA4J1d8NfNeZ/Tl9YEeS4wOXudvvqhvg+cuGclkd\nJ3nzbz6qTBXmrt9NelZehfVFnrIqCcTrpRnOfFN5xR4ue/0XJv66iV73TK0y+ieQyokLYElGNut2\n5vPAl8tZEWAI7qNTVgRMOoGSws68ElSVL9yBAtv8OuMr14b8R0nd+9kywEk8T3y1kn99vYrv0rYD\nTs0NIK+oYpI8/4VZnPr0jwGP88lvVjJ+1nouf2MuURHOadNb48vK3ReTNxdPXrLNd4yesnL+6T6A\nav7GPWQXlnLPJ0sZ9dQM8os95BV7yC4s5blp6Rz5yHcBh8J6ysorDIYIpqDd0ayqHhG5FfgGiATG\nq+pyEXkYSFXVL91tZ4hIGlAG3KWqu4IVk2m5urfb97zoyncmd02O58PfH8vALm1IiI1iwxPnoKrE\nx0Qxsm97kuNjSM/M4/BuSe5EfvPIL/Yw997TKC51TrZvzdpQYZ+nHdoREeHIHsmsqqGpwvv55w3u\nwnmDu3DJ0d25wG+0TyC1DRG97PVfAKet29vEMnXpdt/2V644iqy8Yu77fFnA99/jTjj3j69W1Pg5\nB+KNn9ZXWZdf4tRy/Oe/ysot5sfVmTzuDvvsk9KatX53R5eWlZOZW0zrmEjmrAt86njX7ewvKClj\n9Y5c36CD8174mVnjTmHkEz/w6AWDqiSZFdtyeeOndfz26O68OH3fRI0REUAZbN3rJIP0zDxUlVdn\nruOrZc73vLeglF73TOWSYd25yx0kAbAmM4+z//OTr3nrvi+WMXXpNopKyxnmzuX16JQ03rl+OGXl\nTs2hXesYbnlvAd8s39Eod6BLbT3tTc2wYcM0NbVuTwszpqE5I3AWceOJvRnSvUr3l+9GutS/n0aH\nhFgKSjzsyCnm0wUZHNYliUcmp3F0z7YVhsjO//tpFWao9e4DIEIg0ICVk/ql8OPqmgdd/O3sAVwy\n7GAGP/xthfXeE8sFL86q0LFdkz+eekiFu8SDoU1cFI9eeDh/9Ls/5O7R/SvcpPfFLSMrDJH91/8d\nwd2fLKFDQiwXD+vGyzMafpbd4/q0Z/ba/b9WnXbnSdXWQKqTGBfla66779yBvoEOaQ+fSauY/buW\nF5H5qjqstnI295Ex9RATFcGLlx9Z7fZnLxnCi9PTaevej9EqJopeHaK48wznanH0oE6UlyutYqN4\nb+4m4qIjKiQEgG9vP5ESjzMj7W+GdCEqIoKJv27i39+t5rAubfhpzU6uPq4HL15+JIMe+IajerRl\n/sY9xERGVLip64KhXUlqFc17Nwxn2spMDm7XivMGd/Ft//yWkZR4ypm/cY+vdhHI/ecO9HX4A7xx\n1TDu+HCRr5N58m3H879fNvL+vM1V3rcjp4gV23OZWUsCA2fEkH9CAKo84CglseJ3dfcnTjPczrxi\nducFpz3+QBJCYmyUrzlsSPfkOidh//6b9Mx9Nc0tewo5pGNioLc0GEsKxjSgC4Z25YKhNU/jEREh\nPH7h4dw8qk/AmT77uf/TD+qa5Ft3xYgeXDGiB6t35PLqj+sY2bcDsVGRfP3nE+jdIYHpqzLpk5LA\n9JWZPDbVafZJcZPNcX07cFzfDgFjiYmK4Ng+7atcgU//yyi27Cnk9g8XcfrAjvj3W582sCP/GHME\nt7y3gMS4KAZ1TeLB3xxG6sY99O+UyBS3zf68wV1ISYzlq6XbmLk6iytH9ODGE3pz4pPTa/0ee7Zv\nxfXH96rwlD+Ajm2qn9jwg9SKSemeswYwvHf7WpvjDsQbVw3jhhqec55b7PF1rPdJSahzUvDnP2Ip\nbVtO0JOCNR8ZE2byiz3kFJXSOSm+Xu/bk1/Crxt2071tKwZ2aVNl+10fLeaEfin8ZnAXtmUXcu9n\ny3jgvIH08HtyHjidqQclxvr6ccrKlQ9TN3Ny/4PolBTHP79eycsz1hIXHcFLlx/JdRNSiYmM4O7R\n/Xl0ygoSY6NY+tCZAJz7/E++hzidc0RnXvzdkfQcN4UuSXHMGncK01ZkktQqmotfmQNU7HPwNpOd\n9u8fSc/M49Obj+OJqSv5tYaht8f2bs+cdbv49ObjGPPS7Arbrj62B/+dU7FjfdH9pzuzze7M54hu\nSSzYtJcOCTGs35lPh4RYLn1tXw3s7euO4arxNd8PU1nX5PgKNwZec1xPHvzNYfXah1ddm48sKRhj\nGl12YSkoJLWK5p05GxjZt4NvEkV/P6zcwXUTUis8EyO7oJTISCEhdl9Dxz+mrqBNfDRjjuzK+J/X\nc3i3ZH7jNpUVlZZRXFpOUqtoikrLyC/2kLpxD9uzixjeux0XvTyHvGIPt5zch9tP60e5OjWo8nKl\n99+m0jomkleuPIoTDknhzg8X88mCffcg1NTxq6qc98LPLNuSw5ihXfn3JUN4eFIa42dV7WQf1qMt\nqRv3cMGQLtx5Rn9O+JdTm0qMjfLdgf/6VcM4qV/Kft1bA5YUjDGmwakq36XtYPOeQoZ0T6716X/l\n5crughKS4qOJjozg39+u4rkf0rnnrAGMn7WeHTnFvHfjcI7t3Z7JS7Zx+sCOxEZF0OueqYCTCG58\nO5Wv/3wCAzpVrb3VhyUFY4xpYrILS3nhhzXceUb/gA9t8pq0eCudkuJ8D7RqCDb6yBhjmpik+Gju\nPWdgreX8R4k1tlDPfWSMMaYJsaRgjDHGx5KCMcYYH0sKxhhjfCwpGGOM8bGkYIwxxseSgjHGGB9L\nCsYYY3ya3R3NIpIFVH3cU910AHY2YDjNgR1zy2DH3DIcyDH3UNVaH3Lf7JLCgRCR1Lrc5h1O7Jhb\nBjvmlqExjtmaj4wxxvhYUjDGGOPT0pLCa6EOIATsmFsGO+aWIejH3KL6FIwxxtSspdUUjDHG1MCS\ngjHGGJ8WkxREZLSIrBKRdBEZF+p4GoqIdBeR6SKSJiLLReRP7vp2IvKdiKxx/9vWXS8i8pz7PSwR\nkSNDewT7R0QiRWShiEx2l3uJyFz3uD4QkRh3fay7nO5u7xnKuA+EiCSLyMcislJEVojIseH8O4vI\n7e6/6WUiMlFE4sLxdxaR8SKSKSLL/NbV+3cVkavd8mtE5Or9jadFJAURiQReBM4CBgKXiUjtjz9q\nHjzAnao6EBgB3OIe2zhgmqoeAkxzl8H5Dg5x/8YCLzd+yA3iT8AKv+V/As+oal9gD3C9u/56YI+7\n/hm3XHP1H+BrVR0ADMY5/rD8nUWkK/BHYJiqDgIigUsJz995AjC60rp6/a4i0g54ABgOHAM84E0k\n9aaqYf8HHAt847d8D3BPqOMK0rF+AZwOrAI6u+s6A6vc168Cl/mV95VrLn9AN/d/lFOAyYDg3OUZ\nVfn3Br4BjnVfR7nlJNTHsB/HnASsrxx7uP7OQFdgM9DO/d0mA2eG6+8M9ASW7e/vClwGvOq3vkK5\n+vy1iJoC+/6BeWW468KKW2UeCswFOqrqNnfTdqCj+zocvotngbuBcne5PbBXVT3usv8x+Y7X3Z7t\nlm9uegFZwFtus9kbItKaMP2dVXUL8BSwCdiG87vNJ/x/Z6/6/q4N9nu3lKQQ9kQkAfgE+LOq5vhv\nU+fSISzGHovIuUCmqs4PdSyNLAo4EnhZVYcC+exrUgDC7nduC5yPkwy7AK2p2sTSIjT279pSksIW\noLvfcjd3XVgQkWichPCuqn7qrt4hIp3d7Z2BTHd9c/8uRgK/EZENwPs4TUj/AZJFJMot439MvuN1\ntycBuxoz4AaSAWSo6lx3+WOcJBGuv/NpwHpVzVLVUuBTnN8+3H9nr/r+rg32e7eUpDAPOMQduRCD\n02H1ZYhjahAiIsCbwApV/bffpi8B7wiEq3H6Grzrr3JHMYwAsv2qqU2eqt6jqt1UtSfO7/iDql4O\nTAcucotVPl7v93CRW77ZXU2r6nZgs4j0d1edCqQRpr8zTrPRCBFp5f4b9x5vWP/Ofur7u34DnCEi\nbd1a1hnuuvoLdQdLI3bknA2sBtYC94Y6ngY8ruNxqpZLgEXu39k47anTgDXA90A7t7zgjMRaCyzF\nGd0R8uPYz2MfBUx2X/cGfgXSgY+AWHd9nLuc7m7vHeq4D+B4hwCp7m/9OdA2nH9n4CFgJbAMeAeI\nDcffGZiI029SilMjvH5/flfgOvf404Fr9zcem+bCGGOMT0tpPjLGGFMHlhSMMcb4WFIwxhjjY0nB\nGGOMjyUFY4wxPpYUTFCIyGz3vz1F5HcNvO+/BfqsYBGRC0Tk/iDtOy9I+x3lnUH2APYxQUQuqmH7\nrSJy3YF8hml6LCmYoFDV49yXPYF6JQW/O1arUyEp+H1WsNwNvHSgO6nDcQVdA8cwHritAfdnmgBL\nCiYo/K6AnwBOEJFF7vz4kSLypIjMc+eD/71bfpSI/CQiX+LcuYqIfC4i89059ce6654A4t39vev/\nWe5dnk+68+8vFZFL/PY9Q/Y9i+Bd9y5ZROQJcZ5FsUREngpwHP2AYlXd6S5PEJFXRCRVRFa7czF5\nn+9Qp+MK8BmPichiEflFRDr6fc5FfmXy/PZX3bGMdtctAMb4vfdBEXlHRGYB79QQq4jIC+I8d+R7\n4CC/fVT5nlS1ANggIsfU5d+EaR5CfuViwt444C+q6j15jsW5Nf9oEYkFZonIt27ZI4FBqrreXb5O\nVXeLSDwwT0Q+UdVxInKrqg4J8FljcO76HQx0cN8z0902FDgM2ArMAkaKyArgQmCAqqqIJAfY50hg\nQaV1PXHmrO8DTBeRvsBV9Tguf62BX1T1XhH5F3Aj8GiAcv4CHUsq8DrOXFDpwAeV3jMQOF5VC2v4\nDYYC/d2yHXGS2HgRaV/D95QKnIBzF7EJA1ZTMI3tDJy5WxbhTPHdHueBIQC/Vjpx/lFEFgO/4Ez2\ndQg1Ox6YqKplqroD+BE42m/fGapajjMVSE+c6ZWLgDdFZAxQEGCfnXGmrPb3oaqWq+oaYB0woJ7H\n5a8E51kB4EwN3bOWY6zuWAbgTCC3Rp1pCv5X6T1fqmqh+7q6WE9k3/e3FfjBLV/T95SJM4upCRNW\nUzCNTYDbVLXCZF0iMgpnOmj/5dNwHpxSICIzcOa32V/Ffq/LcB7U4nGbPk7FmUTtVpwrbX+FODNu\n+qs8N4xSx+MKoFT3zTVTxr7/Jz24F20iEgHE1HQsNezfyz+G6mI9O9Aba/me4nC+IxMmrKZggi0X\nSPRb/gb4gzjTfSMi/cR5WExlSTiPVywQkQE4jxr1KvW+v5KfgEvcNvMUnCvfaps1xHkGRZKqTgVu\nx2l2qmwF0LfSuotFJEJE+uBM0LaqHsdVVxuAo9zXvwECHa+/lUBPNyZwnsRVnepincm+768zcLK7\nvabvqR/OhHUmTFhNwQTbEqDMbQaagPPsg57AAreDNAu4IMD7vgZuctv9V+E0IXm9BiwRkQXqTJvt\n9RnOIxoX41y9362q292kEkgi8IWIxOFcPd8RoMxM4GkREb8r+k04yaYNcJOqFonIG3U8rrp63Y1t\nMc53UVNtAzeGscAUESnASZCJ1RSvLtbPcGoAae4xznHL1/Q9jQQerO/BmabLZkk1phYi8h9gkqp+\nLyITcKbr/jjEYYWciAwF7lDVK0Mdi2k41nxkTO0eB1qFOogmqANwX6iDMA3LagrGGGN8rKZgjDHG\nx5KCMcYYH0sKxhhjfCwpGGOM8bGkYIwxxuf/AcpH4GCAUhv0AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n","Train Accuracy: 0.82222223\n","Test Accuracy: 0.69166666\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZZnhzx1CcwV1"},"source":["We've got a perfect training curve, i.e. cost vs epochs curve. This means the model is well optimized."]},{"cell_type":"markdown","metadata":{"id":"dKk3JEfO11mA"},"source":["# Keras - Implementation:\n","\n","![alt text](https://datascience-enthusiast.com/figures/house-members.png)\n","\n","## Creating a model that can classify whether a man is Happy or not."]},{"cell_type":"code","metadata":{"id":"BW4R6cXHc_om","executionInfo":{"status":"ok","timestamp":1567230879323,"user_tz":-330,"elapsed":2511,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"38fe816c-56b6-44d4-abc7-599b2e9086bf","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import h5py\n","from keras import layers\n","from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D\n","#from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n","from keras.models import Model\n","from keras.preprocessing import image\n","from keras.utils import layer_utils\n","from keras.utils.data_utils import get_file\n","from keras.applications.imagenet_utils import preprocess_input\n","import pydot\n","#from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","from keras.utils import plot_model\n","\n","import keras.backend as K\n","K.set_image_data_format('channels_last')\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imshow\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NwDGAmQDpgji"},"source":["### Loading Dataset"]},{"cell_type":"code","metadata":{"id":"wyJdctvf198H"},"source":["def load_dataset():\n","    train_dataset = h5py.File('train_happy.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('test_happy.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yz3ENckv3I_L"},"source":["X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYrSGlhIvgBw"},"source":["### Visualizing Raw Pixel data from the 1st training example"]},{"cell_type":"code","metadata":{"id":"zLgNmBKj3_wO","executionInfo":{"status":"ok","timestamp":1567230883618,"user_tz":-330,"elapsed":561,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"239b3efa-9ee1-44d7-eae0-463fdd809e67","colab":{"base_uri":"https://localhost:8080/","height":850}},"source":["X_train_orig[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[178, 190, 163],\n","        [172, 181, 173],\n","        [188, 196, 184],\n","        ...,\n","        [255, 255, 254],\n","        [254, 255, 250],\n","        [249, 253, 222]],\n","\n","       [[184, 201, 165],\n","        [149, 154, 142],\n","        [149, 158, 139],\n","        ...,\n","        [255, 255, 253],\n","        [254, 255, 250],\n","        [251, 255, 230]],\n","\n","       [[198, 207, 165],\n","        [141, 147, 128],\n","        [168, 184, 154],\n","        ...,\n","        [253, 255, 244],\n","        [254, 255, 248],\n","        [253, 255, 233]],\n","\n","       ...,\n","\n","       [[ 77,  82,  89],\n","        [ 60,  64,  68],\n","        [ 49,  53,  55],\n","        ...,\n","        [ 31,  32,  30],\n","        [ 47,  48,  46],\n","        [ 75,  75,  75]],\n","\n","       [[ 68,  71,  76],\n","        [ 55,  58,  59],\n","        [ 44,  47,  47],\n","        ...,\n","        [ 30,  31,  29],\n","        [ 40,  41,  39],\n","        [ 63,  64,  63]],\n","\n","       [[ 61,  64,  66],\n","        [ 52,  54,  54],\n","        [ 45,  47,  46],\n","        ...,\n","        [ 30,  31,  29],\n","        [ 38,  38,  36],\n","        [ 56,  56,  54]]], dtype=uint8)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"UdPeYTu6wg6e"},"source":["### Performing Post-processing:\n","Scaling the data to keep all the pixel values between 0 and 1."]},{"cell_type":"code","metadata":{"id":"mbGs4ZWf3Myt"},"source":["X_train = X_train_orig/255.\n","X_test = X_test_orig/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EVUHNHtvxMbH"},"source":["### Visualizing scalled pixel data from the 1st training example"]},{"cell_type":"code","metadata":{"id":"cd9GGJC933RH","executionInfo":{"status":"ok","timestamp":1567230887204,"user_tz":-330,"elapsed":582,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"54acd53b-7986-4703-df5b-5a8f0c3bd9ed","colab":{"base_uri":"https://localhost:8080/","height":850}},"source":["X_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.69803922, 0.74509804, 0.63921569],\n","        [0.6745098 , 0.70980392, 0.67843137],\n","        [0.7372549 , 0.76862745, 0.72156863],\n","        ...,\n","        [1.        , 1.        , 0.99607843],\n","        [0.99607843, 1.        , 0.98039216],\n","        [0.97647059, 0.99215686, 0.87058824]],\n","\n","       [[0.72156863, 0.78823529, 0.64705882],\n","        [0.58431373, 0.60392157, 0.55686275],\n","        [0.58431373, 0.61960784, 0.54509804],\n","        ...,\n","        [1.        , 1.        , 0.99215686],\n","        [0.99607843, 1.        , 0.98039216],\n","        [0.98431373, 1.        , 0.90196078]],\n","\n","       [[0.77647059, 0.81176471, 0.64705882],\n","        [0.55294118, 0.57647059, 0.50196078],\n","        [0.65882353, 0.72156863, 0.60392157],\n","        ...,\n","        [0.99215686, 1.        , 0.95686275],\n","        [0.99607843, 1.        , 0.97254902],\n","        [0.99215686, 1.        , 0.91372549]],\n","\n","       ...,\n","\n","       [[0.30196078, 0.32156863, 0.34901961],\n","        [0.23529412, 0.25098039, 0.26666667],\n","        [0.19215686, 0.20784314, 0.21568627],\n","        ...,\n","        [0.12156863, 0.1254902 , 0.11764706],\n","        [0.18431373, 0.18823529, 0.18039216],\n","        [0.29411765, 0.29411765, 0.29411765]],\n","\n","       [[0.26666667, 0.27843137, 0.29803922],\n","        [0.21568627, 0.22745098, 0.23137255],\n","        [0.17254902, 0.18431373, 0.18431373],\n","        ...,\n","        [0.11764706, 0.12156863, 0.11372549],\n","        [0.15686275, 0.16078431, 0.15294118],\n","        [0.24705882, 0.25098039, 0.24705882]],\n","\n","       [[0.23921569, 0.25098039, 0.25882353],\n","        [0.20392157, 0.21176471, 0.21176471],\n","        [0.17647059, 0.18431373, 0.18039216],\n","        ...,\n","        [0.11764706, 0.12156863, 0.11372549],\n","        [0.14901961, 0.14901961, 0.14117647],\n","        [0.21960784, 0.21960784, 0.21176471]]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"5IJ1qADc0C5v"},"source":["### Reshaping the train and test data by taking their transpose:"]},{"cell_type":"code","metadata":{"id":"PWfXY-C63qyO"},"source":["Y_train = Y_train_orig.T\n","Y_test = Y_test_orig.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DSbS-UtNKlDQ"},"source":["### Number of training & testing examples:"]},{"cell_type":"code","metadata":{"id":"VrIWKkO6KfuM","executionInfo":{"status":"ok","timestamp":1567230891233,"user_tz":-330,"elapsed":977,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"d4a20f25-ff34-45f4-fa69-694d33e7fc26","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print (\"Number of training examples = \" + str(len(X_train)))\n","print (\"Number of test examples = \" + str(len(X_test)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples = 600\n","Number of test examples = 150\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ovmoOYZGKrw-"},"source":["### Shape of training & testing examples:"]},{"cell_type":"code","metadata":{"id":"d8AG8wJj3tLp","executionInfo":{"status":"ok","timestamp":1567230893018,"user_tz":-330,"elapsed":1210,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"849c628b-8ff5-4175-c389-f6e84d033fa0","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["print (\"X_train shape: \" + str(X_train.shape))\n","print (\"Y_train shape: \" + str(Y_train.shape))\n","print (\"X_test shape: \" + str(X_test.shape))\n","print (\"Y_test shape: \" + str(Y_test.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X_train shape: (600, 64, 64, 3)\n","Y_train shape: (600, 1)\n","X_test shape: (150, 64, 64, 3)\n","Y_test shape: (150, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KUYUTNw80Ktr"},"source":["### Creating Model using Padding:"]},{"cell_type":"code","metadata":{"id":"ZRdkwfD03wKb"},"source":["def keras_model(input_shape):\n","\n","    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n","    X_input = Input(input_shape)\n","    \n","    # Zero-Padding: pads the border of X_input with zeroes\n","    X = ZeroPadding2D((3, 3))(X_input)\n","    \n","    # CONV -> BN -> RELU Block applied to X\n","    X = Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0')(X)\n","    \n","    # MAXPOOL\n","    X = BatchNormalization(axis = 3, name = 'bn0')(X)\n","    \n","    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n","    X = Activation('relu')(X)\n","    X = MaxPooling2D((2, 2), name='max_pool')(X)\n","    X = Flatten()(X)\n","    X = Dense(1, activation='sigmoid', name='fc')(X)\n","    \n","    # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n","    model = Model(inputs = X_input, outputs = X, name='keras_model')\n","    \n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLXR7hQS5YqV","executionInfo":{"status":"ok","timestamp":1567230899121,"user_tz":-330,"elapsed":4888,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"73918c5c-2e54-4610-a479-2a01bac11ce3","colab":{"base_uri":"https://localhost:8080/","height":292}},"source":["Model = keras_model(X_train.shape[1:])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0831 05:54:54.936635 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0831 05:54:54.976177 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0831 05:54:54.998240 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0831 05:54:55.041786 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","W0831 05:54:55.044647 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","W0831 05:54:58.499510 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","W0831 05:54:58.583917 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"62HdHRWN0rTB"},"source":["### Compiling the model:"]},{"cell_type":"code","metadata":{"id":"LMJMz1xS6Psu","executionInfo":{"status":"ok","timestamp":1567230900745,"user_tz":-330,"elapsed":547,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"72ee9d70-6083-486e-a643-7b8569341d50","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["Model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W0831 05:55:00.598044 139635340552064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0831 05:55:00.630164 139635340552064 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"r0PmDiOd0u0F"},"source":["### Training the model:"]},{"cell_type":"code","metadata":{"id":"uGSIh-x5VuOk","executionInfo":{"status":"ok","timestamp":1567230930727,"user_tz":-330,"elapsed":27331,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"41711305-0466-4fe2-c420-0cbcaddac916","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["history = Model.fit(X_train, Y_train, epochs = 100, batch_size = 40)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","600/600 [==============================] - 3s 5ms/step - loss: 2.5868 - acc: 0.5417\n","Epoch 2/100\n","600/600 [==============================] - 0s 387us/step - loss: 0.8137 - acc: 0.7350\n","Epoch 3/100\n","600/600 [==============================] - 0s 358us/step - loss: 0.3999 - acc: 0.8550\n","Epoch 4/100\n","600/600 [==============================] - 0s 358us/step - loss: 0.3128 - acc: 0.8867\n","Epoch 5/100\n","600/600 [==============================] - 0s 362us/step - loss: 0.2313 - acc: 0.8967\n","Epoch 6/100\n","600/600 [==============================] - 0s 360us/step - loss: 0.1736 - acc: 0.9417\n","Epoch 7/100\n","600/600 [==============================] - 0s 354us/step - loss: 0.0993 - acc: 0.9783\n","Epoch 8/100\n","600/600 [==============================] - 0s 361us/step - loss: 0.0802 - acc: 0.9767\n","Epoch 9/100\n","600/600 [==============================] - 0s 368us/step - loss: 0.0752 - acc: 0.9817\n","Epoch 10/100\n","600/600 [==============================] - 0s 371us/step - loss: 0.0700 - acc: 0.9817\n","Epoch 11/100\n","600/600 [==============================] - 0s 387us/step - loss: 0.0768 - acc: 0.9733\n","Epoch 12/100\n","600/600 [==============================] - 0s 370us/step - loss: 0.0623 - acc: 0.9817\n","Epoch 13/100\n","600/600 [==============================] - 0s 362us/step - loss: 0.0603 - acc: 0.9833\n","Epoch 14/100\n","600/600 [==============================] - 0s 368us/step - loss: 0.0696 - acc: 0.9833\n","Epoch 15/100\n","600/600 [==============================] - 0s 371us/step - loss: 0.0661 - acc: 0.9817\n","Epoch 16/100\n","600/600 [==============================] - 0s 369us/step - loss: 0.0532 - acc: 0.9850\n","Epoch 17/100\n","600/600 [==============================] - 0s 370us/step - loss: 0.0556 - acc: 0.9833\n","Epoch 18/100\n","600/600 [==============================] - 0s 357us/step - loss: 0.0473 - acc: 0.9850\n","Epoch 19/100\n","600/600 [==============================] - 0s 360us/step - loss: 0.0396 - acc: 0.9900\n","Epoch 20/100\n","600/600 [==============================] - 0s 387us/step - loss: 0.0405 - acc: 0.9917\n","Epoch 21/100\n","600/600 [==============================] - 0s 361us/step - loss: 0.0382 - acc: 0.9933\n","Epoch 22/100\n","600/600 [==============================] - 0s 359us/step - loss: 0.0467 - acc: 0.9850\n","Epoch 23/100\n","600/600 [==============================] - 0s 368us/step - loss: 0.0278 - acc: 0.9900\n","Epoch 24/100\n","600/600 [==============================] - 0s 391us/step - loss: 0.0263 - acc: 0.9917\n","Epoch 25/100\n","600/600 [==============================] - 0s 368us/step - loss: 0.0309 - acc: 0.9917\n","Epoch 26/100\n","600/600 [==============================] - 0s 369us/step - loss: 0.0459 - acc: 0.9833\n","Epoch 27/100\n","600/600 [==============================] - 0s 396us/step - loss: 0.0379 - acc: 0.9933\n","Epoch 28/100\n","600/600 [==============================] - 0s 364us/step - loss: 0.0271 - acc: 0.9950\n","Epoch 29/100\n","600/600 [==============================] - 0s 376us/step - loss: 0.0246 - acc: 0.9950\n","Epoch 30/100\n","600/600 [==============================] - 0s 372us/step - loss: 0.0269 - acc: 0.9900\n","Epoch 31/100\n","600/600 [==============================] - 0s 374us/step - loss: 0.0282 - acc: 0.9900\n","Epoch 32/100\n","600/600 [==============================] - 0s 361us/step - loss: 0.0490 - acc: 0.9800\n","Epoch 33/100\n","600/600 [==============================] - 0s 363us/step - loss: 0.0334 - acc: 0.9917\n","Epoch 34/100\n","600/600 [==============================] - 0s 377us/step - loss: 0.0471 - acc: 0.9783\n","Epoch 35/100\n","600/600 [==============================] - 0s 376us/step - loss: 0.0720 - acc: 0.9717\n","Epoch 36/100\n","600/600 [==============================] - 0s 365us/step - loss: 0.0681 - acc: 0.9750\n","Epoch 37/100\n","600/600 [==============================] - 0s 363us/step - loss: 0.0282 - acc: 0.9900\n","Epoch 38/100\n","600/600 [==============================] - 0s 374us/step - loss: 0.0241 - acc: 0.9933\n","Epoch 39/100\n","600/600 [==============================] - 0s 368us/step - loss: 0.0291 - acc: 0.9917\n","Epoch 40/100\n","600/600 [==============================] - 0s 368us/step - loss: 0.0277 - acc: 0.9883\n","Epoch 41/100\n","600/600 [==============================] - 0s 373us/step - loss: 0.0274 - acc: 0.9933\n","Epoch 42/100\n","600/600 [==============================] - 0s 420us/step - loss: 0.0142 - acc: 0.9967\n","Epoch 43/100\n","600/600 [==============================] - 0s 397us/step - loss: 0.0101 - acc: 0.9983\n","Epoch 44/100\n","600/600 [==============================] - 0s 377us/step - loss: 0.0133 - acc: 0.9967\n","Epoch 45/100\n","600/600 [==============================] - 0s 366us/step - loss: 0.0148 - acc: 0.9950\n","Epoch 46/100\n","600/600 [==============================] - 0s 375us/step - loss: 0.0092 - acc: 0.9983\n","Epoch 47/100\n","600/600 [==============================] - 0s 392us/step - loss: 0.0087 - acc: 0.9983\n","Epoch 48/100\n","600/600 [==============================] - 0s 374us/step - loss: 0.0154 - acc: 0.9933\n","Epoch 49/100\n","600/600 [==============================] - 0s 376us/step - loss: 0.0200 - acc: 0.9950\n","Epoch 50/100\n","600/600 [==============================] - 0s 360us/step - loss: 0.0223 - acc: 0.9933\n","Epoch 51/100\n","600/600 [==============================] - 0s 376us/step - loss: 0.0103 - acc: 0.9983\n","Epoch 52/100\n","600/600 [==============================] - 0s 390us/step - loss: 0.0056 - acc: 1.0000\n","Epoch 53/100\n","600/600 [==============================] - 0s 375us/step - loss: 0.0072 - acc: 0.9983\n","Epoch 54/100\n","600/600 [==============================] - 0s 389us/step - loss: 0.0151 - acc: 0.9933\n","Epoch 55/100\n","600/600 [==============================] - 0s 387us/step - loss: 0.0152 - acc: 0.9950\n","Epoch 56/100\n","600/600 [==============================] - 0s 387us/step - loss: 0.0181 - acc: 0.9933\n","Epoch 57/100\n","600/600 [==============================] - 0s 380us/step - loss: 0.0143 - acc: 0.9950\n","Epoch 58/100\n","600/600 [==============================] - 0s 387us/step - loss: 0.0092 - acc: 1.0000\n","Epoch 59/100\n","600/600 [==============================] - 0s 369us/step - loss: 0.0053 - acc: 1.0000\n","Epoch 60/100\n","600/600 [==============================] - 0s 370us/step - loss: 0.0051 - acc: 0.9983\n","Epoch 61/100\n","600/600 [==============================] - 0s 373us/step - loss: 0.0112 - acc: 0.9950\n","Epoch 62/100\n","600/600 [==============================] - 0s 379us/step - loss: 0.0174 - acc: 0.9917\n","Epoch 63/100\n","600/600 [==============================] - 0s 375us/step - loss: 0.0092 - acc: 0.9983\n","Epoch 64/100\n","600/600 [==============================] - 0s 366us/step - loss: 0.0101 - acc: 0.9967\n","Epoch 65/100\n","600/600 [==============================] - 0s 391us/step - loss: 0.0064 - acc: 1.0000\n","Epoch 66/100\n","600/600 [==============================] - 0s 375us/step - loss: 0.0089 - acc: 0.9983\n","Epoch 67/100\n","600/600 [==============================] - 0s 359us/step - loss: 0.0048 - acc: 1.0000\n","Epoch 68/100\n","600/600 [==============================] - 0s 361us/step - loss: 0.0075 - acc: 0.9967\n","Epoch 69/100\n","600/600 [==============================] - 0s 352us/step - loss: 0.0108 - acc: 0.9967\n","Epoch 70/100\n","600/600 [==============================] - 0s 372us/step - loss: 0.0181 - acc: 0.9967\n","Epoch 71/100\n","600/600 [==============================] - 0s 361us/step - loss: 0.0276 - acc: 0.9900\n","Epoch 72/100\n","600/600 [==============================] - 0s 356us/step - loss: 0.0146 - acc: 0.9967\n","Epoch 73/100\n","600/600 [==============================] - 0s 381us/step - loss: 0.0072 - acc: 0.9983\n","Epoch 74/100\n","600/600 [==============================] - 0s 373us/step - loss: 0.0124 - acc: 0.9950\n","Epoch 75/100\n","600/600 [==============================] - 0s 386us/step - loss: 0.0032 - acc: 1.0000\n","Epoch 76/100\n","600/600 [==============================] - 0s 369us/step - loss: 0.0059 - acc: 0.9967\n","Epoch 77/100\n","600/600 [==============================] - 0s 392us/step - loss: 0.0055 - acc: 0.9983\n","Epoch 78/100\n","600/600 [==============================] - 0s 377us/step - loss: 0.0034 - acc: 1.0000\n","Epoch 79/100\n","600/600 [==============================] - 0s 403us/step - loss: 0.0020 - acc: 1.0000\n","Epoch 80/100\n","600/600 [==============================] - 0s 386us/step - loss: 0.0028 - acc: 1.0000\n","Epoch 81/100\n","600/600 [==============================] - 0s 383us/step - loss: 0.0022 - acc: 1.0000\n","Epoch 82/100\n","600/600 [==============================] - 0s 396us/step - loss: 0.0022 - acc: 1.0000\n","Epoch 83/100\n","600/600 [==============================] - 0s 399us/step - loss: 0.0021 - acc: 1.0000\n","Epoch 84/100\n","600/600 [==============================] - 0s 373us/step - loss: 0.0013 - acc: 1.0000\n","Epoch 85/100\n","600/600 [==============================] - 0s 375us/step - loss: 0.0015 - acc: 1.0000\n","Epoch 86/100\n","600/600 [==============================] - 0s 389us/step - loss: 0.0017 - acc: 1.0000\n","Epoch 87/100\n","600/600 [==============================] - 0s 407us/step - loss: 0.0045 - acc: 0.9983\n","Epoch 88/100\n","600/600 [==============================] - 0s 401us/step - loss: 0.0027 - acc: 1.0000\n","Epoch 89/100\n","600/600 [==============================] - 0s 396us/step - loss: 0.0014 - acc: 1.0000\n","Epoch 90/100\n","600/600 [==============================] - 0s 390us/step - loss: 0.0018 - acc: 1.0000\n","Epoch 91/100\n","600/600 [==============================] - 0s 392us/step - loss: 0.0015 - acc: 1.0000\n","Epoch 92/100\n","600/600 [==============================] - 0s 415us/step - loss: 9.6473e-04 - acc: 1.0000\n","Epoch 93/100\n","600/600 [==============================] - 0s 397us/step - loss: 9.9842e-04 - acc: 1.0000\n","Epoch 94/100\n","600/600 [==============================] - 0s 393us/step - loss: 9.1358e-04 - acc: 1.0000\n","Epoch 95/100\n","600/600 [==============================] - 0s 401us/step - loss: 8.9398e-04 - acc: 1.0000\n","Epoch 96/100\n","600/600 [==============================] - 0s 408us/step - loss: 7.6746e-04 - acc: 1.0000\n","Epoch 97/100\n","600/600 [==============================] - 0s 396us/step - loss: 7.2673e-04 - acc: 1.0000\n","Epoch 98/100\n","600/600 [==============================] - 0s 391us/step - loss: 8.2350e-04 - acc: 1.0000\n","Epoch 99/100\n","600/600 [==============================] - 0s 393us/step - loss: 8.1717e-04 - acc: 1.0000\n","Epoch 100/100\n","600/600 [==============================] - 0s 400us/step - loss: 6.6579e-04 - acc: 1.0000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3RisDgd60zFO"},"source":["### Summarizing the model:"]},{"cell_type":"code","metadata":{"id":"QdBx2Nc1V9tI","executionInfo":{"status":"ok","timestamp":1567228468695,"user_tz":-330,"elapsed":167630,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"a4e38bb8-8b52-40fd-f30f-03fef656e0aa","colab":{"base_uri":"https://localhost:8080/","height":425}},"source":["Model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"keras_model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 64, 64, 3)         0         \n","_________________________________________________________________\n","zero_padding2d_1 (ZeroPaddin (None, 70, 70, 3)         0         \n","_________________________________________________________________\n","conv0 (Conv2D)               (None, 64, 64, 32)        4736      \n","_________________________________________________________________\n","bn0 (BatchNormalization)     (None, 64, 64, 32)        128       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 64, 64, 32)        0         \n","_________________________________________________________________\n","max_pool (MaxPooling2D)      (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 32768)             0         \n","_________________________________________________________________\n","fc (Dense)                   (None, 1)                 32769     \n","=================================================================\n","Total params: 37,633\n","Trainable params: 37,569\n","Non-trainable params: 64\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GpSwuTG6Rguc"},"source":["# Building a Residual Network\n","\n","In ResNets, a \"shortcut\" or a \"skip connection\" allows the gradient to be directly backpropagated to earlier layers:\n","\n","![alt text](https://raw.githubusercontent.com/tejaslodaya/keras-signs-resnet/master/images/skip_connection_kiank.png)\n","\n","The image on the left shows the \"main path\" through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, we can form a very deep network. \n","\n","ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. \n","\n","This means that we can stack on additional ResNet blocks with little risk of harming training set performance. \n","\n","* In case of main connection, i.e. without skip connection, the network behaves like this-\n","\n","  $a^{[l]}$---> Linear ---> $ReLU$ --> $a^{[l+1]}$ --> Linear --> $ReLU$ --> $a^{[l+2]}$\n","\n"," Suppose the first layer shown in the figure above for without skip connection is $(l+1)^{th}$ layer. So, \n","\n","  * $z^{[l+1]} = w^{[l+1]}a^{[l]}+b^{[l+1]}$ and\n"," \n","    $a^{[l+1]}=g(z^{[l+1]})$\n","\n","  * $z^{[l+2]} = w^{[l+2]}a^{[l+1]}+b^{[l+2]}$ and\n"," \n","    $a^{[l+2]}=g(z^{[l+2]})$\n"," \n","* In case of skip connection, the network behaves like this-\n","\n","  * $a^{[l+2]}=g(z^{[l+2]}+a^{[l]})$\n","\n","Two main types of blocks are used in a **ResNet**, depending mainly on whether the input/output dimensions are same or different.\n","1. Identity Block\n","2. Convolution Block\n","\n","We are going to implement both of them. "]},{"cell_type":"markdown","metadata":{"id":"4wq8KMkDY5uZ"},"source":["## The identity block\n","The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$). \n","\n","To flesh out the different steps of what happens in a ResNet's identity block, here is an alternative diagram showing the individual steps:\n","\n","![alt text](https://datascience-enthusiast.com/figures/idblock2_kiank.png)\n","\n","The upper path is the \"shortcut path.\" The lower path is the \"main path.\" \n","\n","In this diagram, we have also made explicit the CONV2D and ReLU steps in each layer. \n","\n","To speed up training we have also added a BatchNorm step."]},{"cell_type":"markdown","metadata":{"id":"wPgHTyoiKjjL"},"source":["## The convolutional block\n","We can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path:\n","\n","![alt text](https://user-images.githubusercontent.com/6441756/33685068-3f7ed2da-da85-11e7-8ee9-98f13dff8556.png)\n","\n","The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix $W_s$) \n","\n","For example, to reduce the activation dimensions's height and width by a factor of 2, we can use a $1*1$ convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. It's main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step. \n"]},{"cell_type":"markdown","metadata":{"id":"BKwikN9J1PTA"},"source":["# Detection Algorithm"]},{"cell_type":"markdown","metadata":{"id":"P5ceNeMgdevI"},"source":["## Object Localization\n","\n","To learn Object detection,we've to first learn about object localization.\n","\n","![alt text](https://adeshpande3.github.io/assets/Localization.png)\n","\n","In case of object classification, we've just figured out whether the image is of a particular object or not.\n","\n","In case of Object localization, we just not only figure out whether the image is of a particular object or not, but we also draw a bounding box around the object. This tells us where in the picture is the object is detected.\n","\n","Now suppose, we have to build an autonomous car. So we've to localize these following objects from the image-\n","1. Pedestrian\n","2. Car\n","3. Motorcycle\n","4. Background, i.e. none of above\n","\n","So, if there is car in the image & we want to localize that, then we do the following,\n","\n","**Input Image --> ConvNet --> Flattened 1D matrix --> Softmax Classification + $b_x, b_y, b_h, b_w$**\n","\n","For localization we make the nural network to give output of 4 more numbers- $b_x, b_y, b_h, b_w$. These number parameterize the bounding box of the detected object. Where-\n","\n","* $b_x, b_y$ = Mid-point of the rectangle.\n","\n","* $b_h, b_w$ = Height & Width of the Bounding Box.\n","\n","### How we define the taget level(y) in training set for supervised learning ?\n","\n","\n","$ y = \\begin{bmatrix}\n","  p_c \\\\\n","  b_x \\\\\n","  b_y \\\\\n","  b_h \\\\\n","  b_w \\\\\n","  c_1 \\\\\n","  c_2 \\\\\n","  c_3 \\\\\n"," \\end{bmatrix}$\n"," \n","where,\n","* $p_c$ tell us whether there is any object in the image.\n","* $b_x, b_y, b_h, b_w$ identify the coordinates related to the bounding box around the detected object to localize it.\n","* $c_1, c_2, c_3$ tell us of which class the object belongs to. Here we have 3 classes - Pedestrian, Car, Motorcycle.\n","\n","Here we're assuming our image has only one object.\n","\n","Now **if the object in the image is a car**, then \n","$ y = \\begin{bmatrix}\n","  1 \\\\\n","  b_x \\\\\n","  b_y \\\\\n","  b_h \\\\\n","  b_w \\\\\n","  0 \\\\\n","  1 \\\\\n","  0 \\\\\n"," \\end{bmatrix}$\n"," \n","where,\n","* $p_c=1$ as there is a car in the image.\n","* $b_x, b_y, b_h, b_w$ exists as there is an object to localize.\n","* $c_2=1$ & $c_1=c_3=0$ as the car belongs to the 2nd class.\n","\n","Now **if the object in the image is a car**, then\n","$ y = \\begin{bmatrix}\n","  0 \\\\\n","  ? \\\\\n","  ? \\\\\n","  ? \\\\\n","  ? \\\\\n","  ? \\\\\n","  ? \\\\\n","  ? \\\\\n"," \\end{bmatrix}$\n","\n","where,\n","* $p_c=0$ as there is no object in the image.\n","* Rest of the parameters are 'don't care' i.e. denoted by '?'.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NFwi-RbU2Uei"},"source":["## Landmark Detection\n","\n","Suppose we've a facial image of a human being and we want to detect eyes from the face. \n","\n","Then how do we do that?\n","\n","For that we use landmark detection. Where rather than localizing, we detect different part of the object.\n","\n","Here is an image that shows different facial landmarks.\n","![alt text](https://fiverr-res.cloudinary.com/images/t_main1,q_auto,f_auto/gigs/118915922/original/c6aba283b52adad81979ba784d28dd0bee98e98e/prepare-facial-landmark-detection-dataset.jpeg)\n","\n","Now suppose we've 64 facial landmarks. Thses landmarks can be defined as \n","\n","$ y = \\begin{bmatrix}\n","  l_{1x} & l_{1y} \\\\\n","  l_{2x} & l_{2y} \\\\\n","  l_{3x} & l_{3y} \\\\\n","  \\vdots & \\vdots \\\\\n","  l_{64x} & l_{64y} \\\\\n"," \\end{bmatrix}$\n","\n","where $l_{nx}$ (where $n = 1 \\cdots 64$) defines the position of each landmark with respect to $x-y$ co-ordinate.\n","\n","So to detect landmark we'll be doing the following-\n","\n","**Image --> ConvNet --> Flattened 1D matrix i.e. $ y = \\begin{bmatrix}\n","  face? \\\\\n","  l_{1x} \\\\\n","  l_{1y} \\\\\n","  \\vdots \\\\\n","  l_{64x} \\\\\n","  l_{64y} \\\\\n"," \\end{bmatrix}$ i.e 129 output sample ($64*2$ facial landmarks + 1 that is used to classify a face in the image)**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"O9vYoOe48w4M"},"source":["## Object Detection\n","Suppose we want to detect a car from an image.\n","\n","So to do that we should have a labelled training set of cropped images of cars. \n","\n","We need crooped images so that we can feed $x$ using only the images of cars, not with the background. \n","\n","Then we'll label those images using $1$ and $0$. Where presence of car is defined by $1$.\n","\n","Then we'll train this data using ConvNet and it'll output $y$ i.e. $0$ or $1$\n","\n","### Sliding Window detection:\n","\n","![alt text](https://raw.githubusercontent.com/PnYuan/Practice-of-Machine-Learning/master/imgs/object_detect/concept_sliding_windows.gif)\n","\n","As we can see this is a picture of a car. This red squared box which is sliding arond the image is called a window. This window sees whether there is any car in the region of the classified image that is covered by it.\n","\n","We run sliding window multiple times over the image with different window size, from smaller to larger, hoping a window size would fit the car and will be able to detect the car.\n","\n","Computational cost is a huge disadvantage of sliding window algorithm. We have to crop so many regions and run convnet for each of them individually. Increasing window and stride size makes it faster but at cost of decreased accuracy.\n","\n","To get rid of this problem we implement convolution in sliding windows detection algorithm.\n","\n","### Convolution implementation of Sliding Window detection:\n","\n","This image below shows the process of turning Fully Connected Flattened layer into Convolutional Layer:\n","\n"," ![alt text](https://lh3.googleusercontent.com/bCI3umjFXETlD813JzX5c3Of4opdU-WqMo9iKzFN8ALr2hIAguKibB7x4wVrAMT_EvLGsjpqWzTDzng-SQlVJ1VzhC8qwmuyEHZgg5HEIUklXs_zqr9GIA2w62WEQ9iY9MK5ikOERg=w2400)\n","\n","  Previously we implemented a Fully Connected layer of 400 pixels from $5*5*16$ convolution layer that we got after performing $2*2$ pooling.\n","  \n","  Now,\n","  * We've applied 400 $5*5*16$ filters with the output that we received after applying Maxpooling. So we received $1*1*400$ sized convolution layer. \n","  * Then again we're applying a filter of $1*1*400$ and getting a $1*1*400$ sized convolution layer.\n","  * Now again we're applying 4 $1*1$ filter and getting a $1*1*4$ sized convolution layer.\n","  \n","How Convolution implementation of Sliding Window detection works ?\n","  \n","![alt text](https://slideplayer.com/slide/10395667/35/images/26/Efficient+Sliding+Window%3A+Overfeat.jpg)\n","\n","* At the top row of convolution implementation, we can see the after applying ConvNet the input image of $14*14*3$ returning a output of image $1*1*4$.\n","* At the middle row of convolution implementation,\n","  * We have $16*16*3$ input image, where the size of the window is $14*14*3$.\n","  * We are using stride of $1$, so the window will slide $4$ times in the input image(2 horizontaly & 2 verticaly).\n","  * Thus it'll return us $2*2$ output. As the window image which is coloured in blue is in top-right corner of the input image, the output is showing accordingly.\n","  \n","Using convolution, instead of applying ConvNet several time for detection, we can do the same all at time."]},{"cell_type":"markdown","metadata":{"id":"OE79jpFXYMNN"},"source":["## YOLO\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-17-46-32.png)\n","\n","Here segmenting this image in 9 seperate bounding boxes($3*3$) for accurate classification & localization.\n","\n","Now we'll be applying image classification & localization algorithm to each of the 9 grid.\n","\n","So labels of training for each grid cell for 3 classes(Pedestrian, Class, Motorcycle) will be- $ y = \\begin{bmatrix}\n","  p_c \\\\\n","  b_x \\\\\n","  b_y \\\\\n","  b_h \\\\\n","  b_w \\\\\n","  c_1 \\\\\n","  c_2 \\\\\n","  c_3 \\\\\n"," \\end{bmatrix}$\n","\n","As we can see, $y$ is a 8-Dimensional output vector. So our target output will be $3*3*8$.\n","Where any one $1*1*8$ dimensional vector of the $3*3*8$ represents any one bounding box of that particular image.\n","\n","In one word, we can say we have an i/p image($x$) of $100*100*3$ dimension. We'll be applying ConvNet to get output($y$) of $3*3*8$ dimension.\n","\n","### Intersection Over Union(IoU):\n","IoU is a measure of the overlap between 2 bounding boxes.\n","\n","To check whether our Object Detection Algorithm is working well or not we use this algorithm.\n","\n","If our calculated bounding box's position is not equals to the original bounding box's position then we divide the area of intersection or overlap by area of union\n","\n","![alt text](https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png)\n","\n","If the result of IoU $=$ 1 then can say the detection algorithm is doing perfectly well.\n","\n","![alt text](https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_examples.png)\n","\n","### Non-max Supression:\n","Now suppose we've segmented the above image in 19 separate bounding boxes.\n","\n","So, we can say we have an i/p image($x$) of $100*100*3$ dimension. We'll be applying ConvNet to get output($y$) of $19*19*8$ dimension.\n","\n","As the number of boundig boxes are more here so the number of boundig boxes covering the enitre car is more. So there might be multiple bounding boxes which overlaps with each other but loaclizes the car.\n","\n","Non-max suppression suppresses those bounding boxes among many overlaped bounding boxes which IoU is less. And the bounding boxes localizing the entire car with highest IoU is kept.\n","\n","![alt text](https://appsilon.com/wp-content/uploads/2018/08/nonmax-1.png)\n","\n","### Anchor Boxes:\n","These alogrithms till now can only classify one object of one class with in a bounding box. Anchor box comes in handy when we need to classify 2 different images of 2 different classes. \n","\n","Anchor Box looks like this-\n","\n","![alt text](https://www.mathworks.com/help/vision/ug/anchorbox_whatis.png)\n","\n","Now suppose we have to classify a cycle and a dog with in same boundix boxes.\n","\n","![alt text](https://media.graphcms.com/resize=w:1024,h:304,fit:crop/output=format:webp/compress/8uGAhsw7Q1CeSjNhw7tM)\n","\n","Now the cycle looks like the Anchor box 1 and the dog looks like the Anchor box 2.\n","\n","In this case, $ y = \\begin{bmatrix}\n","  p_c \\\\\n","  b_x \\\\\n","  b_y \\\\\n","  b_h \\\\\n","  b_w \\\\\n","  c_1 \\\\\n","  c_2 \\\\\n","  c_3 \\\\\n","  p_c' \\\\\n","  b_x' \\\\\n","  b_y' \\\\\n","  b_h' \\\\\n","  b_w' \\\\\n","  c_1' \\\\\n","  c_2' \\\\\n","  c_3' \\\\\n"," \\end{bmatrix}$ where $\\begin{bmatrix}\n","  p_c \\\\\n","  b_x \\\\\n","  b_y \\\\\n","  b_h \\\\\n","  b_w \\\\\n","  c_1 \\\\\n","  c_2 \\\\\n","  c_3 \\\\\n"," \\end{bmatrix}$ is for anchor box 1 and $\\begin{bmatrix}\n","  p_c' \\\\\n","  b_x' \\\\\n","  b_y' \\\\\n","  b_h' \\\\\n","  b_w' \\\\\n","  c_1' \\\\\n","  c_2' \\\\\n","  c_3' \\\\\n"," \\end{bmatrix}$ is for anchor box 2.\n","\n"," Now if we have 3 classes and the dog is of class 1 and the cycle is of class 2 then $ y = \\begin{bmatrix}\n","  1 \\\\\n","  b_x \\\\\n","  b_y \\\\\n","  b_h \\\\\n","  b_w \\\\\n","  0 \\\\\n","  1 \\\\\n","  0 \\\\\n","  1 \\\\\n","  b_x' \\\\\n","  b_y' \\\\\n","  b_h' \\\\\n","  b_w' \\\\\n","  1 \\\\\n","  0 \\\\\n","  0 \\\\\n"," \\end{bmatrix}$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tj6KMEZm4sG7"},"source":["# Face Recognition\n","## One shot learning:\n","In this case, we only use one training example of a human face to train the ConvNet.\n","\n","Now suppose we have to recognise multiple faces and to do that we have 4 faces of 4 different people - $a,b,c,d$. We have to feed those 4 faces with ConvNet and we will get some output.\n","\n","Now suppose the face $a$ is front facing and we have an unknown face, i.e. $a_1$ which is nothing but the side face of $a$. So it is obvious the difference of the output after feeding those images to ConvNet will be minimum.\n","\n","Now what one shot learning does is when an unkown face comes to be recognised that face is also undergoes through ConvNet and one shot learning finds the similarity of that unknown face with the other faces that are being trained in the network. If the similarity level of 2 faces matches or crosses a particular threshould value, then this algorithm tells us that those 2 faces are of same people.\n","\n","### Siamese Network:\n","Now suppose we're feeding $a$ to a ConvNet and giving output of 128 vectors i.e. $f(a)$.\n","\n","Now we're feeding another face $b$ to a ConvNet and this also gives us 128 vectors as an output i.e. $f(b)$.\n","\n","Now we'll find the differences between these 2 vectors i.e. $d(a,b)=||f(a)-f(b)||^2$\n","\n","The algorithm of Siamese Network is if the images $a$ & $b$ are of same person, then $d(a,b)$ will be less & if they are of different person, $d(a,b)$ will be high.\n","\n","### Triplet Loss\n","This is a function where we need three images to define it, one is Anchor($A$) image, another is abbreviated as anchor positive($P$) and anchor negetive($N$). \n","\n","For the above examples, Anchor is $a$, anchor positive is $a_1$ & anchor negetive is $b$.\n","\n","Now if we define Triplet Loss on these 3 images, we'll get-\n","$L(a,a_1,b)=max((||f(a)-f(a_1)||^2-||f(a)-f(b)||^2+\\alpha$),0) where, $\\alpha$ is known as Margin. If $\\alpha$ is set to 0.2, then $||f(a)-f(a_1)||^2-||f(a)-f(b)||^2 \\geq 0.2$. That means if $||f(a)-f(a_1)||^2=0.5$ then $||f(a)-f(b)||^2$ will be $\\geq 0.7$.\n","\n","To implement this loss function only a single image of a person will not do the job. We need multiple images of Anchor($A$) & Anchor Positive($P$) to train the network for this loss function.\n","\n"]},{"cell_type":"code","metadata":{"id":"rdOiacbzBktv"},"source":[""],"execution_count":null,"outputs":[]}]}