{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic Regression as a NN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VKNflkLsTKXE"},"source":["# LOGISTIC REGRESSION AS A NEURAL NETWORK\n"]},{"cell_type":"markdown","metadata":{"id":"jE95nS3ZTHSv"},"source":["# Binary Classification\n","\n","---\n","\n","\n","let's suppose we have an input of an image of 64 by 64-\n","\n","We want to output a label to recognize this image as either being a cat, in which case our output is 1, or not-cat in which case our output is 0, and we're going to use 'y' to denote the output label.\n","\n","To store an color image our computer stores three separate matrices corresponding to the red, green, and blue color channels of the image.\n","\n","So if our input image is 64 pixels by 64 pixels, then we would have 3, 64 by 64 matrices corresponding to the red, green and blue pixel intensity values for our images.\n","\n","![alt text](https://forums.fast.ai/uploads/default/original/2X/0/0d44d6a8261812a76e77abfe0403bb6d70876009.png)\n","\n","**[Fig.1]**\n","\n","So to turn these pixel intensity values into a feature vector, we'll unroll all of these pixel values into an input feature vector '$x$'. So to unroll all these pixel intensity values into Feature vector, we're going to define a feature vector '$x$' corresponding to this image. We're just going to take all the pixel values until we've listed all the red pixels. And we'll keep taking the pixel values until we get a long feature vector listing out all the red, green and blue pixel intensity values of this image.\n","\n","Here we've shown the conversion of 3D matrix to 1D matrix using a 7 by 5 by 3 image. Where the height and the width of the image is represented by 7 and 5 pixels respectively and being a colour image it has 3 layers of RGB values which is shown in Fig.1\n","\n",">$x =\n"," \\begin{pmatrix}\n","  253\\\\\n","  144\\\\\n","  \\vdots\\\\\n","  74\\\\\n","  \\vdots\\\\\n","  14\\\\\n","  125\\\\\n","  \\vdots\\\\\n","  196\\\\\n","  \\vdots\\\\\n","  165\\\\\n","  187\\\\\n","  \\vdots\\\\\n","  218\n"," \\end{pmatrix}$\n","\n","If this image is a 64 by 64 image, the total dimension of this vector '$x$' will be 64 by 64 by 3 because that's the total numbers we have in all of these matrixes. Which in this case, turns out to be 12,288, that's what we get if we multiply all those numbers. And so we're going to use '$n_{x}$'=12288 to represent the dimension of the input features '$x$'.\n","\n","$n_{x} = 64*64*3 = 12288$\n","\n","So in binary classification, our goal is to learn a classifier that can input an image represented by this feature vector '$x$' and predict whether the corresponding label '$y$' is 1 or 0, that is, whether this is a cat image or a non-cat image.\n","\n","#### Notations:\n","\n","* A single training example is represented by a pair, $(x,y)$ where $x$ is an $x$-dimensional feature vector and $y$, the label, is either 0 or 1.\n","* Our training set will be represented by '$m$' or '$M_{train}$'. '$m$' training examples : $(x^1, y^1), (x^2, y^2),.........(x^m, y^m) $.\n","* Finally, to output all of the training examples into a more compact notation, we're going to define a matrix($X$).\n",">$X =\n"," \\begin{pmatrix}\n","  \\vdots  & \\vdots  & \\vdots & \\vdots \\\\\n","  x^1 & x^2 &\\cdots & x^m\\\\\n","  \\vdots  & \\vdots  & \\vdots & \\vdots \\\\\n"," \\end{pmatrix}$\n"," \n"," where the width of the matrix is represented by '$m$' training examples and the height of the matrix is represented by '$n_{x}$'. So in python **x.shape** = ($n_{x}, m$)\n","* For the output labels '$Y$',' it turns out that to make our implementation of a neural network easier, it would be convenient to also stack '$Y$'' in columns.\n",">$Y =\n"," \\begin{pmatrix}\n","  y^1 & y^2 &\\cdots & y^m\\\\\n"," \\end{pmatrix}$\n"," \n"," So '$Y$' here is an $1/m$ dimensional matrix. And again, to use the notation without the shape of Y will be 1, m. Which just means this is a 1 by m matrix. So in python **y.shape** = ($1, m$)\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r5QaJr0BTe3-"},"source":["# Logistic Regression\n","\n","---\n","\n","\n","This is a learning algorithm that we use when the output labels '$Y$'' in a supervised learning problem are all either 0 or 1, so for binary classification problems\n","\n","Let's say given an i/p feature vector $x$ corresponding to an image that we want to recognise whether it is a cat picture or not. We want an algorithm that can o/p the prediction ($\\hat{y}$) and say the probability of $y = 1$ given i/p feature $x$.\n","\n","So, $\\hat{y} = P(y = 1|x)$\n","\n","In one word if '$x$' is a picture of cat then '$\\hat{y}$' will tell us whether the picture is of a cat or not.\n","\n","So as we've said earlier '$x$' is a dimensional feature vector and '$w$' and '$b$' are the parameters of Logistic Regression where '$w$' is an '$x$' dimensional feature vector and '$b$' is a real number.\n","\n","'$w$' is known as Weight and '$b$' is known as Bias Term.\n","\n","So the output $\\hat{y}$ can be written as,  $\\hat{y} = (w^T*x + b)$.\n","\n","But this is an logistic regression used for binary classification so, $0\\leq \\hat{y} \\leq 1$. But using above equation the value of '$\\hat{y}$' may be $\\gg$ 1. So we use *'sigmoid function'*.\n","\n","Rewriting the o/p using sigmoid function, $\\hat{y} = \\sigma(w^T*x + b)\\tag{1}$.\n","\n","Equation of **Sigmoid Function** is $\\sigma{(z)} = \\frac{1}{1 + \\mathrm{e}^{-z}}\\tag{2}$\n","\n","Sigmoid Function looks like -\n","\n","![alt text](https://i0.wp.com/www.stokastik.in/wp-content/uploads/2017/07/sigmoid.png?resize=400%2C300)\n"]},{"cell_type":"markdown","metadata":{"id":"i23Y8O1XTl5n"},"source":["#### Defining sigmoid function in Python:"]},{"cell_type":"code","metadata":{"id":"YDEyExQgTBIr"},"source":["import numpy as np\n","def sigmoid(z):\n","  s = 1/(1+np.exp(-z))\n","  return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpyCY8MeTyOY","executionInfo":{"status":"ok","timestamp":1574270997865,"user_tz":-330,"elapsed":1293,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBvbJLem4wdXZ1CLjT853iS171bRJPmZB6Q6PRg=s64","userId":"08800988258615144457"}},"outputId":"e8c01e4a-9dc2-43e9-9918-18285144c8df","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x = np.array([100, -100])\n","sigmoid(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.00000000e+00, 3.72007598e-44])"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"Mxc8jwvyTwKg"},"source":["#### Observing the output of the sigmoid function using different values of $z$:"]},{"cell_type":"markdown","metadata":{"id":"LY37l4_REBvU"},"source":["#### Sigmoid gradient\n","We need to compute gradients to optimize loss functions using backpropagation.\n","\n","Now implementing the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is: $sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{3}$\n"]},{"cell_type":"code","metadata":{"id":"plzLc1mREBUd"},"source":["def sigmoid_derivative(x):\n","    s = 1/(1+np.exp(-x))\n","    ds = s*(1-s)\n","    \n","    return ds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yME3IegQIUl8","executionInfo":{"status":"ok","timestamp":1563303226798,"user_tz":-330,"elapsed":662,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"c6365b99-8e78-4f7f-fbfd-649333a3afcc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x = np.array([1, 2, 3])\n","print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NUJkLNmdT-9S"},"source":["To learn parameters for our model we're given a training set of m training examples ${(x^1, y^1),......(x^m, y^m)}$ and we want to find parameters $W$ and $B$ so that at least on the training set, the outputs we have($\\hat{y}$) will be close to the ground truth labels ($y^i$) that we got in the training set.\n","\n","## Loss Function:\n","It measure how well our model is doing in a single training example. Usually we use squared error loss function which looks like $L(\\hat{y}, y) = \\frac{1}{2}(\\hat{y}-y)^2\\tag{4}$ which doesnot optimize gradiant descent so well. \n","\n","So for Logistic Regression, we use a different error function, named Crossentropy loss, $L(\\hat{y}, y) = -[ylog\\hat{y}+(1-y)log(1-\\hat{y})]\\tag{5}$.\n","* If $y = 1$ $\\hat{y}$ will be large.\n","* If $y = 0$ $\\hat{y}$ will be small."]},{"cell_type":"markdown","metadata":{"id":"BKSd_-5mUFUE"},"source":["## Cost Function:\n","It measure how well our model is doing in the entire training example.\n","\n","Cost function looks like,\n","\n","$J(w,b) = \\frac{1}{m}\\sum_{i=i}^{m}L(\\hat{y}^i, y^i)\\tag{6}$\n","\n","or, $J(w,b) =\\frac{1}{m}\\sum_{i=i}^{m}[y^ilog\\hat{y}^i+(1-y^i)log(1-\\hat{y}^i)]\\tag{7}$\n","\n","While training the logistic regresion model we'll try to calculate $w$ & $b$ so that the cost function $J$ can be minimized.\n"]},{"cell_type":"markdown","metadata":{"id":"tyGI3Dq4UW4b"},"source":["## Gradinat Descent:\n","It is used to train the parameter $w$ and $b$ on our training set so that the cost function $J(w,b)$ will be minimized.\n","\n","The cost function $J(w,b)$ is a convex function. The $w$ & $b$ is in horizontal plane and the cost function $J(w,b)$ is in vertical plane.\n","\n","![alt text](https://miro.medium.com/max/872/0*JHD5ObJvzSxxCqCu)\n","\n","For easier understanding of the working of Gradiant Descent we've considered a 2D plot of $J$ and $w$ -\n","\n","![alt text](https://sebastianraschka.com/images/blog/2015/singlelayer_neural_networks_files/perceptron_gradient_descent_1.png)\n","\n","To train the parameters we have to randomly take any values for the parameters. For logistic regression we initialize 'w' and 'b' by 0. Gradiant descent takes the point where the value is in the graph and take it one step down and finally bring it to the minimum($J_{min}(W)$) which is known as **Global Optimum**.\n","\n","An algorithm for Gradiant Descent $w: = w - \\alpha \\frac{\\delta J(w)}{\\delta w}$, \n","where \n","* $w:$ is the next updated value of $w$ after permforming Gradiant Descent.\n","* '$\\alpha$' is known as Learning Rate, \n","* $\\frac{\\delta J(w)}{\\delta w}$ is also represented as '$\\delta w$'.\n","\n","so the equation can be written as $w: = w -\\alpha \\delta w$.\n","\n","Now '$\\delta w$' is the slope of the function of the point. At the initial point the derivative is +ve so the to obtain $w:$ the $\\alpha \\delta w$ term gets substracted from $w$.\n","\n","So for both parameters '$w$' and '$b$' the algorithm can be written as,\n","$w: = w - \\alpha \\frac{\\delta J(w, b)}{\\delta w}\\tag{8}$\n","$b: = b - \\alpha \\frac{\\delta J(w, b)}{\\delta b}\\tag{9}$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y39vorjRb6Px"},"source":["## General Architecture of the learning algorithm\n","![alt text](https://ask.qcloudimg.com/http-save/yehe-781483/fg03pr1cuo.jpeg?imageView2/2/w/1620)"]},{"cell_type":"markdown","metadata":{"id":"qefDoeBxDOpI"},"source":["## Logistic Regression using Computational Graph\n","\n","Understanding Logistic Regression using computational graph will make a bit more sense when we talk about full-fledged neural networks.\n","![alt text](https://miro.medium.com/max/1400/1*1E9fW_LIXpGoQYQcaSkzZQ.png)"]},{"cell_type":"markdown","metadata":{"id":"IEvJ5vF_IFhU"},"source":["## Vectorization:\n","* For logistic regression we calculate the prediction($\\hat{y}$) using sigmoid function ($\\sigma(z)$). So $\\hat{y} = \\sigma (z)$ where $z = w^Tx + b$.\n","* Then we calculate the loss function  $L(\\hat{y}, y) = -[ylog\\hat{y}+(1-y)log(1-\\hat{y})]$\n","\n","For these calculation we need to use for loops for calculating each parameter of logistic regression mentioned here. Use of 'for loop' make programming unefficient so to efficiently code our neural netowork we use **Vectorization**."]},{"cell_type":"code","metadata":{"id":"EELqnWeoT2sM"},"source":["import time\n","import numpy as np\n","a = np.random.rand(10000000)\n","b = np.random.rand(10000000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLEpHkN1M9kH"},"source":["#### Classical Method:"]},{"cell_type":"code","metadata":{"id":"mJYI_w19KBbL","executionInfo":{"status":"ok","timestamp":1563053314120,"user_tz":-330,"elapsed":5451,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"aed924cc-7e58-45f6-d8b8-af71817ec14d","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["output = 0\n","start_time = time.time()\n","for i in range(10000000):\n","  output += a[i]*b[i]\n","end_time = time.time()\n","print(output)\n","print(\"Time taken to calculate is \" + str(1000*(end_time - start_time)) + \" ms\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2500057.1236310075\n","Time taken to calculate is 4067.5551891326904 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zg_5AFMdMWKW"},"source":["#### After Vectorization:"]},{"cell_type":"code","metadata":{"id":"2MN8vuXaMFf3","executionInfo":{"status":"ok","timestamp":1563387507871,"user_tz":-330,"elapsed":3013,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"5c715077-9e30-4e35-feec-d8629e48791e","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["start_time = time.time()\n","output = np.dot(a,b)\n","end_time = time.time()\n","\n","print(output)\n","print(\"Time taken to calculate is \" + str(1000*(end_time - start_time)) + \" ms\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2499947.778014361\n","Time taken to calculate is 21.017074584960938 ms\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ftMqSje4POav"},"source":["As we can see the non-vectorized version is taking much time than the vectorized version."]},{"cell_type":"markdown","metadata":{"id":"mi3bMUUHUGiD"},"source":["## Broadcasting in python:\n","A very important concept to understand in numpy is \"broadcasting\". It is very useful for performing mathematical operations between arrays of different shapes.\n","\n","Below the number of calories from carbohydrates, proteins, and fats in 100 grams of four different foods are shown-\n","\n",">Nutrients | Apple | Beef | Egg | Potato\n",">--- | --- | --- | --- | ---\n",">Carb | 56.0 | 0.0 | 4.4 | 68.0\n",">Protein | 1.2 | 104.0 | 52.0 | 8.0\n",">Fat | 1.8 | 135.0 | 99.0 | 0.9\n","\n","To calculate the percentage of calories from carbs, proteins and fats for each of the four foods, we'll do the following,"]},{"cell_type":"code","metadata":{"id":"YJgBNtCbWE5-","executionInfo":{"status":"ok","timestamp":1563055730499,"user_tz":-330,"elapsed":1436,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"5e705302-0b31-4f07-aaea-78dc63b2f8e8","colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["import numpy as np\n","food = np.array([[56.0, 0.0, 4.4, 68.0],\n","                 [1.2, 104.0, 52.0, 8.0],\n","                 [1.8, 135.0, 99.0, 0.9]])\n","print(food)\n","print()\n","\n","calories = food.sum(axis = 0) # we want to sum vertiaclly so 'axis = 0' & if horizontally then 'axis = 1'\n","percentage = (food/calories)*100\n","print(\"The percentage of calories from carbs, proteins and fats for each of the four foods are-\")\n","print(percentage)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 56.    0.    4.4  68. ]\n"," [  1.2 104.   52.    8. ]\n"," [  1.8 135.   99.    0.9]]\n","\n","The percentage of calories from carbs, proteins and fats for each of the four foods are-\n","[[94.91525424  0.          2.83140283 88.42652796]\n"," [ 2.03389831 43.51464435 33.46203346 10.40312094]\n"," [ 3.05084746 56.48535565 63.70656371  1.17035111]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tm6XffbgGk5G"},"source":["### Softmax\n","Implementing a softmax function using numpy. We can think of softmax as a normalizing function used when our algorithm needs to classify two or more classes.\n","\n","**Instructions**:\n","- $ \\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     } softmax(x) = softmax(\\begin{bmatrix}\n","    x_1  &&\n","    x_2 &&\n","    ...  &&\n","    x_n  \n","\\end{bmatrix}) = \\begin{bmatrix}\n","     \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n","    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n","    ...  &&\n","    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n","\\end{bmatrix} $ \n","\n","- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  $$softmax(x) = softmax\\begin{bmatrix}\n","    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n","    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n","\\end{bmatrix} = \\begin{bmatrix}\n","    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n","    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n","    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n","    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n","\\end{bmatrix} = \\begin{pmatrix}\n","    softmax\\text{(first row of x)}  \\\\\n","    softmax\\text{(second row of x)} \\\\\n","    ...  \\\\\n","    softmax\\text{(last row of x)} \\\\\n","\\end{pmatrix} $$"]},{"cell_type":"code","metadata":{"id":"6bYHKstmGXN2"},"source":["def softmax(x):\n","    # Apply exp() element-wise to x. Use np.exp(...).\n","    x_exp = np.exp(x)\n","\n","    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n","    x_sum = np.sum(x_exp,axis = 1,keepdims = True)\n","    \n","    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n","    s = x_exp / x_sum\n","    \n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jIWw6eolH4eU","executionInfo":{"status":"ok","timestamp":1563386964070,"user_tz":-330,"elapsed":1248,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"b0840c86-826f-4f7f-e658-588a5892ac79","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["x = np.array([\n","    [9, 2, 5, 0, 0],\n","    [7, 5, 0, 0 ,0]])\n","print(\"softmax(x) = \" + str(softmax(x)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n","  1.21052389e-04]\n"," [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n","  8.01252314e-04]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZkXUBfZJ4FER"},"source":["# Logistic Regression with a Neural Network Python Programming\n","\n","---\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8zu8YJ_u8Spi"},"source":["## Importing libaries and Packages"]},{"cell_type":"code","metadata":{"id":"3EGJoiiuaqqo","executionInfo":{"status":"ok","timestamp":1563559726153,"user_tz":-330,"elapsed":65445,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"fbd88d44-418e-41f4-f2ad-2c304d635f29","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":110}},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-93e9c9fe-1917-4a15-a748-e683b45f9780\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-93e9c9fe-1917-4a15-a748-e683b45f9780\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving train_catvnoncat.h5 to train_catvnoncat.h5\n","Saving test_catvnoncat.h5 to test_catvnoncat.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sjTyva1gFivz"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy\n","from PIL import Image\n","from scipy import ndimage\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdMsNndBecLM"},"source":["## Loading the dataset"]},{"cell_type":"code","metadata":{"id":"vggFw-6xDzV7"},"source":["def load_dataset():\n","    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dm7bqOBvD5Rf"},"source":["train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ElEIGB1neuAS"},"source":["## Exploring the dataset"]},{"cell_type":"code","metadata":{"id":"xRGiVYMqcmDV","executionInfo":{"status":"ok","timestamp":1563560678604,"user_tz":-330,"elapsed":983,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"da2ba47f-62c9-473a-b028-f5591b291a9e","colab":{"base_uri":"https://localhost:8080/","height":287}},"source":["# Example of a picture\n","index = 25\n","plt.imshow(train_set_x_orig[index])\n","print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["y = [1], it's a 'cat' picture.\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"PorLoFuXe88R"},"source":["Many software bugs in deep learning come from having matrix/vector dimensions that don't fit. If we can keep our matrix/vector dimensions straight we will go a long way toward eliminating many bugs.\n","\n","So finding the values for:\n","\n","- m_train (number of training examples)\n","- m_test (number of test examples)\n","- num_px (= height = width of a training image)\n","\n",">*`train_set_x_orig` is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing `train_set_x_orig.shape[0]`.*"]},{"cell_type":"code","metadata":{"id":"kkvhMXWtepCK","executionInfo":{"status":"ok","timestamp":1563561052358,"user_tz":-330,"elapsed":729,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"0e0e3fc1-9710-4dd2-83a3-e7c46346b84f","colab":{"base_uri":"https://localhost:8080/","height":163}},"source":["m_train = train_set_x_orig.shape[0]\n","m_test = test_set_x_orig.shape[0]\n","num_px = train_set_x_orig.shape[2]\n","\n","print (\"Number of training examples: m_train = \" + str(m_train))\n","print (\"Number of testing examples: m_test = \" + str(m_test))\n","print (\"Height/Width of each image: num_px = \" + str(num_px))\n","print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n","print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of training examples: m_train = 209\n","Number of testing examples: m_test = 50\n","Height/Width of each image: num_px = 64\n","Each image is of size: (64, 64, 3)\n","train_set_x shape: (209, 64, 64, 3)\n","train_set_y shape: (1, 209)\n","test_set_x shape: (50, 64, 64, 3)\n","test_set_y shape: (1, 50)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gR7GHto6hRWI"},"source":["## Data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"d_5yELmwgkDj"},"source":["### Reshaping the training examples and test examples\n","Converting the 3-D matrix to 1-D matrix."]},{"cell_type":"code","metadata":{"id":"uyqPIkD9gEVn","executionInfo":{"status":"ok","timestamp":1563561234226,"user_tz":-330,"elapsed":746,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"057a2a7b-0853-49ee-b961-72711c208e3d","colab":{"base_uri":"https://localhost:8080/","height":108}},"source":["train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n","test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n","\n","print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))\n","print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train_set_x_flatten shape: (12288, 209)\n","train_set_y shape: (1, 209)\n","test_set_x_flatten shape: (12288, 50)\n","test_set_y shape: (1, 50)\n","sanity check after reshaping: [17 31 56 22 33]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Vc6yHsZhhbrf"},"source":["### Standardizing the dataset\n","To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n","\n","One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel)."]},{"cell_type":"code","metadata":{"id":"r6D_fgNpgwvb"},"source":["train_set_x = train_set_x_flatten/255.\n","test_set_x = test_set_x_flatten/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APbhO1g-iavz"},"source":["## Building the parts of our algorithm"]},{"cell_type":"markdown","metadata":{"id":"wq7MF1FMidcX"},"source":["### Building the sigmoid function\n","We need to compute $sigmoid(w^T.x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions."]},{"cell_type":"code","metadata":{"id":"xzLxnA_7hO8a"},"source":["def sigmoid(z):\n","    s = 1/(1+np.exp(-z))\n","    return s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0Mt7u_7jtCd","executionInfo":{"status":"ok","timestamp":1563562154879,"user_tz":-330,"elapsed":790,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"a799f1d5-7d2a-4396-a978-8ecc8722b370","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["sigmoid(np.array([100, -100]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.00000000e+00, 3.72007598e-44])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"C6_UShL4kV1O"},"source":["### Initializing parameters\n","You have to initialize 'w' and 'b' as a vector of zeros."]},{"cell_type":"code","metadata":{"id":"lwTJ1abZkCCD"},"source":["def initialize_with_zeros(dim):\n","    w = np.zeros(shape = (dim, 1), dtype = np.float32)\n","    b = 0\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    return w, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bjbwIEtOkr3U","executionInfo":{"status":"ok","timestamp":1563562290735,"user_tz":-330,"elapsed":775,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"b22103e3-194b-4b17-f0b8-461b7a8d299c","colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["dim = 2\n","w, b = initialize_with_zeros(dim)\n","print (\"w = \" + str(w))\n","print (\"b = \" + str(b))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w = [[0.]\n"," [0.]]\n","b = 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"D9K7WPwHk8gq"},"source":["### Forward and Backward propagation\n","Now that our parameters are initialized, we can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n","\n","Forward Propagation:\n","- We get X\n","- We compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n","- We calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n","\n","Here are the two formulas we will be using: \n","\n","$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n","$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\n","\n","In the cell below, we're implementing-\n","* Arguments :\n"," * w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n"," * b -- bias, a scalar\n"," * X -- data of size (num_px * num_px * 3, number of examples)\n"," * Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n","* Returns :\n"," * cost -- negative log-likelihood cost for logistic regression\n"," * dw -- gradient of the loss with respect to w, thus same shape as w\n"," * db -- gradient of the loss with respect to b, thus same shape as b"]},{"cell_type":"code","metadata":{"id":"Zb74IqzckyqW"},"source":["def propagate(w, b, X, Y):\n","    m = X.shape[1]\n","\n","    # FORWARD PROPAGATION (FROM X TO COST)\n","    A = sigmoid(np.dot(w.T, X) + b)                                              # computes activation\n","    cost = (-1. / m) * np.sum((Y*np.log(A) + (1 - Y)*np.log(1-A)), axis=1)       # computes cost\n","    \n","    # BACKWARD PROPAGATION (TO FIND GRAD)\n","    dw = (1./m)*np.dot(X, ((A-Y).T))\n","    db = (1./m)*np.sum(A-Y, axis = 1)\n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    \n","    grads = {\"dw\": dw,\n","             \"db\": db}\n","    \n","    return grads, cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lktxN1yzmZ85","executionInfo":{"status":"ok","timestamp":1563562751578,"user_tz":-330,"elapsed":753,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"50713bc9-b744-4c91-91e5-168fbfce34b5","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n","grads, cost = propagate(w, b, X, Y)\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))\n","print (\"cost = \" + str(cost))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dw = [[0.99845601]\n"," [2.39507239]]\n","db = [0.00145558]\n","cost = 5.801545319394553\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"spYzdSDfmy_C"},"source":["### Optimization\n","- We have initialized your parameters.\n","- We are also able to compute a cost function and its gradient.\n","- Now, We want to update the parameters using gradient descent.\n","\n","The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n","\n","In the cell below, we're implementing -     \n","* Arguments:\n","   * w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","   * b -- bias, a scalar\n","   * X -- data of shape (num_px * num_px * 3, number of examples)\n","   * Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n","   * num_iterations -- number of iterations of the optimization loop\n","   * learning_rate -- learning rate of the gradient descent update rule\n","   * print_cost -- True to print the loss every 100 steps\n","    \n","* Returns:\n","   * params -- dictionary containing the weights w and bias b\n","   * grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n","   * costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve."]},{"cell_type":"code","metadata":{"id":"r5HEZ0otmjLy"},"source":["def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): \n","  costs = []\n","  for i in range(num_iterations):\n","\n","      # Cost and gradient calculation\n","      grads, cost = propagate(w=w, b=b, X=X, Y=Y)\n","\n","      # Retrieving derivatives from grads\n","      dw = grads[\"dw\"]\n","      db = grads[\"db\"]\n","\n","      # Implementing update rule\n","      w = w-learning_rate*dw\n","      b = b-learning_rate*db\n","\n","      # Record the costs\n","      if i % 100 == 0:\n","        costs.append(cost)\n","\n","      # Printing the cost every 100 training iterations\n","      if print_cost and i % 100 == 0:\n","          print (\"Cost after iteration %i: %f\" %(i, cost))\n","\n","  params = {\"w\": w,\n","            \"b\": b}\n","\n","  grads = {\"dw\": dw,\n","           \"db\": db}\n","\n","  return params, grads, costs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPQOYh-BoXIK","executionInfo":{"status":"ok","timestamp":1563563483831,"user_tz":-330,"elapsed":566,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"f3f1e5b3-ff23-481e-f0b1-ee78351f9d80","colab":{"base_uri":"https://localhost:8080/","height":126}},"source":["params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n","\n","print (\"w = \" + str(params[\"w\"]))\n","print (\"b = \" + str(params[\"b\"]))\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["w = [[0.19033591]\n"," [0.12259159]]\n","b = [1.92535983]\n","dw = [[0.67752042]\n"," [1.41625495]]\n","db = [0.2191945]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B_Om2Ta8pf5S"},"source":["### Prediction\n","The previous function will output the learned 'w' and 'b'. We are able to use 'w' and 'b' to predict the labels for a dataset X. Implementing the `predict()` function. There are two steps to computing predictions:\n","\n","1. Calculating $\\hat{Y} = A = \\sigma(w^T X + b)$\n","\n","2. Converting the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`.\n","\n","In the cell below, we're implementing-\n","* Arguments:\n","    * w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    * b -- bias, a scalar\n","    * X -- data of size (num_px * num_px * 3, number of examples)\n","    \n","* Returns:\n","    * Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X"]},{"cell_type":"code","metadata":{"id":"xJsKyaaVobCn"},"source":["def predict(w, b, X):    \n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    \n","    # Computing vector \"A\" predicting the probabilities of a cat being present in the picture\n","    A = sigmoid(np.dot((w.T), X)+b)\n","    \n","    for i in range(A.shape[1]):\n","        \n","        # Converting probabilities A[0,i] to actual predictions p[0,i]\n","        if A[0, i] > 0.5:\n","            Y_prediction[0, i] = 1\n","        else :\n","            Y_prediction[0, i] = 0\n","    \n","    assert(Y_prediction.shape == (1, m))\n","    \n","    return Y_prediction"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_e_D1WK8qaQm","executionInfo":{"status":"ok","timestamp":1563563771832,"user_tz":-330,"elapsed":790,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"7580bba5-ce1d-4e6b-8924-137e427d76ef","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["w = np.array([[0.1124579],[0.23106775]])\n","b = -0.3\n","X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n","print (\"predictions = \" + str(predict(w, b, X)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["predictions = [[1. 1. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UmuM70X3qjRc"},"source":["## Merging all functions into the logistic regression model\n","We will now see how the overall model is structured by putting together all the functions implemented in the previous parts together, in the right order.\n","\n","Implementing the model function using the following notation:\n","\n","- Y_prediction_test for our predictions on the test set\n","- Y_prediction_train for our predictions on the train set\n","- w, costs, grads for the outputs of optimize()\n","\n","In the cell below, we're implementing-\n","* Arguments:\n","    * X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n","    * Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n","    * X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n","    * Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n","    * num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n","    * learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n","    * print_cost -- Set to true to print the cost every 100 iterations\n","    \n","* Returns:\n","    * d -- dictionary containing information about the model."]},{"cell_type":"code","metadata":{"id":"7ZQdzn31qcQr"},"source":["def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","  \n","    # initializing parameters with zeros\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","    \n","    # Gradient descent\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n","    \n","    # Retrieving parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    \n","    # Predicting test/train set examples\n","    Y_prediction_test = predict(w, b, X_test)\n","    Y_prediction_train = predict(w, b, X_train)\n","\n","    # Printing train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","\n","    \n","    d = {\"costs\": costs,\n","         \"Y_prediction_test\": Y_prediction_test, \n","         \"Y_prediction_train\" : Y_prediction_train, \n","         \"w\" : w, \n","         \"b\" : b,\n","         \"learning_rate\" : learning_rate,\n","         \"num_iterations\": num_iterations}\n","    \n","    return d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kZ2dY5ECrigm","executionInfo":{"status":"ok","timestamp":1563564072386,"user_tz":-330,"elapsed":2977,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"b96aaf6b-7ee9-4b08-ce44-309cc9f8be7f","colab":{"base_uri":"https://localhost:8080/","height":417}},"source":["d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cost after iteration 0: 0.693147\n","Cost after iteration 100: 0.584508\n","Cost after iteration 200: 0.466949\n","Cost after iteration 300: 0.376007\n","Cost after iteration 400: 0.331463\n","Cost after iteration 500: 0.303273\n","Cost after iteration 600: 0.279880\n","Cost after iteration 700: 0.260042\n","Cost after iteration 800: 0.242941\n","Cost after iteration 900: 0.228004\n","Cost after iteration 1000: 0.214820\n","Cost after iteration 1100: 0.203078\n","Cost after iteration 1200: 0.192544\n","Cost after iteration 1300: 0.183033\n","Cost after iteration 1400: 0.174399\n","Cost after iteration 1500: 0.166521\n","Cost after iteration 1600: 0.159305\n","Cost after iteration 1700: 0.152667\n","Cost after iteration 1800: 0.146542\n","Cost after iteration 1900: 0.140872\n","train accuracy: 99.04306220095694 %\n","test accuracy: 70.0 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kweCA5X0r4E1","executionInfo":{"status":"ok","timestamp":1563564515321,"user_tz":-330,"elapsed":929,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"7f547b40-db67-4470-f9a7-3b6495d98ed7","colab":{"base_uri":"https://localhost:8080/","height":295}},"source":["# Plot learning curve (with costs)\n","costs = np.squeeze(d['costs'])\n","plt.plot(costs)\n","plt.ylabel('cost')\n","plt.xlabel('iterations (per hundreds)')\n","plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naaVQEBGRVgoFERFp9f8BoOz8cLQZtuMAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]}]}