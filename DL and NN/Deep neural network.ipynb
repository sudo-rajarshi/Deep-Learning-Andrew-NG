{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deep neural network.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6tb5a3MmVdSX","colab_type":"text"},"source":["# Deep Neural Network Representation\n","\n","---\n","\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/10/Screenshot-from-2018-10-16-12-16-27.png)\n","\n","* This is a 4 layer Neural Network\n","* Here we have an i/p layer consisting 3 i/ps' - $x_{1}$, $x_{2}$, $x_{3}$ and then we have 3 hidden layers and an output layer ($\\hat{y}$).\n","* $\\mathrm{n}^{[l]}$ = no. of units in layer $l$. \n","  \n","   Here $\\mathrm{n}^{[0]} = 3$, $\\mathrm{n}^{[1]} = 5$, $\\mathrm{n}^{[2]} = 5$, $\\mathrm{n}^{[3]} = 3$, $\\mathrm{n}^{[4]} = 1$\n","* $\\mathrm{a}^{[l]}$ = activation function of layer $l$  =  $\\mathrm{g}^{[l]}(\\mathrm{z}^{[l]})$\n","\n","   Here $\\mathrm{a}^{[0]} = x$, $\\mathrm{a}^{[l]} = \\hat{y}$"]},{"cell_type":"markdown","metadata":{"id":"k8dxL4MKrLGH","colab_type":"text"},"source":["# Forward & Backward Propagation in Deep Neural Network\n","\n","---\n","\n","## Forward Propagation:\n","* O/P from the 1st hidden layer is, $\\mathrm{a}^{[1]} = \\mathrm{g}^{[1]}(\\mathrm{z}^{[1]})$, where $\\mathrm{z}^{[1]} = \\mathrm{w}^{[1]}*\\mathrm{a}^{[0]}+\\mathrm{b}^{[1]}$, *Here $\\mathrm{a}^{[0]}=x$*\n","* O/P from the 2nd hidden layer is $\\mathrm{a}^{[2]} = \\mathrm{g}^{[2]}(\\mathrm{z}^{[2]})$, where $\\mathrm{z}^{[2]} = \\mathrm{w}^{[2]}*\\mathrm{a}^{[1]}+\\mathrm{b}^{[2]}$\n","* O/P from the 3rd hidden layer is $\\mathrm{a}^{[3]} = \\mathrm{g}^{[3]}(\\mathrm{z}^{[3]})$, where $\\mathrm{z}^{[3]} = \\mathrm{w}^{[3]}*\\mathrm{a}^{[2]}+\\mathrm{b}^{[3]}$\n","* O/P from the o/p layer is $\\mathrm{a}^{[4]} = \\mathrm{g}^{[4]}(\\mathrm{z}^{[4]})$, where $\\mathrm{z}^{[4]} = \\mathrm{w}^{[4]}*\\mathrm{a}^{[3]}+\\mathrm{b}^{[4]} = \\hat{y}$ \n","\n","**General Forward Propagation Equation: $\\mathrm{a}^{[l]} = \\mathrm{g}^{[l]}(\\mathrm{z}^{[l]})$, where $\\mathrm{z}^{[l]} = \\mathrm{w}^{[l]}*\\mathrm{a}^{[l-1]}+\\mathrm{b}^{[l]}$**\n","\n","### Vectorizing Forward Propagation Equation:\n","* $\\mathrm{A}^{[1]} = \\mathrm{g}^{[1]}(\\mathrm{Z}^{[1]})$, where $\\mathrm{Z}^{[1]} = \\mathrm{W}^{[1]}*\\mathrm{A}^{[0]}+\\mathrm{b}^{[1]}$\n","* $\\mathrm{A}^{[2]} = \\mathrm{g}^{[2]}(\\mathrm{Z}^{[2]})$, where $\\mathrm{Z}^{[2]} = \\mathrm{W}^{[2]}*\\mathrm{A}^{[1]}+\\mathrm{b}^{[2]}$\n","* $\\mathrm{A}^{[3]} = \\mathrm{g}^{[3]}(\\mathrm{Z}^{[3]})$, where $\\mathrm{Z}^{[3]} = \\mathrm{W}^{[3]}*\\mathrm{A}^{[2]}+\\mathrm{b}^{[3]}$\n","* $\\mathrm{A}^{[4]} = \\mathrm{g}^{[4]}(\\mathrm{Z}^{[4]})$, where $\\mathrm{Z}^{[4]} = \\mathrm{W}^{[4]}*\\mathrm{A}^{[3]}+\\mathrm{b}^{[4]} = \\hat{y}$\n","\n","**General vectorized form of Forward Propagation Equation: $\\mathrm{A}^{[l]} = \\mathrm{g}^{[l]}(\\mathrm{Z}^{[l]})$, where $\\mathrm{Z}^{[l]} = \\mathrm{W}^{[l]}*\\mathrm{A}^{[l-1]}+\\mathrm{b}^{[l]}$**\n","\n","Here,\n","\n"," * $\\mathrm{w}^{[l]} = (\\mathrm{n}^{[l]}*\\mathrm{n}^{[l-1]})$ matrix\n","  \n"," * $\\mathrm{b}^{[l]} = (\\mathrm{n}^{[l]}*1)$ matrix\n","  \n"," * $\\mathrm{Z}^{[l]} = \n"," \\begin{pmatrix}\n","  \\mathrm{z}^{[l]}(1) & \\mathrm{z}^{[l]}(2) & \\cdots & \\mathrm{z}^{[l]}(m) \\\\ \n"," \\end{pmatrix}$ . Here $\\mathrm{z}^{[l]} =(\\mathrm{n}^{[l]}*1)$ matrix\n"," \n"," * $\\mathrm{A}^{[l]} = \n"," \\begin{pmatrix}\n","  \\mathrm{a}^{[l]}(1) & \\mathrm{a}^{[l]}(2) & \\cdots & \\mathrm{a}^{[l]}(m) \\\\ \n"," \\end{pmatrix}$. Here $\\mathrm{a}^{[l]} =(\\mathrm{n}^{[l-1]}*1)$ matrix\n"," \n","## Backward Propagation:\n","* $dW_{1} = \\frac{dL}{dW_{1}} = x_{1}*dz$\n","* $dW_{2} = \\frac{dL}{dW_{2}} = x_{2}*dz$\n","* $dW_{3} = \\frac{dL}{dW_{3}} = x_{3}*dz$\n","* $db = \\frac{dL}{dB} = dz$"]},{"cell_type":"markdown","metadata":{"id":"2hUaGcKcDaBF","colab_type":"text"},"source":["# Parameters & Hyperparameters\n","For a Neural Network model, $\\mathrm{w}^{[l]}, \\mathrm{b}^{[l]}$ are main parameters.\n","\n","Some of the hyperparameters are-\n","1. Learning Rate $\\alpha$\n","2. Number of iterations\n","3. Number of hidden layers\n","4. Number of hidden units\n","5. Choice of activation function\n","\n","These hyperparameters controlls $w$ and $b$."]},{"cell_type":"markdown","metadata":{"id":"NmgBWXzoQJjg","colab_type":"text"},"source":["# Building DNN step by step"]},{"cell_type":"code","metadata":{"id":"NxxHI8ceQ15q","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u01CLznKQSr4","colab_type":"text"},"source":["## Implementing Sigmoid Function\n","\n","\n","\n","```\n","    Arguments:\n","    Z -- numpy array of any shape\n","    \n","    Returns:\n","    A -- output of sigmoid(z), same shape as Z\n","    cache -- returns Z as well, useful during backpropagation\n","```\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"p02TbzW3TDPz","colab_type":"code","colab":{}},"source":["def sigmoid(Z):\n","    \n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","    \n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KknYHMnVc1aX","colab_type":"code","colab":{}},"source":["def sigmoid_test_case():\n","  \n","  z = np.random.randn()\n","  \n","  return z"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2yrhrznQXWL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1563915248611,"user_tz":-330,"elapsed":1227,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"3ecf4ed4-bbbd-4cd3-a8f3-21843b3627ae"},"source":["z = sigmoid_test_case()\n","A, cache = sigmoid(z)\n","\n","print(\"z =\" + str(z))\n","print(\"Sigmoid = \" + str(A))\n","print(\"Cache = \" + str(cache))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["z =-0.19282691566774998\n","Sigmoid = 0.4519420872071204\n","Cache = -0.19282691566774998\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d2H2xrUaQ_dq","colab_type":"text"},"source":["## Implementing ReLU Function\n","\n","\n","```\n","    Arguments:\n","    Z -- Output of the linear layer, of any shape\n","\n","    Returns:\n","    A -- Post-activation parameter, of the same shape as Z\n","    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"DsE8OFY4Qy7b","colab_type":"code","colab":{}},"source":["def relu(Z):\n","\n","    A = np.maximum(0,Z)\n","    \n","    assert(A.shape == Z.shape)\n","    \n","    cache = Z \n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3YuAEFzlec3t","colab_type":"code","colab":{}},"source":["def relu_test_case():\n","  \n","  z = np.random.randn()\n","  \n","  return z"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NQ6ilRaemtT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1563915251022,"user_tz":-330,"elapsed":1195,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"b9b4383b-6f6f-40cd-a64e-3040f9c99642"},"source":["z = relu_test_case()\n","A, cache = relu(np.array([z]))\n","\n","print(\"z =\" + str(z))\n","print(\"Relu = \" + str(A))\n","print(\"Cache = \" + str(cache))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["z =-0.22916360952618312\n","Relu = [0.]\n","Cache = [-0.22916361]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oRjy7DEDRQ6x","colab_type":"text"},"source":["## Implementing the backward propagation for a single SIGMOID unit\n","\n","\n","```\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"4ILNS0L1RGoc","colab_type":"code","colab":{}},"source":["def sigmoid_backward(dA, cache):\n","    \n","    Z = cache\n","    \n","    s = 1/(1+np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlRJZ4ycfVmM","colab_type":"code","colab":{}},"source":["def sigmoid_backward_test_case():\n","  \n","  dA = np.random.randn()\n","  cache = np.random.randn()\n","  \n","  return dA, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESm-7OU2f0iu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1563915252659,"user_tz":-330,"elapsed":953,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"a326b9e3-6b6e-402c-ec84-143e11c71f28"},"source":["dA, cache = sigmoid_backward_test_case()\n","dZ = sigmoid_backward(np.array([dA]), np.array([cache]))\n","\n","print(\"dA =\" + str(dA))\n","print(\"cache =\" + str(cache))\n","print(\"Sigmoid Backward = \" + str(dZ))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dA =0.690707054736731\n","cache =1.7260192920596784\n","Sigmoid Backward = [0.08859494]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XSdvDqzDRpec","colab_type":"text"},"source":["## Implementing the backward propagation for a single RELU unit\n","\n","\n","```\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","```\n"]},{"cell_type":"code","metadata":{"id":"Vqqo8MAeRaOK","colab_type":"code","colab":{}},"source":["def relu_backward(dA, cache):\n","    Z = cache\n","    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n","\n","    # When z <= 0, we should set dz to 0\n","    dZ[Z <= 0] = 0\n","\n","    assert (dZ.shape == Z.shape)\n","\n","    return dZ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CmlD1YI5hJT0","colab_type":"code","colab":{}},"source":["def relu_backward_test_case():\n","  \n","  dA = np.random.randn()\n","  cache = np.random.randn()\n","  \n","  return dA, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dksKw03hRcf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1563915255074,"user_tz":-330,"elapsed":1568,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"36bdc974-8b3b-43c3-adb9-d3e9ed5e2518"},"source":["dA, cache = relu_backward_test_case()\n","dZ = relu_backward(np.array([dA]), np.array([cache]))\n","\n","print(\"dA =\" + str(dA))\n","print(\"cache =\" + str(cache))\n","print(\"ReLU Backward = \" + str(dZ))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dA =0.9157550385590463\n","cache =-0.8093509617462485\n","ReLU Backward = [0.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vIahwEE4SoFT","colab_type":"text"},"source":["## Initializing Parameters\n"]},{"cell_type":"markdown","metadata":{"id":"gLCvARj-TZcu","colab_type":"text"},"source":["### For 2-Layer Neural Network\n","\n","\n","```\n","```\n","    Argument:\n","    n_x -- size of the input layer\n","    n_h -- size of the hidden layer\n","    n_y -- size of the output layer\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters:\n","                    W1 -- weight matrix of shape (n_h, n_x)\n","                    b1 -- bias vector of shape (n_h, 1)\n","                    W2 -- weight matrix of shape (n_y, n_h)\n","                    b2 -- bias vector of shape (n_y, 1)\n","```\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"y1pjVMxbSViV","colab_type":"code","colab":{}},"source":["def initialize_parameters(n_x, n_h, n_y):\n","  \n","    np.random.seed(1)\n","    \n","    W1 = np.random.randn(n_h, n_x)*0.01\n","    b1 = np.zeros((n_h, 1))\n","    W2 = np.random.randn(n_y, n_h)*0.01\n","    b2 = np.zeros((n_y, 1))\n","    \n","    assert(W1.shape == (n_h, n_x))\n","    assert(b1.shape == (n_h, 1))\n","    assert(W2.shape == (n_y, n_h))\n","    assert(b2.shape == (n_y, 1))\n","    \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","    \n","    return parameters    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bYzH6mygTjnx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1563915256905,"user_tz":-330,"elapsed":1347,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"798430d4-f87c-465b-d9e8-8af3812d289e"},"source":["parameters = initialize_parameters(3,2,1)\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n"," [-0.01072969  0.00865408 -0.02301539]]\n","b1 = [[0.]\n"," [0.]]\n","W2 = [[ 0.01744812 -0.00761207]]\n","b2 = [[0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WRIT9IX5Tv7j","colab_type":"text"},"source":["### For L-Layer Neural Network\n","\n","When we compute $W X + b$ in python, it carries out broadcasting. For example, if: \n","\n","$$ W = \\begin{bmatrix}\n","    j  & k  & l\\\\\n","    m  & n & o \\\\\n","    p  & q & r \n","\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n","    a  & b  & c\\\\\n","    d  & e & f \\\\\n","    g  & h & i \n","\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n","    s  \\\\\n","    t  \\\\\n","    u\n","\\end{bmatrix}\\tag{2}$$\n","\n","Then $WX + b$ will be:\n","\n","$$ WX + b = \\begin{bmatrix}\n","    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n","    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n","    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n","\\end{bmatrix}\\tag{3}  $$\n","\n","---\n","\n","\n","\n","```\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"XZz4hd7HTnqf","colab_type":"code","colab":{}},"source":["def initialize_parameters_deep(layer_dims):\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vo-nquYZUfsy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1563915258293,"user_tz":-330,"elapsed":1072,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"115fcb9c-747b-4341-d4de-e19d4161c779"},"source":["parameters = initialize_parameters_deep([5,4,3])\n","print(\"W1 = \" + str(parameters[\"W1\"]))\n","print(\"b1 = \" + str(parameters[\"b1\"]))\n","print(\"W2 = \" + str(parameters[\"W2\"]))\n","print(\"b2 = \" + str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n"," [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n"," [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n"," [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n","b1 = [[0.]\n"," [0.]\n"," [0.]\n"," [0.]]\n","W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n"," [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n"," [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n","b2 = [[0.]\n"," [0.]\n"," [0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XgpPmO-1Yvla","colab_type":"text"},"source":["## Forward Propagation Model\n"]},{"cell_type":"markdown","metadata":{"id":"T-a3HXuJZM6L","colab_type":"text"},"source":["### Linear Forward:\n","- LINEAR\n","- LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. \n","- [LINEAR -> RELU] $\\times$ $(L-1)$ -> LINEAR -> SIGMOID (whole model)\n","The linear forward module (vectorized over all the examples) computes the following equations:\n","\n","$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n","\n","where $A^{[0]} = X$. \n","\n","---\n","\n","\n","\n","\n","```\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"PM56FI_zXjLL","colab_type":"code","colab":{}},"source":["def linear_forward(A, W, b):\n","    \n","    Z = np.dot(W, A)+b\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9bENNrPLZy0P","colab_type":"code","colab":{}},"source":["def linear_forward_test_case():\n","\n","    A = np.random.randn(3,2)\n","    W = np.random.randn(1,3)\n","    b = np.random.randn(1,1)\n","    \n","    return A, W, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2RfOCDmZnS1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1563915261119,"user_tz":-330,"elapsed":859,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"4f989ea8-3ff1-4ee5-e651-03ad4ee40846"},"source":["A, W, b = linear_forward_test_case()\n","\n","Z, linear_cache = linear_forward(A, W, b)\n","print(\"Z = \" + str(Z))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Z = [[0.85391458 1.92375077]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"81g5cit5Z-xs","colab_type":"text"},"source":["### Linear Activation\n","* **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. We have provided you with the `sigmoid` function. This function returns **two** items: the activation value \"`a`\" and a \"`cache`\" that contains \"`Z`\" (it's what we will feed in to the corresponding backward function). To use it you could just call: \n","``` python\n","A, activation_cache = sigmoid(Z)\n","```\n","\n","* **ReLU**: The mathematical formula for ReLu is $A = ReLU(Z) = max(0, Z)$. We have provided you with the `relu` function. This function returns **two** items: the activation value \"`A`\" and a \"`cache`\" that contains \"`Z`\" (it's what we will feed in to the corresponding backward function). To use it you could just call:\n","```\n","A, activation_cache = relu(Z)\n","```\n","\n","\n","---\n","\n","\n","\n","```\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"2pATCFD_Zplw","colab_type":"code","colab":{}},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \n","    if activation == \"sigmoid\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","    \n","    elif activation == \"relu\":\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mpd56-7Xbb99","colab_type":"code","colab":{}},"source":["def linear_activation_forward_test_case():\n","  \n","    A_prev = np.random.randn(3,2)\n","    W = np.random.randn(1,3)\n","    b = np.random.randn(1,1)\n","    \n","    return A_prev, W, b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_wU8zmyNbY4G","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1563915263540,"user_tz":-330,"elapsed":921,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"643c2534-c597-4511-ed33-6dc512420917"},"source":["A_prev, W, b = linear_activation_forward_test_case()\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","\n","A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n","print(\"With ReLU: A = \" + str(A))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["With sigmoid: A = [[0.71669845 0.99303271]]\n","With ReLU: A = [[0.92814329 4.95953689]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zRwQ52zThuNV","colab_type":"text"},"source":["## L-Layer Model\n","\n","For even more convenience when implementing the $L$-layer Neural Net, we will need a function that replicates the previous one (`linear_activation_forward` with ReLU) $(L-1)$ times, then follows that with one `linear_activation_forward` with SIGMOID.\n","\n","![alt text](https://datascience-enthusiast.com/figures/model_architecture_kiank.png)\n","\n","---\n","\n","\n","\n","```\n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    \n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"ihtDfUOMbgf2","colab_type":"code","colab":{}},"source":["def L_model_forward(X, parameters):\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2 # number of layers in the neural network\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev, \n","                                             parameters['W' + str(l)], \n","                                             parameters['b' + str(l)], \n","                                             activation='relu')\n","        caches.append(cache)\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n","    AL, cache = linear_activation_forward(A, \n","                                          parameters['W' + str(L)], \n","                                          parameters['b' + str(L)], \n","                                          activation='sigmoid')\n","    caches.append(cache)\n","    \n","    assert(AL.shape == (1,X.shape[1]))\n","            \n","    return AL, caches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GGs2V8cRuJqY","colab_type":"code","colab":{}},"source":["def L_model_forward_test_case_2hidden():\n","\n","    X = np.random.randn(5,4)\n","    W1 = np.random.randn(4,5)\n","    b1 = np.random.randn(4,1)\n","    W2 = np.random.randn(3,4)\n","    b2 = np.random.randn(3,1)\n","    W3 = np.random.randn(1,3)\n","    b3 = np.random.randn(1,1)\n","  \n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2,\n","                  \"W3\": W3,\n","                  \"b3\": b3}\n","    \n","    return X, parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4C058fFp_9A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1563915266483,"user_tz":-330,"elapsed":755,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"2d65bff0-7a7f-43af-b825-58abf07ff83a"},"source":["X, parameters = L_model_forward_test_case_2hidden()\n","AL, caches = L_model_forward(X, parameters)\n","print(\"AL = \" + str(AL))\n","print(\"Length of caches list = \" + str(len(caches)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["AL = [[0.44307552 0.33404182 0.26785456 0.15502747]]\n","Length of caches list = 3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hKO6X91qyU0S","colab_type":"text"},"source":["## Cost Function\n","\n","\n","```\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"msxjcDUZuQ9z","colab_type":"code","colab":{}},"source":["def compute_cost(AL, Y):\n","    \n","    m = Y.shape[1]\n","\n","    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IEXU1zarzPnx","colab_type":"code","colab":{}},"source":["def compute_cost_test_case():\n","    Y = np.asarray([[1, 1, 1]])\n","    aL = np.array([[.8,.9,0.4]])\n","    \n","    return Y, aL"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BYHhH4Apymmm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1563915270427,"user_tz":-330,"elapsed":1347,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"2112590b-1f49-480b-c9ec-82f8871f46af"},"source":["Y, AL = compute_cost_test_case()\n","\n","print(\"cost = \" + str(compute_cost(AL, Y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cost = 0.41493159961539694\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IvE5-Ivn0fhW","colab_type":"text"},"source":["## Backward propagation module\n","![alt text](https://datascience-enthusiast.com/figures/backprop_kiank.png)\n","\n","Now, similar to forward propagation, you are going to build the backward propagation in three steps:\n","- LINEAR backward\n","- LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n","- [LINEAR -> ReLU] $\\times$ $(L-1)$ -> LINEAR -> SIGMOID backward (whole model)"]},{"cell_type":"markdown","metadata":{"id":"xwEDDFoa08jh","colab_type":"text"},"source":["### Linear Backward:\n","For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n","\n","Suppose we have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$.\n","![alt text](https://datascience-enthusiast.com/figures/linearback_kiank.png)\n","\n","The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n","$$ dW^{[l]} = \\frac{\\partial{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n","$$ db^{[l]} = \\frac{\\partial{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n","$$ dA^{[l-1]} = \\frac{\\partial{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n","\n","\n","\n","---\n","\n","\n","\n","```\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"G2c1ILrPzZrH","colab_type":"code","colab":{}},"source":["def linear_backward(dZ, cache):\n","\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1]\n","\n","    dW = (1.0/m)*np.dot(dZ, A_prev.T)\n","    db = (1.0/m)*np.sum(dZ, axis=1, keepdims=True)\n","    dA_prev = np.dot(W.T, dZ)\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFz8_bUV2ARe","colab_type":"code","colab":{}},"source":["def linear_backward_test_case():\n","\n","    dZ = np.random.randn(1,2)\n","    A = np.random.randn(3,2)\n","    W = np.random.randn(1,3)\n","    b = np.random.randn(1,1)\n","    linear_cache = (A, W, b)\n","    return dZ, linear_cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4gxPHsW2IIP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1563915273652,"user_tz":-330,"elapsed":913,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"d16c2c3f-97bd-4f50-e877-31892ebe1f8e"},"source":["dZ, linear_cache = linear_backward_test_case()\n","\n","dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dA_prev = [[-0.84836047 -1.48514293]\n"," [ 0.1098082   0.19223063]\n"," [ 0.13961806  0.24441588]]\n","dW = [[-0.23692617  0.03835643 -0.21702733]]\n","db = [[0.66896671]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8YrjQBHD3BTu","colab_type":"text"},"source":["### Linear Activation Backward:\n","We'll create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n","\n","To implement `linear_activation_backward`, we provided two backward functions:\n","- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. We can call it as follows:\n","\n","```python\n","dZ = sigmoid_backward(dA, activation_cache)\n","```\n","\n","- **`relu_backward`**: Implements the backward propagation for ReLU unit. We can call it as follows:\n","\n","```python\n","dZ = relu_backward(dA, activation_cache)\n","```\n","\n","If $g(.)$ is the activation function, \n","`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n","\n","\n","\n","---\n","\n","\n","\n","```\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"afvDemUA2gZN","colab_type":"code","colab":{}},"source":["def linear_activation_backward(dA, cache, activation):\n","\n","    linear_cache, activation_cache = cache\n","    \n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","        \n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNl7oelW3zOv","colab_type":"code","colab":{}},"source":["def linear_activation_backward_test_case():\n","\n","    dA = np.random.randn(1,2)\n","    A = np.random.randn(3,2)\n","    W = np.random.randn(1,3)\n","    b = np.random.randn(1,1)\n","    Z = np.random.randn(1,2)\n","    linear_cache = (A, W, b)\n","    activation_cache = Z\n","    linear_activation_cache = (linear_cache, activation_cache)\n","    \n","    return dA, linear_activation_cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iY1ro6h-4N5E","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1563915277594,"user_tz":-330,"elapsed":2028,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"6c6f7c07-8b55-46a7-8c05-0b164f840ef8"},"source":["dAL, linear_activation_cache = linear_activation_backward_test_case()\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n","print (\"sigmoid:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db) + \"\\n\")\n","\n","dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n","print (\"relu:\")\n","print (\"dA_prev = \"+ str(dA_prev))\n","print (\"dW = \" + str(dW))\n","print (\"db = \" + str(db))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sigmoid:\n","dA_prev = [[-0.01753949  0.06356647]\n"," [-0.01676571  0.06076213]\n"," [ 0.03150611 -0.11418413]]\n","dW = [[-0.00235162 -0.01913512 -0.04665459]]\n","db = [[-0.04810867]]\n","\n","relu:\n","dA_prev = [[0. 0.]\n"," [0. 0.]\n"," [0. 0.]]\n","dW = [[0. 0. 0.]]\n","db = [[0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z-gfZvvZl6Nr","colab_type":"text"},"source":["\n","\n","```\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","               the cache of linear_activation_forward() with \"sigmoid\" (it's caches(L-1))\n","    \n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h8Uz2xPvmre4","colab_type":"text"},"source":["\n","\n","```\n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    \n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TDArJv_bne2S","colab_type":"text"},"source":["## Update Parameters\n","In this section we will update the parameters of the model, using gradient descent: \n","\n","$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n","$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n","\n","where $\\alpha$ is the learning rate. After computing the updated parameters, we'll store them in the parameters dictionary. "]},{"cell_type":"code","metadata":{"id":"fvKpL2EhmlYp","colab_type":"code","colab":{}},"source":["def update_parameters(parameters, grads, learning_rate):\n","    \n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","    for l in range(L):\n","        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n","        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)]\n","        \n","    return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFc1s-AOnDu3","colab_type":"code","colab":{}},"source":["def update_parameters_test_case():\n","\n","    W1 = np.random.randn(3,4)\n","    b1 = np.random.randn(3,1)\n","    W2 = np.random.randn(1,3)\n","    b2 = np.random.randn(1,1)\n","    parameters = {\"W1\": W1,\n","                  \"b1\": b1,\n","                  \"W2\": W2,\n","                  \"b2\": b2}\n","\n","    dW1 = np.random.randn(3,4)\n","    db1 = np.random.randn(3,1)\n","    dW2 = np.random.randn(1,3)\n","    db2 = np.random.randn(1,1)\n","    grads = {\"dW1\": dW1,\n","             \"db1\": db1,\n","             \"dW2\": dW2,\n","             \"db2\": db2}\n","    \n","    return parameters, grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30mqXiOwm0Kf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"executionInfo":{"status":"ok","timestamp":1563915279685,"user_tz":-330,"elapsed":719,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"d6f20d48-0969-44d5-d43e-3250b3a90387"},"source":["parameters, grads = update_parameters_test_case()\n","parameters = update_parameters(parameters, grads, 0.1)\n","\n","print (\"W1 = \"+ str(parameters[\"W1\"]))\n","print (\"b1 = \"+ str(parameters[\"b1\"]))\n","print (\"W2 = \"+ str(parameters[\"W2\"]))\n","print (\"b2 = \"+ str(parameters[\"b2\"]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["W1 = [[ 1.68006676e-01  1.20733268e-01  1.26774057e+00 -5.86221134e-01]\n"," [ 4.63854489e-01  4.33088653e-01 -3.47965123e-04  6.94981206e-01]\n"," [ 5.99177453e-01 -8.99395484e-01  9.85400720e-01 -1.15605872e+00]]\n","b1 = [[0.10270444]\n"," [0.05647119]\n"," [0.43757182]]\n","W2 = [[-0.56318736  0.08862028 -0.12654138]]\n","b2 = [[0.89319316]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kO_ri-A0nJ5d","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}