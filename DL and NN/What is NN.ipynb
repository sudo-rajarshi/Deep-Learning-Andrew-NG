{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"What is NN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5IGnLVlQaWDi","colab_type":"text"},"source":["# What is Neural Network ?\n","The term, Deep Learning, refers to training Neural Networks, sometimes very large Neural Networks.\n","\n","Let's start to the Housing Price Prediction example. Let's say we have a data sets with six houses, so we know the size of the houses in square feet or square meters and we know the price of the house and we want to fit a function to predict the price of the houses(the function of the size). So by linear regression, putting a straight line to these data so.\n","\n","![alt text](http://www.programmersought.com/images/974/a36cccd8f3be8a0a8dfb090caea5dc6e.png)\n","\n","**Fig-1**\n","\n","Well we know that prices can never be negative. So instead of the straight line fit which eventually will become negative, the line ends up to the zero (Fig-1). So the blue line ends up being yur function for predicting the price of the house as a function of this size.\n","\n","We have as the input to the neural network the size of a house which one we call `'x'`. It goes into the little circle and then it outputs the price which is `'y'`. So this little circle, which is a single neuron in a neural network, implements this function that we drew on the left (Fig-1).\n","\n","The neuron computes this linear function after takeing the input size then takes a max of zero, and then outputs the estimated price.\n","\n","This function which goes to zero sometimes and then it'll takes of as a straight line. This function is called a **ReLU** function which stands for **Rectified Linear Units**. \n","\n","This is the simpliest Neural Network example.\n","\n","To make it little be complex let's assume the price of the house is dependent on other factors also.\n","Like \n","* 'Family Size' depends on 'size of the house' and 'No. of bedrooms'.\n","* 'Walkability' depends on the address of the house i.e is the 'zip-code'.\n","* 'School quality' also depends on address i.e. the 'zip-code' and 'wealth'.\n","\n","So as a whole the the 'price of the house' now depends on all these factors.\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/10/Screenshot-from-2018-10-12-13-55-07.png)\n","\n","**Fig-2**\n","\n","In Fig-2, \n","* The layer created by 'Size', 'No. of bedrooms', 'Zip Code', 'Wealth' is konwn as the **input layer** of this neural network.\n","* The layer created by 'Family size', 'Walkability', 'School Quality' is known as the **hidden layer** of the neural network.\n","* 'Price' is the output of the neural network.\n","\n","So what we actually implement is the following:\n","\n","![alt text](https://i.imgur.com/iq2YcW7.png)\n","\n","**Fig-3**\n","\n","So for example, rather than saying the first nodes represent 'family size' and it depends only on the features X1 and X2. Instead, we're going to say, well neural network, you decide whatever you want this known to be. And we'll give you all four of the features to complete whatever you want. \n","\n","So we say that layers that this is input layer and this layer in the middle of the neural network are density connected. Because every input feature is connected to every one of these circles in the middle. And the remarkable thing about neural networks is that, given enough data about x and y, given enough training examples with both x and y, neural networks are remarkably good at figuring out functions that accurately map from x to y.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0jk9jzxqlzZn","colab_type":"text"},"source":["# Supervised Learning with Neural Networks\n","Supervised learning refers to a task where we need to find a function that can map input to corresponding outputs (given a set of input-output pairs). We have a defined output for each given input and we train the model on these examples. Below is a pretty handy table that looks at the different applications of supervised learning and the different types of neural networks that can be used to solve those problems\n","\n","Input(x) | Output(y) | Application | Type of NN\n","--- | --- | --- | ---\n","Home Features | Price | Real Estate | Standard NN\n","Image | Image Class | Photo tagging | Covolutional NN\n","Audio | Text Transcript | Speech Recognition | Recurrent NN\n","Image, Radar info. | Position of Car | Autonomous Driving |Custom/Hybrid NN\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/10/Screenshot-from-2018-10-12-14-10-51.png)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PFTma7EFpl10","colab_type":"text"},"source":["# Why is Deep Learning Taking off ?\n","There are multiple reasons for which deep learning is excelling over traditinal machine learning algorithms.\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/10/Screenshot-from-2018-10-12-14-29-37.png)\n","\n","**Fig - 4**\n","\n","From the Fig-4 we can see,\n","* For traditional learning algorithms as the amount of data gets increased, the perfromance gets saturated after a certain point. So the peak performance from the model of machine learning cannot be achieved.\n","\n","* But for Neual Networks as we keep going increasing the amount of data, the perfomance gets better as well. So to achieve the peak performance from the machine learning model using deep learning we need to feed lots of data to the neural network. \n","\n","Here's where deep learning shines. It can perform much better than traditional machine learning algorithms when it comes to nurture with lots of data.\n","\n","Another bigger advantages of using Deep Learning is it replaces 'Sigmoid Function' with 'Relu Function'.\n","\n","![alt text](http://deepdish.io/public/images/activation-functions.svg)\n","\n","**Fig-5**\n","\n","In Fig-5 we can see,\n","* In case of Sigmoid function the slope at the Saturated region is nearly 0, so the gradiant is 0, so the SGD optimizer performs much slower and the entire training process takes longer time.\n","* In case of ReLU function the slope > 0 for all its input to the right side, so the gradiant is 1, so the SGD optimizer performs much faster and the entire training process takes less time."]}]}