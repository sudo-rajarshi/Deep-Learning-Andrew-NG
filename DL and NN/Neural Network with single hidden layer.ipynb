{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Network with single hidden layer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"MXkcDm-9MGal"},"source":["# Neural Network Representation\n","\n","---\n","\n","![alt text](https://notesonml.files.wordpress.com/2015/05/neural-network.png)\n","\n","Let, i/p layer is denoted by '$X$' and o/p layer is denoted by '$\\hat{y}$'.\n","\n","* There are 4 i/p features. Let's name them '$X_{1}$',' $X_{2}$', '$X_{3}$' & '$X_{4}$' respectively.\n","* These 4 i/p features combine to create a hidden layer. Let's name the nodes of hidden layer as $a_{1}$, $a_{2}$, $a_{3}$, $a_{4}$, $a_{5}$.\n","\n","So for each of these layers there must be an activation function. Let's name them-\n","* For i/p layer the activation function is $\\mathrm{a}^{[0]}$.\n","* For hidden layer the activation function is $\\mathrm{a}^{[1]}$.\n","* For o/p layer the activation function is $\\mathrm{a}^{[2]}$.\n","\n","This is a 2 layer Neural Network because in case of Neural Network we donot count the i/p Layer.\n","\n","Parameters are associated with the Hidden layer and the o/p layer, such as-\n","* For hidden layer $\\mathrm{w}^{[1]}$ and $\\mathrm{b}^{[1]}$ is associated, where the shape of $\\mathrm{w}^{[1]}$ is $[5*4]$ & $\\mathrm{b}^{[1]}$ is $[5*1]$. Where - \n"," * For $\\mathrm{w}^{[1]}$, 5 = nodes of hidden layer and 4 = features of i/p layer.\n","* For o/p layer $\\mathrm{w}^{[2]}$ and $\\mathrm{b}^{[2]}$ is associated, where the shape of $\\mathrm{w}^{[2]}$ is $[1*5]$ & $\\mathrm{b}^{[2]}$ is $[1*1]$. Where -\n"," * For $\\mathrm{w}^{[2]}$, 1 = Classes of o/p and 5 = nodes of hidden layer.\n"," \n","#### Notations :\n"," \n","* $\\mathrm{a}^{[l]}_{i}$ = activation function for nodes for different layers, where \n"," * $l$ = index of layers in the neural network. \n"," * $i$ = index of nodes of a particular layer of activation function."]},{"cell_type":"markdown","metadata":{"id":"U3kYfYkAVRLp"},"source":["# Computing output\n","\n","---\n","\n","\n","Here we'll see how the activation function for each node ($a_{i}$) calculates the o/p of the neural network.\n","\n","Each activation function computes the o/p in 2 steps:\n","* **Step-1:** It calculates the $z = w*x + b$\n","* **Step-2:** It calculates $\\sigma(z)$.\n","\n","Now if we compute the o/p from each node of hidden layer, we get-\n","* from 1st node the activation function of the hidden layer, we get-\n","\n"," $\\mathrm{a}^{[1]}_{1} = \\sigma(\\mathrm{z}^{[1]}_{1})$ where $\\mathrm{z}^{[1]}_{1} = \\mathrm{w}^{[1]}_{1}*x + \\mathrm{b}^{[1]}_{1}$\n","* from 2nd node the activation function of hidden layer, we get-\n","\n"," $\\mathrm{a}^{[1]}_{2} = \\sigma(\\mathrm{z}^{[1]}_{2})$ where $\\mathrm{z}^{[1]}_{2} = \\mathrm{w}^{[1]}_{2}*x + \\mathrm{b}^{[1]}_{2}$\n","* from 3rd node the activation function of hidden layer, we get-\n","\n"," $\\mathrm{a}^{[1]}_{3} = \\sigma(\\mathrm{z}^{[1]}_{3})$ where $\\mathrm{z}^{[1]}_{3} = \\mathrm{w}^{[1]}_{3}*x + \\mathrm{b}^{[1]}_{3}$\n","* from 4th node the activation function of hidden layer, we get-\n","\n"," $\\mathrm{a}^{[1]}_{4} = \\sigma(\\mathrm{z}^{[1]}_{4})$ where $\\mathrm{z}^{[1]}_{4} = \\mathrm{w}^{[1]}_{4}*x + \\mathrm{b}^{[1]}_{4}$\n","* from 5th node the activation function of hidden layer, we get-\n","\n"," $\\mathrm{a}^{[1]}_{5} = \\sigma(\\mathrm{z}^{[1]}_{5})$ where $\\mathrm{z}^{[1]}_{5} = \\mathrm{w}^{[1]}_{5}*x + \\mathrm{b}^{[1]}_{5}$\n"," \n","Now if we compute the o/p from the o/p layer, we get-\n","* $\\mathrm{a}^{[2]} = \\sigma(\\mathrm{z}^{[2]})$ where $\\mathrm{z}^{[2]} = \\mathrm{w}^{[2]}*\\mathrm{a}^{[1]} + \\mathrm{b}^{[2]}$\n","\n","\n","**The o/p from activation function of each layer is-**\n","$$\\mathrm{a}^{[l]} = \\sigma(\\mathrm{z}^{[l]})$$ where $$\\mathrm{z}^{[l]} = \\mathrm{w}^{[l]}*\\mathrm{a}^{[l-1]} + \\mathrm{b}^{[l]}$$\n","\n","\n","\n"," \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x7CwR39ya4BH"},"source":["# Vectorizing across multiple training examples\n","\n","---\n","\n","\n","In order to implement these equation for 'm' training examples, we need to run a for loop for 1 to m.\n","\n","Now \n","* for $X_{1}$ input, the activation will be $\\mathrm{a}^{[2]}(1)$ and the o/p will be $\\hat{y}(1)$.\n","* for $X_{2}$ input, the activation will be $\\mathrm{a}^{[2]}(2)$ and the o/p will be $\\hat{y}(2)$.\n","* similarly for for $X_{m}$ input, the activation will be $\\mathrm{a}^{[2]}(m)$ and the o/p will be $\\hat{y}(m)$.\n","\n","So the for loop will look like,\n","\n","\n","```\n","for i = 1 to m:\n","      z[1](i) = w[1]*x(i) + b[1]\n","      a[1](i) = sigmoid(z[1](i))\n","      z[2](i) = w[2]*a[1](i) + b[1]\n","      a[2](i) = sigmoid(z[2](i))\n","```\n","\n","Now for vectorization,\n","\n","* $\\mathrm{z}^{[1]} = \\mathrm{w}^{[1]}*X+ \\mathrm{b}^{[1]}$ or as $X = \\mathrm{A}^{[0]}$ we can say, $\\mathrm{z}^{[1]} = \\mathrm{w}^{[1]}*\\mathrm{A}^{[0]}+ \\mathrm{b}^{[1]}$\n","* $\\mathrm{A}^{[1]} = \\sigma({\\mathrm{z}^{[1]}})$\n","* $\\mathrm{z}^{[2]} = \\mathrm{w}^{[1]}*\\mathrm{A}^{[1]}+ \\mathrm{b}^{[1]}$\n","* $\\mathrm{A}^{[2]} = \\sigma({\\mathrm{z}^{[2]}})$\n","\n","where , \n","$$X = \n"," \\begin{pmatrix}\n","  \\vdots  & \\vdots  &  \\vdots & \\vdots \\\\\n","  \\mathrm{x}^{[1]} & \\mathrm{x}^{[1]} & \\cdots & \\mathrm{x}^{[m]} \\\\ \n","  \\vdots  & \\vdots  &  \\vdots & \\vdots \\\\\n"," \\end{pmatrix}$$\n","\n","\n","$$\\mathrm{Z}^{[1]} = \n"," \\begin{pmatrix}\n","  \\vdots  & \\vdots  &  \\vdots & \\vdots \\\\\n","  \\mathrm{z}^{[1]}(1) & \\mathrm{z}^{[1]}(2) & \\cdots & \\mathrm{z}^{[1]}(m) \\\\ \n","  \\vdots  & \\vdots  &  \\vdots & \\vdots \\\\\n"," \\end{pmatrix}$$\n"," \n"," $$\\mathrm{A}^{[1]} = \n"," \\begin{pmatrix}\n","  \\vdots  & \\vdots  &  \\vdots & \\vdots \\\\\n","  \\mathrm{a}^{[1]}(1) & \\mathrm{a}^{[1]}(2) & \\cdots & \\mathrm{a}^{[1]}(m) \\\\ \n","  \\vdots  & \\vdots  &  \\vdots & \\vdots \\\\\n"," \\end{pmatrix}$$\n"," \n","* Horizontally the matrix A goes over different training examples and vertically the different indices in the matrix A corresponds to different hidden units.\n","* And a similar intuition holds true for the matrix Z as well as for X where horizontally corresponds to different training examples and vertically it corresponds to different input features."]},{"cell_type":"markdown","metadata":{"id":"BvzXhC9zszgL"},"source":["# Activation Functions\n","\n","---\n","\n","\n","There are many activation functions used to compute o/p from the layers of Neural Network. Activation functions can be divided in to 2 classes. Such as- \n","* Linear Activation Functions\n","* Non-linear Activation Functions\n","\n","Linear Activation Function is used when training a model using Linear Regression.\n","\n","In case of Deep Learning a neural network model consists many hidden layer. Using Linear Activation Function we cannot compute the output from different hidden layers.So Non-linear Activaton Function is needed in case of Deep Learning."]},{"cell_type":"markdown","metadata":{"id":"UyhTM0qRuCug"},"source":["## Sigmoid Function\n","\n","* **Function :** $\\sigma(z) = \\frac{1}{1 + \\mathrm{e}^{-z}}$\n","\n","* **Derivative :** $\\sigma'(z) = \\sigma(z)*(1-\\sigma(z))$\n","\n"," ![alt text](https://miro.medium.com/max/500/1*Myto4ZQagAOoyom4tqkaRQ.png)\n","  \n","Sigmoid takes a real value as input and outputs another value between 0 and 1. It’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.\n","\n","* **Pros:** \n"," 1. It is nonlinear in nature. Combinations of this function are also nonlinear.\n"," 2. The output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range.\n","* **Cons:**\n"," 1. Towards either end of the sigmoid function, the Y values tend to respond very less to changes in X.\n"," 2.It gives rise to a problem of “vanishing gradients”.\n"," \n","   **Vanishing Gradiants:** Gradient is small or has vanished (cannot make significant change because of the extremely small value). The network refuses to learn further or is drastically slow.\n"," 3. Its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n","\n"]},{"cell_type":"code","metadata":{"id":"Av_h4dZu2puc"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IeRXhTSkrW2E"},"source":["def sigmoid(z):\n","  return 1/(1+np.exp(-z))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdek4-mnuLqA","executionInfo":{"status":"ok","timestamp":1563650861589,"user_tz":-330,"elapsed":663,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh4.googleusercontent.com/-tTSVxsnwuoM/AAAAAAAAAAI/AAAAAAAAAdo/ulSsfX1qqsc/s64/photo.jpg","userId":"00168502276290574931"}},"outputId":"b12cdf16-11b4-4557-8a80-5497466f9759","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["sigmoid(np.array([100, 50, 10, -10, -50, -100]))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.00000000e+00, 1.00000000e+00, 9.99954602e-01, 4.53978687e-05,\n","       1.92874985e-22, 3.72007598e-44])"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"QLTJw3LIuz5T"},"source":["## Tanh or Hyperbolic Tangent Activation Function\n","tanh is also like sigmoid but better. The range of the tanh function is from (-1 to 1). \n","\n","* **Function:** $\\tanh(z) = \\frac{e^z - \\mathrm{e}^{-z}}{e^z + \\mathrm{e}^{-z}}$\n","* **Derivative:** $\\tanh'(z) = 1 - \\tanh(z)^2$\n","\n","![alt text](https://miro.medium.com/max/500/1*51Q7QouspCkOvENni2RwfQ.png)\n","\n","* **Pros:**\n"," 1. The gradient is stronger for tanh than sigmoid ( derivatives are steeper).\n"," \n","* **Cons:**\n"," 1. Tanh also has the vanishing gradient problem. \n","\n"]},{"cell_type":"code","metadata":{"id":"BaejgxekuWQ3"},"source":["def tanh(z):\n","  t = np.divide((np.exp(z) - np.exp(-z)),(np.exp(z) - np.exp(-z)))\n","  return t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NV8WNYIB4xrO","executionInfo":{"status":"ok","timestamp":1563651558927,"user_tz":-330,"elapsed":1418,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"71a18766-f079-40f9-e4d8-3e6bbf7b0d7b","colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["tanh(np.array([100, 10, 1, 0, -1, -10, -100]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n","  \n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["array([ 1.,  1.,  1., nan,  1.,  1.,  1.])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"NuMnyByj5fDA"},"source":["## ReLU\n","A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: $max(0,z)$. Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid but with better performance.\n","\n","* **Function:** $ R(z) =\n","  \\begin{cases}\n","    z       & \\quad \\text{if } z >0\\\\\n","    0  & \\quad \\text{if } z \\leq 0\n","  \\end{cases}\n","$\n","\n","* **Derivative:** $ R'(z) =\n","  \\begin{cases}\n","    1       & \\quad \\text{if } z >0\\\\\n","    0  & \\quad \\text{if } z < 0\n","  \\end{cases}\n","$\n","\n","![alt text](https://miro.medium.com/max/1000/1*m_0v2nY5upLmCU-0SuGZXg.png)\n","\n","* **Pros:**\n"," 1. It avoids and rectifies vanishing gradient problem.\n"," 2. ReLU is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations.\n"," \n","* **Cons:**\n"," 1. One of its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n"," 2. Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again.\n"," 3. Simply saying that ReLU could result in Dead Neurons.\n"," 4. In another words, For activations in the region (x<0) of ReLU, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input ( simply because gradient is 0, nothing changes ). This is called dying ReLU problem.\n"," 5. The range of ReLU is [0, inf). This means it can blow up the activation.\n"]},{"cell_type":"code","metadata":{"id":"urPZfMOxCQap"},"source":["def relu(x, y):\n","  x <= 0\n","  r = np.maximum(x, y)\n","  return(r)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRecvBKTCcU-","executionInfo":{"status":"ok","timestamp":1563654668809,"user_tz":-330,"elapsed":1260,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"a4fe3aae-4393-470f-dbd1-1787e432cb75","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["relu(10,-10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"rlCqWYco9_kD"},"source":["## LeakyReLU\n","LeakyReLU is a variant of ReLU. Instead of being 0 when $z<0, a$ leaky ReLU allows a small, non-zero, constant gradient $\\alpha$ (Normally, $\\alpha$=0.01).\n","\n","* **Function:** $ R(z) =\n","  \\begin{cases}\n","    z       & \\quad \\text{if } z >0\\\\\n","    \\alpha z  & \\quad \\text{if } z \\leq 0\n","  \\end{cases}\n","$\n","* **Derivative:** $ R'(z) =\n","  \\begin{cases}\n","    1       & \\quad \\text{if } z >0\\\\\n","    \\alpha  & \\quad \\text{if } z \\leq 0\n","  \\end{cases}\n","$\n","\n","![alt text](https://miro.medium.com/max/500/1*gDIUV3yonKbIWh_9Kl4ShQ.png)\n","\n","* **Pros:**\n"," 1. Leaky ReLUs are one attempt to fix the “dying ReLU” problem by having a small negative slope (of 0.01, or so).\n","* **Cons:**\n"," 1. As it possess linearity, it can’t be used for the complex Classification. It lags behind the Sigmoid and Tanh for some of the use cases."]},{"cell_type":"code","metadata":{"id":"lEgbWK1vDm3D"},"source":["az = 0.01*z\n","def leekyrelu(az, z):\n","  lr = np.maximum(az, z)\n","  return lr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOgsy1TY4Hv0","executionInfo":{"status":"ok","timestamp":1563654674922,"user_tz":-330,"elapsed":817,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"688329a7-5fc4-47d8-e9dd-03cc24b6940c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["leekyrelu(10, -10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"aQytVW-DK_tS"},"source":["# Initializing Parameters\n","To train Neural Network we cannot initalize the parameters('w' & 'b') by 0. Because if we do so, the activation function for different nodes ($a_{i}$) will be symmetric in nature. So the o/p of 1st hidden layer say $\\mathrm{a}^{[1]}$ = o/p of 2nd hidden layer say $\\mathrm{a}^{[2]}$. So we will not be able to get different o/ps' from different hidden layer by initializing 'w' & 'b' by 0.\n","\n","So we randomly take 'w'. 'b' doesn't have any problem if we initialize it by 0."]},{"cell_type":"code","metadata":{"id":"HSj-ASWND5dP","executionInfo":{"status":"ok","timestamp":1563659847039,"user_tz":-330,"elapsed":1183,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"6bac573a-d139-4ab6-945d-9d3ed65f97a5","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["w = np.random.randn(2,2)*0.01\n","b = np.zeros((2,1))\n","print(w)\n","print(b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[-0.01275736  0.01106708]\n"," [-0.01041753 -0.00275047]]\n","[[0.]\n"," [0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tnhXMcI_PJ_n"},"source":["Now to initialize 'w' we've multiplied the random number with 0.01. Because it is always preferrable to initiliaize parameter with a number as small as possibl. While using 'Sigmoid' or 'tanh' as an activation function, if we use bigger initial value for 'w' then 'w' will be positioned at such a place where the gradiant of those activation function will be close to 0. So the gradiant descent will be much slower.\n","\n","For this reason we usually initialize our parameters as small as possible."]}]}