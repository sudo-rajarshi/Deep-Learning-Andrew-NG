{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Improving DNN","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7FFku2ucruwq","colab_type":"text"},"source":["# Setting up ML application\n","\n","\n","---\n","\n","# Training and Test Sets: Splitting Data\n","\n","![alt text](https://developers.google.com/machine-learning/crash-course/images/PartitionTwoSets.svg)\n","\n",">**[Fig:. Slicing a single data set into a training set and test set.]**\n","\n","Test set should meet the following two conditions:\n","\n","* It should be large enough to yield statistically meaningful results.\n","* It should be representative of the data set as a whole. In other words, we shouldn't pick a test set with different characteristics than the training set.\n","\n","Assuming that test set meets the preceding two conditions, the goal is to create a model that generalizes well to new data. Our test set serves as a proxy for new data. \n","\n","For example, in the following figure the model learned for the training data is very simple. This model doesn't do a perfect job—a few predictions are wrong. However, this model does about as well on the test data as it does on the training data. In other words, this simple model does not overfit the training data.\n","\n","![alt text](https://developers.google.com/machine-learning/crash-course/images/TrainingDataVsTestData.svg)\n","\n","**We should never train on test data**. If we have surprisingly good results on our evaluation metrics, it might be a sign that we are accidentally training on the test set. For example, high accuracy might indicate that test data has leaked into the training set.\n","\n","![alt text](https://developers.google.com/machine-learning/crash-course/images/WorkflowWithTestSet.svg)\n","\n",">**[Fig: A workflow with test set]**\n","\n","In the figure 4, \"Tweak model\" means adjusting anything about the model we can dream up—from changing the learning rate, to adding or removing features, to designing a completely new model from scratch. At the end of this workflow, we pick the model that does best on the test set.\n","\n","# Validation Set\n","\n","![alt text](https://developers.google.com/machine-learning/crash-course/images/PartitionThreeSets.svg)\n","\n","We can greatly reduce our chances of overfitting by partitioning the data set into the three subsets.\n","\n","We use the validation set to evaluate results from the training set. Then, we use the test set to double-check our evaluation after the model has \"passed\" the validation set. The following figure shows this new workflow:\n","\n","![alt text](https://developers.google.com/machine-learning/crash-course/images/WorkflowWithValidationSet.svg)\n","\n",">**[Fig: A workflow with test & validation set]**\n","\n","In this improved workflow:\n","\n","* We should pick the model that does best on the validation set.\n","* We should double-check that model against the test set.\n","\n","This is a better workflow because it creates fewer exposures to the test set.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1XkibfCsuWU9","colab_type":"text"},"source":["# Bias and Variance\n","## Bias\n","Bias is how far are the predicted values from the actual values. If the average predicted values are far off from the actual values then the bias is high.\n","High bias causes algorithm to miss relevant relationship between input and output variable. \n","\n","When a model has a high bias then it implies that the model is too simple and does not capture the complexity of data thus underfitting the data.\n","\n","## Variance\n","Variance occurs when the model performs good on the trained dataset but does not do well on a dataset that it is not trained on, like a test dataset or validation dataset. Variance tells us how scattered are the predicted value from the actual value.\n","\n","High variance causes overfitting that implies that the algorithm models random noise present in the training data.\n","\n","When a model has a high variance then the model becomes very flexible and tune itself to the data points of the training set. when a high variance model encounters a different data point that it has not learnt then it cannot make right prediction.\n","\n","![alt text](https://miro.medium.com/max/700/1*MNVNs4mbxoJTjwC2ghUVOQ.png)\n","\n","If we look at the diagram above, we see that a model with high bias looks very simple. A model with high variance tries to fit most of the data points making the model complex and difficult to model. This can be visible from the plot below between test and training prediction error as a function of model complexity.\n","\n","![alt text](https://miro.medium.com/max/700/1*gy5zEJ2_QfhKMNc3BAjLSA.png)\n","\n","We would like to have a model complexity that trades bias off with variance so that we minimize the test error and would make our model perform better. This is illustrated the the bias variance trade off below.\n","\n","![alt text](https://miro.medium.com/max/700/1*wTbewHG05BXOIwFjtilU8A.png)\n","\n","* **High Bias Low Variance:** Models are consistent but inaccurate on average\n","* **High Bias High Variance :** Models are inaccurate and also inconsistent on average\n","* **Low Bias Low Variance:** Models are accurate and consistent on averages. We strive for this in our model\n","* **Low Bias High variance:** Models are somewhat accurate but inconsistent on averages. A small change in the data can cause a large error.\n","\n","**High Bias can be identified when we have,**\n","1. High training error\n","2. Validation error or test error is same as training error\n","\n","**High Variance can be identified when we have,**\n","1. Low training error\n","2. High validation error ot high test error\n","\n","## Methods of Reducing Bias and Variance:\n","High bias is due to a simple model and we also see a high training error. To fix that we can do following things,\n","1. Add more input features\n","2. Add more complexity by introducing polynomial features\n","3. Decrease Regularization term\n","\n","High variance is due to a model that tries to fit most of the training dataset points and hence gets more complex. To resolve high variance issue we need to work on\n","1. Getting more training data\n","2. Reduce input features\n","3. Increase Regularization term"]},{"cell_type":"markdown","metadata":{"id":"cWWghilGx1e1","colab_type":"text"},"source":["# Regularization in Neural Network\n","Regularization is a technique to discourage the complexity of the model. It does this by penalizing the loss function. This helps to solve the overfitting problem.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B50iQKrV86IA","colab_type":"text"},"source":["### L2 Regularization:\n","\n","In deep learning, we wish to minimize the following cost function:\n","\n","$J(w^{[1]},b^{[1]},....w^{[L]},b^{[l]}) = \\frac{1}{m}\\sum_{i=1}^{m} L({\\hat{y}}^{(i)}, {y}^{(i)})$\n","\n","where,'L' can be any loss function. \n","\n","In L2 regularization, we add a component that will penalize large weights.\n","\n","L2 regularization forces the weights to be small but does not make them zero and does non sparse solution.\n","\n","**The equation of L2 regularization is:**\n","\n","**$J(w^{[1]},b^{[1]},....w^{[L]},b^{[l]}) = \\frac{1}{m}\\sum_{i=1}^{m} L({\\hat{y}}^{(i)}, {y}^{(i)} + \\frac{\\lambda}{2*m}\\sum_{i=1}^{L}||w^{[l]}||_{F}^2$**\n","\n","where $\\lambda$ = **Regularization Parameter**. \n","\n","Larger weight values will be more penalized if the value of $\\lambda$ is large. Similarly, for a smaller value of $\\lambda$, the regularization effect is smaller.\n","\n","Notice the addition of the **Frobenius norm**, denoted by the subscript **'F'**. This is in fact equivalent to the squared norm of a matrix.\n","\n","By adding the squared norm of the weight matrix and multiplying it by the regularization parameters, large weights will be driven down in order to minimize the cost function.\n","\n","### Why Regularization works?\n","As aforementioned, adding the regularization component will drive the values of the weight matrix down. This will effectively decorrelate the neural network.\n","\n","The activation function with the following weighted sum:\n","$$z = w^T*x+b$$\n","\n","By reducing the values in the weight matrix, z will also be reduced, which in turns decreases the effect of the activation function. Therefore, a less complex function will be fit to the data, effectively reducing overfitting."]},{"cell_type":"markdown","metadata":{"id":"ZRM74M8K5ChY","colab_type":"text"},"source":["### Dropout Regularization\n","Dropout involves going over all the layers in a neural network and setting probability of keeping a certain nodes or not.\n","\n","Of course, the input layer and the output layer are kept the same.\n","\n","The probability of keeping each node is set at random. You only decide of the threshold: a value that will determine if the node is kept or not.\n","\n","For example, if you set the threshold to 0.7, then there is a probability of 30% that a node will be removed from the network.\n","\n","Therefore, this will result in a much smaller and simpler neural network, as shown below.\n","![alt text](https://miro.medium.com/max/700/1*iWQzxhVlvadk6VAJjsgXgg.png)\n","\n","### Why Dropout Regularization works?\n","\n","Dropout means that the neural network cannot rely on any input node, since each have a random probability of being removed. Therefore, the neural network will be reluctant to give high weights to certain features, because they might disappear.\n","\n","Consequently, the weights are spread across all features, making them smaller. This effectively shrinks the model and regularizes it."]},{"cell_type":"markdown","metadata":{"id":"HipEHOy17y-X","colab_type":"text"},"source":["## Data Augmentation\n","Naturally, if we have a lot of parameters, we would need to show our machine learning model a proportional amount of examples, to get good performance. Also, the number of parameters we need is proportional to the complexity of the task our model has to perform.\n","\n","**How do I get more data, if I don’t have “more data”?**\n","\n","We don’t need to hunt for novel new images that can be added to our dataset. Why? Because, neural networks aren’t smart to begin with. For instance, a poorly trained neural network would think that these three tennis balls shown below, are distinct, unique images.\n","![alt text](https://miro.medium.com/max/700/1*L07HTRw7zuHGT4oYEMlDig.jpeg)\n","\n","So, to get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway.\n","\n","![alt text](https://miro.medium.com/max/700/1*dJNlEc7yf93K4pjRJL55PA.png)\n","\n","**A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above).**\n","\n","Some popular augmentation techniques are-\n","1. Flip\n","2. Rotation\n","3. Scale\n","4. Crop\n","5. Translation\n","6. Noise"]},{"cell_type":"markdown","metadata":{"id":"HvyvhBmmAHMt","colab_type":"text"},"source":["## Early Stopping\n","When training neural networks, numerous decisions need to be made regarding the hyperparameters used, in order to obtain good performance. Once such hyperparameter is the number of training epochs: that is, how many full passes of the data set (epochs) should be used? If we use too few epochs, we might underfit (i.e., not learn everything we can from the training data); if we use too many epochs, we might overfit (i.e., fit the ‘noise’ in the training data, and not the signal).\n","\n","Early stopping attempts to remove the need to manually set this value. It can also be considered a type of regularization method (like L1/L2 weight decay and dropout) in that it can stop the network from overfitting.\n","\n","The idea behind early stopping is relatively simple:\n","\n","* Split data into training and test sets\n","* At the end of each epoch (or, every N epochs):\n","  * evaluate the network performance on the test set\n","  * if the network outperforms the previous best model: save a copy of the network at the current epoch\n","* Take as our final model the model that has the best test set performance\n","\n","![alt text](https://deeplearning4j.org/images/guide/earlystopping.png)"]},{"cell_type":"markdown","metadata":{"id":"ac6lK0D3BK8B","colab_type":"text"},"source":["# Seeting up our optimization problem"]},{"cell_type":"markdown","metadata":{"id":"TYxfsCj2FHtv","colab_type":"text"},"source":["## Normalizing inputs\n","Let's take a second to imagine a scenario in which we have a very simple neural network with two inputs. The first input value, x1, varies from 0 to 1 while the second input value, x2, varies from 0 to 0.01. Since our network is tasked with learning how to combine these inputs through a series of linear combinations and nonlinear activations, the parameters associated with each input will also exist on different scales.\n","\n","Unfortunately, this can lead toward an awkward loss function topology which places more emphasis on certain parameter gradients.\n","\n","![alt text](https://www.jeremyjordan.me/content/images/2018/01/Screen-Shot-2018-01-23-at-2.27.20-PM.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X5-hGrp_pgpG","colab_type":"text"},"source":["## Vaninshing Gradiant:\n","Usually, when we train a Deep model using through backprop using Gradient Descent, we calculate the gradient of the output w.r.t to weight matrices and then subtract it from respective weight matrices to make its(matrix’s) values more accurate to give correct output.\n","\n","### Reason of Gradiants becoming negligble:\n","![alt text](http://deepdish.io/public/images/activation-functions.svg)\n","\n","In case of Sigmoid function the slope at the Saturated region is nearly 0, so the gradiant is 0, so the SGD optimizer performs much slower and the entire training process takes longer time.\n","\n","It gives rise to a problem of “vanishing gradients”.\n","\n","**Vanishing Gradiants:** Gradient is small or has vanished (cannot make significant change because of the extremely small value). The network refuses to learn further or is drastically slow.\n","\n","#### The Mathematical Reason:\n","Consider a neural network with 4 hidden layers with a single neuron in each matrix.\n","![alt text](https://hackernoon.com/hn-images/1*Io6GzudqlSN8AIIA1uNw0A.png)\n","\n","The computation graph for the neural network above is:\n","\n","![alt text](https://hackernoon.com/hn-images/1*UATNEUQ0dKbvDdI79y3z9A.png)\n","\n","In forward propagation, we just multiply the input with weight matrices and add bias as shown above. We then find the sigmoid of the output.\n","\n","![alt text](https://hackernoon.com/hn-images/1*aVaJIAm2hKo_6YAWY8Y9wA.png)\n","\n","During backprop, we find the derivative of the output w.r.t. different weight matrices in order to make our output more accurate. Suppose that we want to find derivative of C(output) w.r.t weight matrix (b1).\n","\n","The terms which are going to be included in this are:\n","\n","![alt text](https://hackernoon.com/hn-images/1*Io6GzudqlSN8AIIA1uNw0A.png)\n","\n","The sigmoid($z_{1}$),sigmoid($z_{2}$).. etc are less than 1/4. Because derivative of sigmoid function is less than 1/4. See below. The weight matrices $w_{1},w_{2},w_{3},w_{4}$ are initialized using gaussian method to have a mean of 0 and standard deviation of 1. Hence $||w(i)||$ is less than 1. Therefore, in derivative we multiply such terms which are less than 1 and 1/4. Hence on multiplying such small terms for a huge number of times we get very small gradient which makes the model to almost stop learning.\n","\n","The reason that if we have deeper models than starting hidden layers will have low speed of learning is: we move deeper as we reach the starting hidden layers during backprop and hence more such terms are involved which makes the gradient small."]},{"cell_type":"markdown","metadata":{"id":"D0lyCt4lHykh","colab_type":"text"},"source":["## Weight Initialization Techniques in Neural Networks\n","Consider a L layer neural network, which has L-1 hidden layers and 1 input and output layer each. The parameters (weights and biases) for layer l are represented as-\n","\n","* $w^{[l]}$ = weight matrices of dimension(sizer of layer $l$, size of layer $l-1$)\n","* $b^{[l]}$ = bias vectors of dimension (size of layer $l$, 1)\n","\n","Following are some techniques generally practiced to initialize parameters :\n","1. Zero Initialization\n","2. Random Initialization\n"]},{"cell_type":"markdown","metadata":{"id":"gVkb5Kz5JMEd","colab_type":"text"},"source":["### Zero Initialization:\n","In general practice biases are initialized with 0 and weights are initialized with random numbers, what if weights are initialized with 0 ?\n","\n","In order to understand this lets consider we applied sigmoid activation function for the output layer .\n","\n","![alt text](https://miro.medium.com/max/500/1*Myto4ZQagAOoyom4tqkaRQ.png)\n","\n","If all the weights are initialized with 0 , the derivative w.r.t. loss function is same for every $w$ in $w^{[l]}$, thus all weights have same value in subsequent iterations. This makes hidden units symmetric and continues for all the n iterations i.e. setting weights to 0 does not make it better than a linear model. An important thing to keep in mind is that biases have no effect what so ever when initialized with 0.\n","\n","lets consider a neural network with only three hidden layer with ReLu activation function in hidden layers and sigmoid for the output layer.\n","\n","Using above neural network on dataset “make circles” from sklearn.datasets, result obtained was following :\n","\n","for 15000 iterations, loss = 0.6931471805599453, **accuracy = 50 %**\n","\n","![alt text](https://miro.medium.com/max/700/1*aaKYKl892E8v_dw24wDC1w.png)\n","\n","clearly, zero initialization isn’t successful in classification.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Vnr4rvciKTrj","colab_type":"text"},"source":["### Random Initialization:\n","Assigning random values to weights is better than just 0 assignment. But there is one thing to keep in my mind is that what happens if weights are initialized high values or very low values and what is a reasonable initialization of weight values.\n","\n","1. If weights are initialized with very high values the term $w^T*x+b$ becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where slope of gradient changes slowly and learning takes a lot of time.\n","2. If weights are initialized with low values it gets mapped to 0, where the case is same as above.\n","\n","**This problem is often referred to as vanishing gradient.**\n","\n","Neural network is same as earlier, using this initialization on dataset “make circles” from sklearn.datasets, result obatined was following :\n","\n","for 15000 iterations, loss = 0.38278397192120406, **accuracy = 86 %**\n","\n","![alt text](https://miro.medium.com/max/700/1*RzEieVSQ988z1vjJxioyEA.png)\n","\n","This solution is better but doesn’t properly fulfill the needs.\n"]},{"cell_type":"code","metadata":{"id":"JdNtlDLBOG_-","colab_type":"code","colab":{}},"source":["def random_init(l,x):\n","  \"\"\"\n","  Arguments:\n","    l = No. of layers\n","    x = multiplying factor used to increase the initial weight\n","  Returns:\n","    w = weight matrix after random initialization\n","  \"\"\"\n","  w = np.random.randn(l-1, l)*x # l-1 = hidden layer of the neural network\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bwxO_jbkOtz7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1564999421433,"user_tz":-330,"elapsed":662,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"5be60841-986d-45b1-ae2b-0d24a5fb0079"},"source":["random_init(4, 10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-12.59708579,   6.82837727, -16.74282239,  -7.65527381],\n","       [  3.33963891,  -8.87915087,  -2.4818102 ,   2.62870454],\n","       [-12.54196195,   0.08097929,   9.22612999,  -4.61861288]])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"fxunobViMOD4","colab_type":"text"},"source":["### He Initialization:\n","As we saw above that with large or 0 initialization of weights(w), not significant result is obtained even if we use appropriate initialization of weights it is probable that training process is going to take longer time. There are certain problems associated with it :\n","\n","1. If model is too large and takes many days to train then what ?\n","2. What about vanishing/exploding gradient problem ?\n","\n","These were some problems that stood in the path for many years but in 2015, He et al.(2015) proposed activation aware initialization of weights (for ReLu) that was able to resolve this problem. ReLu and leaky ReLu also solves the problem of vanishing gradient.\n","\n","We just simply multiply random initialization with $\\sqrt{\\frac{2}{size^{[l-1]}}}$\n","\n","To see how effective this solution is, lets use the previous dataset and neural network we took for above initialization and results are :\n","\n","for 15000 iterations, loss =0.07357895962677366, **accuracy = 96 %**\n","\n","![alt text](https://miro.medium.com/max/700/1*lCXe3BlmFR7JGui9JPctig.png)\n"]},{"cell_type":"code","metadata":{"id":"Cjg8c_iI-V3a","colab_type":"code","colab":{}},"source":["def he_init(l):\n","  \"\"\"\n","  Arguments:\n","    l = No. of layers\n","  Returns:\n","    w = weight matrix after random initialization\n","  \"\"\"\n","  w = np.random.randn(l-1, l)*np.sqrt(2/(l-1)) # l-1 = hidden layer of the neural network\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUQXekCsP5iA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1564999665695,"user_tz":-330,"elapsed":925,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"d57c3949-558a-468a-d15e-e40998385cf4"},"source":["he_init(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.0843713 , -1.33313512, -0.51114655, -0.87690153],\n","       [-0.88873882,  0.98347858, -1.8212223 , -0.82831689],\n","       [-0.41781933,  0.69958941, -0.07588963, -0.05871127]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"JxihnFVdQQHE","colab_type":"text"},"source":["### Xavier initialization:\n","It is same as He initialization but it is used for tanh() activation function, in this method 2 is replaced with 1, $\\sqrt{\\frac{1}{size^{[l-1]}}}$"]},{"cell_type":"code","metadata":{"id":"yOZAEYwIP8Qh","colab_type":"code","colab":{}},"source":["def xavier_init(l):\n","  \"\"\"\n","  Arguments:\n","    l = No. of layers\n","  Returns:\n","    w = weight matrix after random initialization\n","  \"\"\"\n","  w = np.random.randn(l-1, l)*np.sqrt(1/(l-1)) # l-1 = hidden layer of the neural network\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1NQDKnF-Q7ke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1564999933672,"user_tz":-330,"elapsed":1183,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"8776085d-6e3e-446b-9d99-77de8047ab96"},"source":["xavier_init(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.21127617, -0.46802677, -0.42384669, -0.44683414],\n","       [-0.1386518 ,  0.01733624, -0.55925612, -0.23944043],\n","       [-0.7215475 ,  0.05019488, -0.92468178,  0.33440517]])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"wsnsTxYLRYez","colab_type":"text"},"source":["Some also use the following technique for initialization : $\\sqrt{\\frac{1}{size^{[l-1]}+size^{[l]}}}$"]},{"cell_type":"code","metadata":{"id":"pw6XgRr9Q9np","colab_type":"code","colab":{}},"source":["def init(l):\n","  \"\"\"\n","  Arguments:\n","    l = No. of layers\n","  Returns:\n","    w = weight matrix after random initialization\n","  \"\"\"\n","  w = np.random.randn(l-1, l)*np.sqrt(1/((l-1)+l)) # l-1 = hidden layer of the neural network\n","  return w"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_tlsdJJRwm9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1565000402299,"user_tz":-330,"elapsed":1073,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"bcf2da57-7606-4678-965c-ac1610440ebb"},"source":["init(4)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.18884685,  0.26471383,  0.02889929, -0.10014333],\n","       [ 0.07169204, -0.23356503, -0.05924364,  0.1871465 ],\n","       [-0.06871722, -0.3989525 ,  0.33052548,  0.12403013]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"ihSwvCGLj5Vq","colab_type":"text"},"source":["## Gradient Checking\n","We know that backpropagation calculates the derivatives (or gradient). From your calculus course, you might remember that the definition of a derivative is the following:\n","\n"," $$\\lim\\limits_{\\epsilon \\to 0}\\frac{f(x+\\epsilon)-f(x-\\epsilon)}{2\\epsilon}$$\n","\n"," ![alt text](https://miro.medium.com/max/700/0*RyRy2pd7UcpynLvR.png)\n","\n","The definition above can be used as a numerical approximation of the derivative. Taking $\\epsilon$ small enough, the calculated approximation will have an error in the range of $\\epsilon^2$.\n","\n","In other words, if $\\epsilon$ is 0.001, the approximation will be off by 0.00001.\n","\n","Therefore, we can use this to approximate the gradient, and in turn make sure that backpropagation is implemented properly. This forms the basis of gradient checking!\n","\n","## Vectorized implementation\n","Now, we need to define a vectorized form of gradient checking before implementing it Python.\n","\n","Let’s take the weights($w$) and bias($b$) matrices and reshape them into a big vector $\\theta$. Similarly, all their respective derivatives will be placed into a vector $d\\theta$. Therefore, the approximate gradient can be expressed as:\n","\n","$$d\\theta_{approx}=\\frac{J(\\theta_{1},\\theta_{2},...,\\theta_{i}+\\epsilon)-(\\theta_{1},\\theta_{2},...,\\theta_{i}-\\epsilon)}{2\\epsilon}$$\n","\n","Then, we apply the following formula for gradient check:\n","\n","$$\\frac{||d\\theta_{approx}-d\\theta||_{2}}{||d\\theta_{approx}||_{2}+||d\\theta||_{2}}$$\n","\n","The equation above is basically the Euclidean distance normalized by the sum of the norm of the vectors. We use normalization in case that one of the vectors is very small.\n","\n","As a value for epsilon, we usually opt for $1*e^{-7}$. Therefore, if gradient check return a value less than $1*e^{-7}$, then it means that backpropagation was implemented correctly. Otherwise, there is potentially a mistake in our implementation. If the value exceeds $1*e^{-3}$, then we are sure that the code is not correct."]},{"cell_type":"markdown","metadata":{"id":"aCJ0tyGKrOtE","colab_type":"text"},"source":["### Implementing Gradiant Checking"]},{"cell_type":"code","metadata":{"id":"zQmyL5LKSwDG","colab_type":"code","colab":{}},"source":["def forward_propagation(x, theta):\n","  J = np.dot(theta, x)\n","  return J"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-eHmXEurdnC","colab_type":"code","colab":{}},"source":["def backward_propagation(x, theta):\n","  dtheta = x\n","  return dtheta"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfdOx2R7rtOt","colab_type":"code","colab":{}},"source":["def gradiant_check(x, theta, epsilon=1e-7):\n","  theta_plus = theta + epsilon\n","  theta_minus = theta - epsilon\n","\n","  J_plus = forward_propagation(x, theta_plus)\n","  J_minus = forward_propagation(x, theta_minus)\n","\n","  grad_approx = (J_plus - J_minus) / (2 * epsilon)\n","\n","  # Checking if grad_approx is close enough backward_propagation\n","  grad = backward_propagation(x, theta)\n","\n","  numerator = np.linalg.norm(grad - grad_approx)\n","  denominator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n","  difference = numerator/denominator\n","\n","  if difference < epsilon:\n","    print('The gradiant is correct')\n","  else:\n","    print('The gradiant is wrong')\n","\n","  return difference"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bp-WYXFQuDim","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1565007707140,"user_tz":-330,"elapsed":1333,"user":{"displayName":"Rajarshi Bhadra","photoUrl":"https://lh5.googleusercontent.com/-_s5bp4lWYPk/AAAAAAAAAAI/AAAAAAAAAAc/KyTn6Iyo19o/s64/photo.jpg","userId":"08800988258615144457"}},"outputId":"cd1431c9-f7a2-4b37-ab55-606ccb66b3bc"},"source":["difference = gradiant_check(2,4)\n","print('Difference = ' + str(difference))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The gradiant is correct\n","Difference = 2.919335883291695e-10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X-opKi5Ju31S","colab_type":"text"},"source":["# Optimization algorithms"]},{"cell_type":"markdown","metadata":{"id":"v0p-JZrpvhlM","colab_type":"text"},"source":["## Mini-Batch Gradiant Descent\n","\n","Gradient descent is an optimization technique used for computing the model parameters (weight and bias) for algorithms like linear regression, logistic regression, neural networks, etc. In this technique, we repeatedly iterate through the training set and update the model parameters in accordance with the gradient of error with respect to the training set.\n","\n","In case of **Mini-Batch Gradient Descent** Parameters are updated after computing the gradient of error with respect to a subset of the training set.\n","\n","Since a subset of training examples is considered, it can make quick updates in the model parameters and can also exploit the speed associated with vectorizing the code.\n","\n","Depending upon the batch size, the updates can be made less noisy – **greater the batch size less noisy is the update**.\n","\n","Vectorization allows us to efficiently compute on all 'm' examples, that allows us to process our whole training set without an explicit formula. That's why we would take our training examples and stack them into these huge matrix capsule $x^{(1)}, x^{(2)}, x^{(3)},$ and then eventually it goes up to $x^{(m)}$ training samples. And similarly for Y this is $y^{(1)}$ and $y^{(2)}$, $y^{(3)}$ and so on up to $y^{(m)}$ (Here $^{(m)}$ = 1 $million^{th}$ example). So, the dimension of X was an x by m and Y was 1 by m. \n","\n","\n","$$X=x^{(1)}, x^{(2)}, x^{(3)},..,x^{(1000)}, x^{(1001)},..x^{(2000)},......x^{(m)}$$\n","\n","Now stacking\n","* $x^{(1)}$ to $x^{(1000)}$ to $x^1$\n","* $x^{(1001)}$ to $x^{(2000)}$ to $x^2$\n","* upto $x^{(m)}$ to $x^{5000}$\n","\n","$$Y=y^{(1)}, y^{(2)}, y^{(3)},..,y^{(1000)}, y^{(1001)},..y^{(2000)},......y^{(m)}$$\n","\n","Similarly\n","* $y^{(1)}$ to $y^{(1000)}$ to $y^1$\n","* $y^{(1001)}$ to $y^{(2000)}$ to $y^2$\n","* upto $y^{(m)}$ to $y^{5000}$\n","\n","That means we have divided this  big 1 million training examples to 5000 parts. These parts named $x^1, x^2, x^{5000}$ or $y^1, y^2, y^{5000}$ are known as Mini-Batch.\n","\n","### Building mini-batches from training set:\n","There are two steps:\n","* **Shuffle:**Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.\n","\n","![alt text](https://datascience-enthusiast.com/figures/kiank_shuffle.png)\n","\n","* **Partition:**Partition the shuffled (X, Y) into mini-batches of size `mini_batch_size` (here 64). The number of training examples is not always divisible by `mini_batch_size`. The last mini batch might be smaller, but we don't need to worry about this. When the final mini-batch is smaller than the full `mini_batch_size`, it will look like this:\n","\n","![alt text](https://datascience-enthusiast.com/figures/kiank_partition.png)\n","\n","### Algorithm:\n","Let theta = model parameters and max_iters = number of epochs = 5000.\n","\n","for itr = 1, 2, 3, …, 5000:\n","  for mini_batch (X_mini, y_mini):\n","\n","* Forward Pass on the batch X_mini:\n"," * Make predictions on the mini-batch\n","  \n","   * $z^{[1]}=w^{[1]}x^t+b^{[1]}$ where, $x^t$ is mini-batch representation.\n","   * $A^{[1]}=g^{[1]}(z^{[1]})$\n"," * Compute error in predictions (J($\\theta$)) with the current values of the parameters\n","   * $J^t = \\frac{1}{1000}\\sum_{i=1}^{l} L({\\hat{y}}^{(i)}, {y}^{(i)} + \\frac{\\lambda}{2*1000}\\sum_{i=1}^{L}||w^{[t]}||_{F}^2$\n","* Backward Pass:\n"," * Compute gradient($\\theta$) = partial derivative of J($\\theta$) w.r.t. $\\theta$\n","   \n","* Update parameters:\n"," * theta = theta – $\\alpha$*gradient(theta) where $\\alpha$ = Learning Rate.\n","   * $w^{[l]}:=w^{[l]}-\\alpha dw^{[l]}$ & $b^{[l]}:=b^{[l]}-\\alpha b^{[l]}$\n","\n","**The main advantages:**\n","1. Faster than Batch version because it goes through a lot less examples than Batch (all examples).\n","2. Randomly selecting examples will help avoid redundant examples or examples that are very similar that don’t contribute much to the learning.\n","3. With batch size < size of training set, it adds noise to the learning process that helps improving generalization error.\n","4. Even though with more examples the estimate would have lower standard error, the return is less than linear compared to the computational burden we incur.\n","\n","**The main disadvantages:**\n","1. It won’t converge. On each iteration, the learning step may go back and forth due to the noise. Therefore, it wanders around the minimum region but never converges.\n","2. Due to the noise, the learning steps have more oscillations and requires adding learning-decay to decrease the learning rate as we become closer to the minimum.\n","\n","![alt text](https://miro.medium.com/max/700/1*5mHkZw3FpuR2hBNFlRxZ-A.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"taFmuM2RMxTZ"},"source":["## Exponentially Weighted Average\n","A fast and efficient way to compute moving averages - implemented in the different optimization algorithms.\n","\n","Example: Temperature $\\theta_{t}$ over days, calculate the moving averages:\n","\n","$\\theta_{1} = 40^o F$\n","\n","$\\theta_{2} = 49^o F$\n","\n","$\\theta_{3} = 45^o F$\n","\n","$\\vdots$\n","\n","$\\theta_{180} = 60^o F$\n","\n","$\\theta_{181} = 56^o F$\n","\n","$\\vdots$\n","\n","\n","Now lets say,\n","\n","$v_{t}:$ Moving average value at day 't'\n","\n","$v_{0}$ = $0.9v_{0}+0.1\\theta_{0}$\n","\n","$v_{2}$ = $0.9v_{1}+0.1\\theta_{2}$\n","\n","In general we can write $v_{t}$ = $0.9v_{t-1}+0.1\\theta_{t}$\n","\n","If $\\beta = 0.9$\n","\n","$v_{t}$ = $\\beta v_{t-1}+(1-\\beta)\\theta_{t}$\n","\n","![alt text](http://ashukumar27.io/assets/neuralnets/temp1.png)\n","\n","The above equation would give the moving average line in Red over the data points $\\theta_{t}$ in Blue.\n","\n","$v_{t}$ = averaging over $\\frac{1}{1-\\beta}$ days\n","\n","* For $\\beta = 0.9$, $\\frac{1}{1-\\beta}~=10$\n","\n","  $\\beta=0.9$ averages over 10 days - **Smooth curve: Red Line**\n","* For $\\beta=0.98$, $\\frac{1}{1-\\beta}~=50$\n"," \n"," $\\beta=0.98$ averages over 50 days - **smoother curve: Green Line - Not very accurate representation**\n","* For $\\beta = 0.5$, $\\frac{1}{1-\\beta}~=2$ \n","  \n","  $\\beta = 0.5$ averages over 2 days - **Fluctuations: Yellow Line - Much more noisy**\n","\n","![alt text](http://ashukumar27.io/assets/neuralnets/temp2.png)\n","\n","### Working Principle:\n","$v_{t}$ = $\\beta v_{t-1}+(1-\\beta)\\theta_{t}$\n","\n","Now going backwards from $v_{100}$,\n","\n","$v_{100}=0.9v_{99}+0.1\\theta_{100}$\n","\n","$v_{99}=0.9v_{98}+0.1\\theta_{99}$\n","\n","Now substituting $v_{99}$ from $v_{100}$ using the above equation-\n","\n","$v_{100}=0.9(0.9v_{98}+0.1\\theta_{99})+0.1\\theta_{100}$\n","\n","In this way we can expand this equation by substituting $v_{t}$ by it's previous term $v_{t-1}$.\n","\n","Like,\n","$v_{100}=0.9(0.9(0.9v_{97}+0.1\\theta_{98})+0.1\\theta_{99})+0.1\\theta_{100}$\n","\n","If we generalize this equation, we get-\n","\n","$$v_{100}=0.1\\theta_{100}+0.1*0.9*\\theta_{99}+0.1*(0.9)^2*\\theta_{98}+0.1*(0.9)^3*\\theta_{97}+....$$\n","\n","$v_{100}$ is basically an element wise computation of two metrices/functions-\n","\n","1. An exponential decay function containing diminishing value - $0.9, (0.9)^2, (0.9)^3, ....$\n","2. Another with all elements $\\theta_{t}$.\n","\n","### Implementing Exponentially Weighted Average:\n","\n","$v_{0}=0$\n","\n","for $\\theta_{t} = 1:end$:\n","\n","> Get $\\theta_{t}:$\n","\n","> Get $v_{\\theta}:=\\beta v_{\\theta}+(1-\\beta)\\theta_{t}$\n","\n","### Bias Correction in Exponentially Weighted Moving Average:\n","Making EWMA more accurate - Since the curve starts from 0, there are not many values to average on in the initial days. Thus, the curve is lower than the correct value initially and then moves in line with expected values.\n","\n","Figure: The ideal curve shoule be the GREEN one, but it starts as the PURPLE curve since the values initially are zero\n","\n","![alt text](http://ashukumar27.io/assets/neuralnets/temp3.png)\n","\n","The initial values of $v_{t}$ will be bery low which need to be compensated.\n","\n","$v_{t}=\\frac{v_t}{1-\\beta^t}$\n","\n","for $t=2, 1-\\beta^t = 1-0.98^2 = 0.0396$ (Bias Correction Factor)\n","\n","$v_{2}=\\frac{v_2}{0.0396}=\\frac{0.0196\\theta_1+0.02\\theta_2}{0.0396}$\n","\n","When 't' is large, $\\frac{1}{1-\\beta^t}=1$, hence bias correction factor has no effect when 't' is sufficiently large. It only jacks up the initial values.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sW6Sx8UdM0sb","colab_type":"text"},"source":["## Gradiant Descent with momentum\n","Mini-batch gradient descent makes a parameter update with just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Gradient Descent with Momentum considers the past gradients to smooth out the update. It computes an exponentially weighted average of our gradients, and then use that gradient to update our weights instead. It works faster than the standard gradient descent algorithm.\n","\n","### Working:\n","Considering an example where we are trying to optimize a cost function which has contours like below and the red dot denotes the position of the local optima (minimum).\n","![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig1-2-768x151.jpg)\n","\n","We start gradient descent from point ‘A’ and after one iteration of gradient descent we may end up at point ‘B’, the other side of the ellipse. Then another step of gradient descent may end up at point ‘C’. With each iteration of gradient descent, we move towards the local optima with up and down oscillations. If we use larger learning rate then the vertical oscillation will have higher magnitude. So, this vertical oscillation slows down our gradient descent and prevents us from using a much larger learning rate.\n","![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig2-2.jpg)\n","\n","By using the exponentially weighted average values of dW and db, we tend to average out the oscillations in the vertical direction closer to zero as they are in both directions (positive and negative). Whereas, on the horizontal direction, all the derivatives are pointing to the right of the horizontal direction, so the average in the horizontal direction will still be pretty big. It allows our algorithm to take more straight forwards path towards local optima and damp out vertical oscillations. Due to this reason, the algorithm will end up at local optima with a few iterations.\n","![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig3-2-768x167.jpg)\n","\n","### Implementation:\n","\n","During backward propagation, we use dw and db to update our parameters w and b as follows:\n","\n","$$w: = w – \\alpha * dw$$\n","\n","$$b: = b – \\alpha * db$$\n","\n","In momentum, instead of using dW and db independently for each epoch, we take the exponentially weighted averages of dw and db.\n","\n","$$vdw: = β * vdw + (1 – β) * dw$$\n","\n","$$vdb: = β * vdb + (1 – β) * db$$\n","\n","Where beta ‘β’ is another hyperparameter called momentum and ranges from 0 to 1. It sets the weight between the average of previous values and the current value to calculate the new weighted average.\n","\n","After calculating exponentially weighted averages, we will update our parameters.\n","\n","$$w: = w – \\alpha *vdw:$$\n","\n","$$b: = b – \\alpha * vdb:$$\n","\n","### Selecting $\\beta$\n","\n","* The momentum ($\\beta$) must be higher to smooth out the update because we give more weight to the past gradients.\n","* It is recommended to use the default value for β = 0.9 but if required, it can be tuned between 0.8 to 0.999.\n","* Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vNKb99UpJBZs","colab_type":"text"},"source":["## RMSprop\n","The RMSprop optimizer is similar to the gradient descent algorithm with momentum. The RMSprop optimizer restricts the oscillations in the vertical direction. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. \n","\n","The difference between RMSprop and gradient descent is on how the gradients are calculated. \n","\n","The following equations show how the gradients are calculated for the RMSprop and gradient descent with momentum. The value of momentum($\\beta$) is denoted by beta and is usually set to 0.9.\n","\n","$$sdw: = β * sdw + (1 – β) * dw^2$$\n","\n","$$sdb: = β * sdb + (1 – β) * db^2$$\n","\n","$$w: = w – \\alpha *\\frac{dw}{\\sqrt{sdw:}+\\epsilon}\\tag1$$\n","\n","$$b: = b – \\alpha *\\frac{db}{\\sqrt{sdb:}+\\epsilon}\\tag2$$\n","\n","![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig1-2-768x151.jpg)\n","\n","For the above graph of SGD if we consider,\n","\n","**$b$ -> Vertical Axis**\n","\n","**$w$ -> Horizontal Axis**\n","\n","We can see from the above plot that $b>w$. That means vertically the step is higher then horizontally. So the randomly initialized point takes longer time to reach local optima. So learning is comparetively slow.\n","\n","But we want $w>b$. That means the steps will be larger in horizontal direction than vertical direction. So the randomly initialized point takes less time to reach local optima. So the learning is faster.\n","\n","To get our desired speed of learning,\n","1. We multiply $dw^2$ (where $dw^2<dw<db$) with $(1-\\beta)$ to get $sdw:$. \n","\n"," So $sdw:$ will become smaller and then we divide $\\sqrt{sdw:}$ from $\\alpha*dw$ to get updated $w:$. \n"," \n"," So  $w:$ will be larger.\n","\n","2. We multiply $db^2$ (where $db^2>db>dw$) with $(1-\\beta)$ to get $sdb:$. \n","\n"," So $sdb:$ will become larger and then we divide $\\sqrt{sdb:}$ from $\\alpha*db$ to get updated $b:$. \n"," \n"," So $b:$ will be smaller.\n","\n","Thus we get $w>b$ using RMSProp and get the following kind of graph.\n","\n","![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig3-2-768x167.jpg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_ykoHpH0ED2d","colab_type":"text"},"source":["## Adam\n","The Adaptive Moment Estimation or Adam optimization algorithm is one of those algorithms that work well across a wide range of deep learning architectures. It is recommended by many well-known neural network algorithm experts. The Adam optimization algorithm is a combination of gradient descent with momentum and RMSprop algorithms.\n","\n","### Algorithm:\n","* Initialize $vdw=sdw=vdb=sdb=0$.\n","* For itreration T = 1:end:\n"," * Compute $dw, db$\n","\n"," * Update $vdw:, vdb:$ like momentum & $sdw:, sdb:$ like RMSProp.\n"," \n","* Incase of ADAM optimizer, we implement bias correction.\n"," \n"," $$vdw:^{corrected}=\\frac{vdw}{1-\\beta_1^t}$$\n"," \n"," $$vdb:^{corrected}=\\frac{vdb}{1-\\beta_1^t}$$\n"," \n"," $$sdw:^{corrected}=\\frac{sdw}{1-\\beta_2^t}$$\n"," \n"," $$sdb:^{corrected}=\\frac{sdb}{1-\\beta_2^t}$$\n"," \n","* Update parameters $w,b$\n","\n","$$w: = w – \\alpha *\\frac{vdw:^{corrected}}{\\sqrt{sdw:^{corrected}}+\\epsilon}\\tag1$$\n","\n","$$b: = b – \\alpha *\\frac{vdb:^{corrected}}{\\sqrt{sdb:^{corrected}}+\\epsilon}\\tag2$$\n"," \n","  \n","$\\beta_1$ and $\\beta_2$ are hyper parameters that control the two exponentially weighted averages. In practice we use the default values for $\\beta_1$ = 0.9 and $\\beta_2$ = 0.999.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZFkD7uHrnol6","colab_type":"text"},"source":["## Learning Rate Decay\n","When training deep neural networks, it is often useful to reduce learning rate as the training progresses. This can be done by using pre-defined learning rate schedules or adaptive learning rate methods.\n","\n","Here a convolutional neural network has been trained on CIFAR-10 using differing learning rate schedules and adaptive learning rate methods to compare their model performances.\n","### Constant Learning Rate\n","Constant learning rate is the default learning rate schedule in SGD optimizer in Keras. Momentum and decay rate are both set to 0 by default. It is tricky to choose the right learning rate. \n","\n","By experimenting with range of learning rates in our example, `lr=0.1` shows a relative good performance to start with. This can serve as a baseline for us to experiment with different learning rate strategies.\n","\n","```\n","keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\n","```\n","\n","![alt text](https://miro.medium.com/max/432/1*Lv7-jMtHOoucryv9mUtFGg.jpeg)\n","\n","### Time-Based Decay\n","The mathematical form of time-based decay is:\n","```\n","lr: = lr/(1+kt)\n","```\n","\n","where,\n","\n","* `lr, k` are hyperparameters\n"," \n","* `t` is the iteration number. \n","\n","Looking into the source code of Keras, the SGD optimizer takes `decay` and `lr` arguments and update the learning rate by a decreasing factor in each epoch.\n","\n","Momentum is another argument in SGD optimizer which we could tweak to obtain faster convergence. Unlike classical SGD, momentum method helps the parameter vector to build up velocity in any direction with constant gradient descent so as to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.\n","\n","SGD optimizer also has an argument called `nesterov` which is set to `False` by default. Nesterov momentum is a different version of the momentum method which has stronger theoretical converge guarantees for convex functions. In practice, it works slightly better than standard momentum.\n","\n","In Keras, we can implement time-based decay by setting the initial learning rate, decay rate and momentum in the SGD optimizer.\n","\n","```\n","learning_rate = 0.1\n","decay_rate = learning_rate / epochs\n","momentum = 0.8\n","sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n","```\n","![alt text](https://miro.medium.com/max/432/1*YpzU0MkpNaZ8f6cGvqex7g.jpeg)\n","\n","### Step Decay\n","Step decay schedule drops the learning rate by a factor every few epochs. The mathematical form of step decay is:\n","\n","```\n","lr: = lr * drop^floor(epoch / epochs_drop)\n","```\n","\n","A typical way is to to drop the learning rate by half every 10 epochs. To implement this in Keras, we can define a step decay function and use `LearningRateScheduler` callback to take the step decay function as argument and return the updated learning rates for use in SGD optimizer.\n","\n","```\n","def step_decay(epoch):\n","   initial_lrate = 0.1\n","   drop = 0.5\n","   epochs_drop = 10.0\n","   lrate = initial_lrate * math.pow(drop,  \n","           math.floor((1+epoch)/epochs_drop))\n","   return lrate\n","lrate = LearningRateScheduler(step_decay)\n","```\n","As a digression, a callback is a set of functions to be applied at given stages of the training procedure. We can use callbacks to get a view on internal states and statistics of the model during training. In our example, we create a custom callback by extending the base class keras.callbacks.Callback to record loss history and learning rate during the training procedure.\n","```\n","class LossHistory(keras.callbacks.Callback):\n","    def on_train_begin(self, logs={}):\n","       self.losses = []\n","       self.lr = []\n"," \n","    def on_epoch_end(self, batch, logs={}):\n","       self.losses.append(logs.get(‘loss’))\n","       self.lr.append(step_decay(len(self.losses)))\n","```\n","Putting everything together, we can pass a callback list consisting of `LearningRateScheduler` callback and our custom callback to fit the model. We can then visualize the learning rate schedule and the loss history by accessing `loss_history.lr` and `loss_history.losses`.\n","```\n","loss_history = LossHistory()\n","lrate = LearningRateScheduler(step_decay)\n","callbacks_list = [loss_history, lrate]\n","history = model.fit(X_train, y_train, \n","   validation_data=(X_test, y_test), \n","   epochs=epochs, \n","   batch_size=batch_size, \n","   callbacks=callbacks_list, \n","   verbose=2)\n","```\n","![alt text](https://miro.medium.com/max/432/1*qite6RcBZHFmiVAZBxwM0g.jpeg)\n","\n","\n","![alt text](https://miro.medium.com/max/432/1*VQkTnjr2VJOz0R2m4hDucQ.jpeg)\n","\n","### Exponential Decay\n","Another common schedule is exponential decay. It has the mathematical form,\n","```\n","lr = lr0 * e^(−kt)\n","```\n","where, \n","* `lr, k` are hyperparameters \n","* `t` is the iteration number. \n","\n","Similarly, we can implement this by defining exponential decay function and pass it to `LearningRateScheduler`. In fact, any custom decay schedule can be implemented in Keras using this approach. The only difference is to define a different custom decay function.\n","```\n","def exp_decay(epoch):\n","   initial_lrate = 0.1\n","   k = 0.1\n","   lrate = initial_lrate * exp(-k*t)\n","   return lrate\n","lrate = LearningRateScheduler(exp_decay)\n","```\n","![alt text](https://miro.medium.com/max/432/1*MpVdh9K7nmpZ0VeT17q5FQ.jpeg)\n","\n","![alt text](https://miro.medium.com/max/432/1*iSZv0xuVCsCCK7Z4UiXf2g.jpeg)\n","\n","**Now compare the model accuracy using different learning rate schedules in our example:**\n","\n","![alt text](https://miro.medium.com/max/700/1*5L0VxjeCL3m-k5nfbm2lZg.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"vCNjNDzMW22J","colab_type":"text"},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"TljfVs0n8LNZ","colab_type":"text"},"source":["## Tuning Process\n","Some of the important hyperparameters which effect our model's accuracy and learning time are- \n","\n","1. Learning rate – $\\alpha$\n","2. Momentum – $\\beta$\n","3. Adam’s hyperparameter – $\\beta_{1}, \\beta_{2}, \\epsilon$\n","4. Number of hidden layers\n","5. Number of hidden units for different layers\n","6. Learning rate decay\n","7. Mini-batch size\n","\n","Learning rate usually proves to be the most important among the above. This is followed by the number of hidden units, momentum, mini-batch size, the number of hidden layers, and then the learning rate decay.\n","\n","Now,let's suppose we have two hyperparameters. \n","\n","Hyperparameter1 = Learning Rate($\\alpha$)\n","\n","Hyperparameter2 = Adam's hyperparameter($\\epsilon$)\n","\n","We sample the points in a grid and then systematically explore these values. Consider a five-by-five grid:\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/11/Screenshot-from-2018-11-05-12-48-22.png)\n","\n","Now among these two hyperparameters, $\\alpha$ is more dominant than $\\epsilon$. \n","\n","For this reason, incase of grid search we end up getting only 5 trial values out of 25 for $\\alpha$.\n","\n","So to overcome this problem we choose random search.\n","\n","![alt text](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/11/Screenshot-from-2018-11-05-12-55-19.png)\n","\n","For random search we will be trying out 25 distinct value of $\\alpha$. Therefore we will be more likely to find a value that works well.\n","\n","## Using an appropriate scale to pick hyperparameters:\n","Let's consider, \n","1. Number of hidden units ($n^{[l]}$) in a neural network is 50 to 100. In this case choosing random search will be reasonable to tuning process.\n","2. We have $\\alpha$ from 0.0001 to 1.\n","\n","$$0.0001 -------------------------------1$$\n","\n"," 90% of the values will be from 0.1 to 1 so we're using 90% of resources to search between 0.1 and 1 and only 10% resources we're using to search between 0.0001 to 0.1. In this case random search will not work well.\n","\n"," Here scaling these values in a log scale will do the job better.\n","\n"," $$0.0001 ----0.001 ----0.01 ----0.1 ----1$$\n","\n"," Now we can use random search.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"h5sbVp2Qpx0g","colab_type":"text"},"source":["# Multiclass classification"]},{"cell_type":"markdown","metadata":{"id":"h3HFlfO52NQB","colab_type":"text"},"source":["## Softmax Regression\n","This is used when we have multiple objects to classify. So in this case we'll be having multiple units in the o/p layer.\n","\n","If we have 4 classes then we'll be having 4 units in the o/p layer.\n","\n","![alt text](https://engmrk.com/wp-content/uploads/2018/05/Fig1-3.jpg)\n","\n","$z^{[L]}=w^{[l]}*a^{[l-1]}+b^{[l]}$\n","\n","**Now calculating the softmax activation function-**\n","\n","$t=e^{z^{[l]}}$ where $t$ = temporary variable\n","\n","$a^{[l]}=\\frac{e^{z^{l}}}{\\sum_{j=1}^{4} t_i}$\n","& $a_{i}^{[l]}=\\frac{t_{i}}{\\sum_{j=1}^{4} t_i}$\n","\n","Now for example, if we find $z = \n"," \\begin{bmatrix}\n","  5 \\\\\n","  2 \\\\\n","  -1 \\\\\n","  3 \n"," \\end{bmatrix}\n","$\n","then $t = \n"," \\begin{bmatrix}\n","  e^5 \\\\\n","  e^2 \\\\\n","  e^{-1} \\\\\n","  e^3 \n"," \\end{bmatrix}\n","= \n"," \\begin{bmatrix}\n","  148.4 \\\\\n","  7.4 \\\\\n","  0.4 \\\\\n","  20.1 \n"," \\end{bmatrix}\n","$\n","& $\\sum_{j=1}^{4} t_i = 1766.3$\n","\n","So, $a^{[l]} = \\frac{t}{176.3}$\n","\n","So the softmax activation for the each unit of the output layer will be- $ \n"," \\begin{bmatrix}\n","  \\frac{e^5}{176.3} \\\\\n","  \\frac{e^2}{176.3} \\\\\n","  \\frac{e^{-1}}{176.3} \\\\\n","  \\frac{e^3}{176.3} \n"," \\end{bmatrix}\n","= \n"," \\begin{bmatrix}\n","  0.842 \\\\\n","  0.042 \\\\\n","  0.002 \\\\\n","  0.114 \n"," \\end{bmatrix}\n","$\n","\n","We can also say that, \n","$a^{[l]} = g^{[l]}(z^{[l]}) =  \\begin{bmatrix}\n","  \\frac{e^5}{e^5+e^2+e^{-1}+e^3} \\\\\n","  \\frac{e^2}{e^5+e^2+e^{-1}+e^3} \\\\\n","  \\frac{e^{-1}}{e^5+e^2+e^{-1}+e^3} \\\\\n","  \\frac{e^3}{e^5+e^2+e^{-1}+e^3} \n"," \\end{bmatrix}\n","= \n"," \\begin{bmatrix}\n","  0.842 \\\\\n","  0.042 \\\\\n","  0.002 \\\\\n","  0.114 \n"," \\end{bmatrix}$\n","where $g^{[l]}$ is the activation function of output layer.\n","\n","### Loss Function of Softmax Regression:\n","$L(\\hat{y},y) = \\sum_{j=1}^{4} y_jlog(\\hat{y}_j)$\n","### Cost Function of Softmax Regression:\n","$J(w^{[1]},b^{[1]},...) = \\frac{1}{m}\\sum_{j=1}^{4}L(\\hat{y}^i,y^i)$\n","### Gradiant Descent with Softmax:\n","* Forward Prop.:\n"," \n"," $z^{[l]}->a^{[l]}=\\hat{y}->L(\\hat y,y)$\n","* Backward Prop.:\n","\n"," $dz^{[l]}=\\hat y-y$"]}]}